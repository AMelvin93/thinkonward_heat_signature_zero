{
  "version": "15.7",
  "last_updated": "2026-01-27T04:45:06.049811",
  "mode": "ACTIVE",
  "resume_tracking": {
    "summaries_analyzed": [
      "ensemble_voting",
      "cluster_transfer",
      "lq_cma_es_builtin",
      "bayesian_optimization_gp",
      "cmaes_to_nm_sequential",
      "multi_fidelity_pyramid",
      "fast_source_count_detection",
      "early_timestep_filtering",
      "adaptive_sample_budget",
      "warm_start_cmaes",
      "niching_cmaes_diversity",
      "extended_nm_polish",
      "temporal_40pct_higher_sigma",
      "ipop_cmaes_temporal",
      "adaptive_timestep_fraction",
      "physics_informed_init",
      "pod_reduced_order_model",
      "larger_cmaes_population",
      "two_source_specialized",
      "adaptive_sigma_schedule",
      "active_cmaes_covariance",
      "progressive_polish_fidelity",
      "boundary_aware_initialization",
      "jax_differentiable_solver",
      "levenberg_marquardt_inverse",
      "cmaes_then_gradient_refinement",
      "conjugate_gradient_adjoint",
      "openai_evolution_strategy",
      "differential_evolution",
      "variable_projection_separable",
      "adaptive_simulated_annealing",
      "pretrained_nn_surrogate",
      "pinn_inverse_heat_source",
      "early_rejection_partial_sim",
      "cmaes_restart_from_best",
      "weighted_sensor_loss",
      "multistart_elite_selection",
      "solution_injection_cmaes",
      "polar_parameterization",
      "log_rmse_loss",
      "adaptive_nm_iterations",
      "random_timestep_selection",
      "adaptive_population_size",
      "diagonal_decoding_cmaes",
      "separable_cmaes_diagonal",
      "powell_polish_instead_nm",
      "learning_rate_adapted_cmaes",
      "coordinate_wise_sigma",
      "bfgs_polish_after_cmaes",
      "cmaes_early_stopping",
      "lower_sigma_baseline",
      "checkpointed_adjoint_method",
      "gappy_clustering_pod",
      "physics_compressive_sensing",
      "bipop_cmaes_restart",
      "frequency_domain_optimization",
      "greens_function_inversion",
      "modal_identification_method",
      "informative_sensor_subset",
      "landscape_adaptive_cmaes",
      "rbf_meshless_inverse",
      "cmaes_rbf_surrogate",
      "scipy_slsqp_optimizer",
      "direct_algorithm_search",
      "temporal_fidelity_sweep",
      "more_inits_select_best",
      "correct_temporal_sweep",
      "reduced_cmaes_more_nm",
      "ica_seeded_init",
      "sigma_ladder",
      "weighted_centroid_nm",
      "boundary_handling_methods",
      "confidence_based_early_exit",
      "genetic_algorithm_optimizer",
      "growth_optimizer",
      "mae_polish_loss",
      "multi_objective_pareto",
      "parallel_nm_polish",
      "polish_top3_reduced",
      "coordinate_descent_polish",
      "split_polish_fidelity",
      "tv_regularization_sparse",
      "asymmetric_polish_budget",
      "micro_restart_polish",
      "adaptive_nm_coefficients",
      "moment_based_inversion",
      "nm_dimension_adaptive",
      "adaptive_temporal_fidelity",
      "intensity_prior_from_peak",
      "larger_popsize_exploration",
      "reduced_nm_polish_4iter",
      "sigma_restart_on_stagnation",
      "bobyqa_polish",
      "simulation_result_caching",
      "kriging_local_infill",
      "pso_then_cmaes_hybrid",
      "learned_sampling_policy",
      "condition_number_init",
      "center_spread_parameterization",
      "full_sim_reranking",
      "scaled_parameter_space",
      "laplace_domain_initialization",
      "spsa_gradient_optimizer",
      "vectorized_batch_evaluation",
      "smart_early_termination",
      "mini_batch_sensor_eval",
      "sacobra_rbf_optimizer",
      "extended_kalman_filter_inversion",
      "particle_filter_inversion",
      "variance_reduced_cmaes",
      "cmaes_quasi_newton_polish",
      "separable_position_intensity",
      "ensemble_weighted_solution",
      "parallel_cmaes_info_sharing",
      "greedy_coordinate_refinement",
      "intensity_first_then_position",
      "confidence_weighted_candidate_selection",
      "lightweight_ensemble_postprocessing",
      "adaptive_polish_per_sample",
      "cmaes_restart_inject_best",
      "median_ensemble_solution",
      "top3_ensemble_averaging",
      "multi_restart_nm_polish",
      "pinn_initialization",
      "ensemble_on_40pct_temporal_baseline",
      "simple_position_average_best2",
      "fcmaes_speed_test",
      "adei_heat_source",
      "coarse_to_fine_temporal",
      "stratified_sample_processing",
      "solution_verification_pass",
      "baseline_consistency_test",
      "larger_candidate_pool",
      "tighter_intensity_range",
      "adjusted_dissimilarity_threshold",
      "multi_seed_best_selection",
      "rerun_solution_verification",
      "greedy_diversity_selection",
      "candidate_weighted_ensemble",
      "higher_population_density",
      "improved_triangulation_init",
      "position_bounds_from_init",
      "reduced_fevals_more_polish",
      "extended_verification_all_candidates",
      "double_nm_polish_round",
      "intensity_refinement_only"
    ],
    "last_cycle_timestamp": "2026-01-27T04:45:06.049788",
    "notes": "W2 completed extended_verification_all_candidates: FAILED. Verifying ALL candidates gives same score as verifying just best. verify_top_n=1 is optimal."
  },
  "best_scores": {
    "in_budget": {
      "score": 1.1688,
      "time_min": 58.4,
      "experiment": "temporal_40pct_higher_sigma (W2's config)",
      "algorithm": "CMA-ES + 40% temporal + 8 NM polish + sigma 0.18/0.22",
      "date": "2026-01-19",
      "note": "TRUE BEST IN-BUDGET. From W2's optimized configuration."
    },
    "baseline_simple": {
      "score": 1.1246,
      "time_min": 32.6,
      "experiment": "early_timestep_filtering",
      "note": "Simpler baseline - 25% temporal. NOT the best configuration."
    },
    "solution_verification": {
      "score": 1.1373,
      "time_min": 42.6,
      "note": "Beats simple baseline (1.1246) but NOT the optimized baseline (1.1688)"
    }
  },
  "queue_status": {
    "available_experiments": 4,
    "claimed_experiments": 1,
    "status": "HEALTHY",
    "note": "\u2605\u2605\u2605 Cycle 38 SUCCESS! NEW BEST: solution_verification = 1.1373 @ 42.6 min (+1.0%)"
  },
  "exploration_progress": {
    "total_experiments": 60,
    "experiments_per_family": {
      "evolutionary_cmaes": 34,
      "evolutionary_other": 3,
      "gradient_based": 4,
      "gradient_free_local": 6,
      "surrogate": 2,
      "surrogate_lq": 1,
      "ensemble": 1,
      "decomposition": 2,
      "meta_learning": 3,
      "hybrid": 1,
      "bayesian_opt": 1,
      "multi_fidelity": 1,
      "temporal_fidelity": 1,
      "budget_allocation": 1
    },
    "families_exhausted": [
      "evolutionary_cmaes",
      "gradient_based",
      "evolutionary_other",
      "ensemble",
      "decomposition",
      "meta_learning",
      "surrogate_lq",
      "bayesian_opt",
      "multi_fidelity_spatial",
      "hybrid",
      "preprocessing",
      "budget_allocation",
      "diversity",
      "temporal_fidelity_extended",
      "initialization",
      "surrogate_pod",
      "cmaes_accuracy",
      "source_specific",
      "sigma_scheduling",
      "cmaes_variants",
      "problem_specific",
      "initialization_v2",
      "loss_reformulation",
      "polish_strategy_v2",
      "polish_method_v2",
      "fidelity_polish",
      "regularization",
      "polish_tuning_v2",
      "direct_inversion_v2",
      "restart_v3",
      "nm_adaptive",
      "local_search_v3",
      "temporal_fidelity_v2",
      "physics_init_v2",
      "polish_budget_v2",
      "cmaes_tuning_v2",
      "restart_strategy_v2",
      "surrogate_v2",
      "hybrid_v2",
      "meta_v2",
      "initialization_v4",
      "engineering",
      "surrogate_optimization",
      "state_estimation",
      "evaluation_v2"
    ],
    "families_to_explore": [
      "gradient_approx_v2 - SPSA dimension-independent gradient",
      "evaluation_v2 - Mini-batch sensor evaluation",
      "initialization_v5 - Laplace domain analysis",
      "problem_transform - Parameter scaling",
      "candidate_selection_v2 - Full-sim reranking",
      "parameterization_v2 - Center-spread for 2-source"
    ],
    "research_directions": [
      "The leaderboard top scores (1.22+) prove higher accuracy IS achievable",
      "L-BFGS-B achieved excellent accuracy but was slow due to finite differences - adjoint gradients could fix this",
      "40% temporal fidelity worked - what about frequency domain which naturally captures temporal structure?",
      "Neural operators (FNO) can learn PDE solution operators with high accuracy",
      "The problem may benefit from reformulation - different loss functions, constraints, or parameterizations"
    ]
  },
  "experiments_in_progress": {
    "EXP_COORDINATE_POLISH_001": {
      "worker": "W1",
      "started": "2026-01-25T02:29:10.536783Z",
      "name": "coordinate_descent_polish"
    },
    "EXP_NM_DIM_ADAPTIVE_001": {
      "worker": "W3",
      "started_at": "2026-01-25T03:16:34.054315Z"
    }
  },
  "experiments_completed": [
    {
      "id": "EXP_WEIGHTED_LOSS_001",
      "worker": "W1",
      "score": 1.0131,
      "time_min": 90.7,
      "result": "FAILED",
      "family": "loss_function",
      "note": "Weighted RMSE optimization finds different optimum than unweighted (scored). Diversity destroyed."
    },
    {
      "id": "EXP_CMAES_RESTART_BEST_001",
      "worker": "W1",
      "score": 1.0986,
      "time_min": 175.7,
      "result": "FAILED",
      "family": "cmaes_improvement",
      "note": "Two-phase restart WORSE than single phase. Phase 2 doubles sims without improving accuracy. Diversity lost due to small sigma."
    },
    {
      "id": "EXP_POD_SURROGATE_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "surrogate_pod",
      "note": "POD not viable - sample-specific physics prevents universal basis. Temporal fidelity already achieves same speedup."
    },
    {
      "id": "EXP_PHYSICS_INIT_001",
      "worker": "W1",
      "score": 1.1593,
      "time_min": 71.9,
      "result": "FAILED",
      "family": "initialization",
      "note": "Gradient init WORSE than hottest-sensor (-0.0046). Temperature gradients corrupted by diffusion. Simple heuristics win."
    },
    {
      "id": "EXP_ADAPTIVE_TIMESTEP_001",
      "worker": "W2",
      "score": 1.1635,
      "time_min": 69.9,
      "result": "FAILED",
      "family": "temporal_fidelity_extended",
      "note": "Adaptive 25%->40% WORSE than fixed 40%. CMA-ES covariance needs consistent landscape. Switching fidelity mid-run is counterproductive."
    },
    {
      "id": "EXP_IPOP_TEMPORAL_001",
      "worker": "W2",
      "score": 1.1687,
      "time_min": 75.7,
      "result": "FAILED",
      "family": "temporal_fidelity_extended",
      "note": "IPOP FAILED. Splitting fevals across restarts reduces convergence. Problem doesn't have local optima - restarts don't help."
    },
    {
      "id": "EXP_TEMPORAL_HIGHER_SIGMA_001",
      "worker": "W1",
      "score": 1.1745,
      "time_min": 63.0,
      "best_in_budget_score": 1.1584,
      "best_in_budget_time": 52.1,
      "result": "FAILED",
      "family": "temporal_fidelity_extended",
      "note": "Higher sigma (0.25/0.30) +0.0057 score but +5min. Best in-budget 1.1584 is WORSE than W2 baseline 1.1688. W2's sigma 0.18/0.22 is optimal."
    },
    {
      "id": "EXP_EXTENDED_POLISH_001",
      "worker": "W2",
      "score": 1.1703,
      "time_min": 82.3,
      "result": "FAILED",
      "family": "refinement",
      "note": "12 iterations OVER BUDGET (82.3 min). +0.0015 score not worth +24 min. 8 iterations is optimal."
    },
    {
      "id": "EXP_NICHING_CMAES_001",
      "worker": "W2",
      "score": 1.0622,
      "time_min": 46.9,
      "result": "FAILED",
      "family": "diversity",
      "note": "Niching FAILED. Scoring averages accuracy over candidates - worse diverse candidates hurt. Baseline already at 2.75/3 N_valid."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_001",
      "worker": "W2",
      "score": 1.1688,
      "time_min": 58.4,
      "result": "SUCCESS",
      "family": "temporal_fidelity",
      "note": "NEW BEST! 40% timesteps + 8 NM polish (full): +0.0441 vs baseline. 2-src RMSE dropped 33%."
    },
    {
      "id": "EXP_ADAPTIVE_BUDGET_001",
      "worker": "W1",
      "score": 1.1143,
      "time_min": 56.2,
      "result": "FAILED",
      "family": "budget_allocation",
      "note": "Early termination hurts CMA-ES accuracy. Fixed-budget baseline is optimal."
    },
    {
      "id": "EXP_WS_CMAES_001",
      "worker": "W1",
      "score": 0.276,
      "time_min": 65.8,
      "result": "FAILED",
      "family": "meta_learning",
      "note": "WS-CMA-ES causes divergence. Probing wastes budget. meta_learning family EXHAUSTED."
    },
    {
      "id": "EXP_FAST_SOURCE_DETECT_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "preprocessing",
      "note": "INVALID PREMISE - n_sources already in sample data."
    },
    {
      "id": "EXP_SEQUENTIAL_HANDOFF_001",
      "worker": "W2",
      "score": 1.1132,
      "time_min": 56.6,
      "result": "FAILED",
      "family": "hybrid",
      "note": "Sequential CMA-ES to NM handoff FAILED. Best in-budget 1.1132 is WORSE than baseline."
    },
    {
      "id": "EXP_MULTIFID_OPT_001",
      "worker": "W1",
      "score": 0.259,
      "time_min": 44.9,
      "result": "FAILED",
      "family": "multi_fidelity",
      "note": "Coarse grid RMSE landscape differs from fine grid."
    },
    {
      "id": "EXP_BAYESIAN_OPT_001",
      "worker": "W1",
      "score": 0.31,
      "time_min": 57.6,
      "result": "FAILED",
      "family": "bayesian_opt",
      "note": "GP surrogate doesn't model thermal RMSE landscape."
    },
    {
      "id": "EXP_LQ_CMAES_001",
      "worker": "W1",
      "score": 0.253,
      "time_min": 64.1,
      "result": "FAILED",
      "family": "surrogate_lq",
      "note": "fmin_lq_surr API mismatch - returns ONE solution but we need MULTIPLE candidates."
    },
    {
      "id": "EXP_TRANSFER_LEARN_001",
      "worker": "W2",
      "score": 1.0804,
      "time_min": 59.7,
      "result": "FAILED",
      "family": "meta_learning",
      "note": "Cluster transfer FAILED - sensor features don't predict solution similarity."
    },
    {
      "id": "EXP_ENSEMBLE_001",
      "worker": "W1",
      "score": 1.0972,
      "time_min": 347.9,
      "result": "FAILED",
      "family": "ensemble",
      "note": "Ensemble 5.8x over budget."
    },
    {
      "id": "CMAES_TUNING_BATCH",
      "worker": "ALL",
      "score": 1.1247,
      "time_min": 57.2,
      "result": "EXHAUSTED",
      "family": "evolutionary_cmaes",
      "note": "32+ experiments. Best in-budget: 1.1247."
    },
    {
      "id": "EXP_ADAPTIVE_NM_POLISH_001",
      "worker": "W2",
      "score": 1.1607,
      "time_min": 78.3,
      "result": "FAILED",
      "family": "refinement",
      "note": "Adaptive NM iterations does NOT improve. Fixed 8 is optimal. refinement family EXHAUSTED."
    },
    {
      "id": "EXP_SOLUTION_INJECTION_001",
      "worker": "W1",
      "score": 0.9929,
      "time_min": 122.5,
      "result": "FAILED",
      "family": "population_enhancement",
      "note": "Solution injection requires SEQUENTIAL CMA-ES (to share solutions). Destroys parallelism - 2x over budget, -15% score."
    },
    {
      "id": "EXP_LOG_RMSE_LOSS_001",
      "worker": "W2",
      "score": 1.1677,
      "time_min": 70.5,
      "result": "FAILED",
      "family": "loss_reformulation",
      "note": "Log(1+RMSE) objective: same accuracy but +12 min overhead. Monotonic transformations don't help CMA-ES."
    },
    {
      "id": "EXP_RANDOM_TIMESTEPS_001",
      "worker": "W2",
      "score": null,
      "time_min": 350,
      "result": "FAILED",
      "family": "temporal_sampling",
      "note": "Random timesteps requires FULL simulation (3-4x slower). Projected 350 min (5.8x over budget). Efficiency comes from running FEWER timesteps, not DIFFERENT timesteps."
    },
    {
      "id": "EXP_SEP_CMAES_001",
      "worker": "W2",
      "score": 1.1501,
      "time_min": 312.0,
      "result": "FAILED",
      "family": "cmaes_variants",
      "note": "Diagonal covariance HURTS both accuracy (-0.0187) and time (5.3x over budget). Position correlations along heat gradient are essential. Full covariance is optimal."
    },
    {
      "id": "EXP_EARLY_STOP_CMA_001",
      "worker": "W2",
      "score": 1.1378,
      "time_min": 313.3,
      "result": "FAILED",
      "family": "efficiency",
      "note": "Early stopping NEVER triggered (0 saved fevals). CMA-ES continues improving >1% throughout its full budget. 5.4x over budget due to overhead. efficiency family EXHAUSTED."
    },
    {
      "id": "EXP_CHECKPOINTED_ADJOINT_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "FAILED",
      "family": "gradient_revisited",
      "note": "Single-timestep adjoint correct (0.5% error), but full gradient has 5x error due to chain rule through optimal q(x,y). gradient_revisited family EXHAUSTED."
    },
    {
      "id": "EXP_GAPPY_CPOD_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "surrogate_v2",
      "note": "Physics clustering works (4 clusters), BUT 100% unique sensor locations (80/80) defeats Gappy C-POD. surrogate_v2 family EXHAUSTED."
    },
    {
      "id": "EXP_PHYSICS_CS_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "inverse_method",
      "note": "Observation matrix needs 97 min/sample (budget 9 sec). 646x over budget. Coarse grids 6x over. Sample-specific sensors prevent pre-computation. inverse_method family EXHAUSTED."
    },
    {
      "id": "EXP_FREQUENCY_DOMAIN_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "frequency_domain",
      "note": "RMSE computation is 0.082ms, simulation is 1207ms (14,721x slower). Frequency domain addresses wrong bottleneck. frequency_domain family EXHAUSTED."
    },
    {
      "id": "EXP_GREENS_FUNCTION_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "direct_inversion",
      "note": "Green's function is 4.3x SLOWER than ADI (5151ms vs 1189ms). Sensor heterogeneity (100% unique) prevents pre-computation. Inversion is nonlinear. direct_inversion family EXHAUSTED."
    },
    {
      "id": "EXP_MODAL_IDENTIFICATION_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "model_reduction",
      "note": "Only 2 sensors = 2 modes max. Simulation is bottleneck (1167ms), SVD is 0.02ms. Modal space doesn't reduce simulations. model_reduction family EXHAUSTED."
    },
    {
      "id": "EXP_SENSOR_SUBSET_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "sensor_optimization",
      "note": "Subset RMSE has r=0.68 correlation with full RMSE. This is proxy optimization which failed in weighted loss (-14%). sensor_optimization family EXHAUSTED."
    },
    {
      "id": "EXP_BIPOP_CMAES_001",
      "worker": "W1",
      "score": 1.1609,
      "time_min": 54.9,
      "result": "FAILED",
      "family": "cmaes_restart_v2",
      "note": "BIPOP restarts add 8 min overhead without accuracy gain. Confirms IPOP finding: problem lacks local optima."
    },
    {
      "id": "EXP_LANDSCAPE_ADAPTIVE_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "meta_optimization",
      "note": "ELA probing (10 evals \u00d7 1 sec = 10 sec) exceeds per-sample budget (9 sec). All prior adaptive strategies failed. meta_optimization family EXHAUSTED."
    },
    {
      "id": "EXP_RBF_MESHLESS_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "meshless_direct",
      "note": "Matrix A requires n_rbf_points simulations. Even coarsest 5\u00d73=15 pts = 15 sec exceeds budget (9 sec). 100% unique sensors prevent pre-computation. Same issue as D-PBCS (646x over). meshless_direct family EXHAUSTED."
    },
    {
      "id": "EXP_CMAES_RBF_SURROGATE_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "surrogate_hybrid",
      "note": "CMA-ES covariance adaptation is already implicit surrogate modeling. ~10% rejection rate (same as early rejection), small population (6-8) mean minimal benefit (~6% sim reduction). surrogate_hybrid family EXHAUSTED."
    },
    {
      "id": "EXP_SCIPY_SLSQP_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "gradient_numerical",
      "note": "SLSQP uses finite diff gradients = same O(n) overhead as L-BFGS-B which FAILED. NM x8 is optimal polish method. gradient_numerical family EXHAUSTED."
    },
    {
      "id": "EXP_DIRECT_ALGORITHM_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "deterministic_global",
      "note": "DIRECT uses 441-443 evals vs CMA-ES 20-36 (12-22x more). Projected 236 min vs 60 min budget (4x over). deterministic_global family EXHAUSTED."
    },
    {
      "id": "EXP_GENETIC_ALGORITHM_001",
      "worker": "W1",
      "score": 1.1615,
      "time_min": 64.8,
      "result": "FAILED",
      "family": "alternative_optimizer",
      "note": "GA fundamentally inferior to CMA-ES. All runs over budget with worse scores. GA lacks CMA-ES covariance adaptation."
    },
    {
      "id": "EXP_MULTI_OBJECTIVE_001",
      "worker": "W1",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "multi_objective",
      "note": "Theoretically unsound - scoring formula is already multi-objective. NSGA-II also 6-10x over eval budget."
    },
    {
      "id": "EXP_GROWTH_OPTIMIZER_001",
      "worker": "W1",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "alternative_metaheuristic",
      "note": "Growth Optimizer lacks covariance adaptation. Pattern of failures (GA, DE, OpenAI ES, SA) proves CMA-ES is optimal."
    },
    {
      "id": "EXP_EARLY_EXIT_THRESHOLD_001",
      "worker": "W1",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "early_exit",
      "note": "Same as prior failed experiments. Early exit hurts CMA-ES covariance adaptation."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_SWEEP_001",
      "worker": "W2",
      "score": 1.1656,
      "time_min": 67.4,
      "result": "FAILED",
      "family": "temporal_tuning",
      "note": "Sigma 0.18/0.22 is WORSE than 0.15/0.20. All runs over budget. Temporal tuning family EXHAUSTED."
    },
    {
      "id": "EXP_MORE_INITS_SELECT_BEST_001",
      "worker": "W2",
      "score": 1.159,
      "time_min": 66.9,
      "result": "FAILED",
      "family": "candidate_selection",
      "note": "More inits adds overhead (10-12 min) without accuracy benefit. Baseline 2-init is optimal."
    },
    {
      "id": "EXP_CORRECT_TEMPORAL_SWEEP_001",
      "worker": "W1",
      "score": 1.1641,
      "time_min": 71.2,
      "result": "FAILED",
      "family": "temporal_tuning_v2",
      "note": "40% baseline is OPTIMAL. All other fractions worse. temporal_tuning EXHAUSTED."
    },
    {
      "id": "EXP_REDUCED_CMAES_MORE_NM_001",
      "worker": "W2",
      "score": 1.1584,
      "time_min": 71.1,
      "result": "FAILED",
      "family": "budget_reallocation",
      "note": "Reducing CMA-ES fevals (15/30 vs 20/36) + more NM (10 vs 8) = over budget, worse score. Baseline budget allocation is optimal."
    },
    {
      "id": "EXP_SIGMA_LADDER_001",
      "worker": "W2",
      "score": 1.1658,
      "time_min": 70.0,
      "result": "FAILED",
      "family": "sigma_scheduling_v2",
      "note": "Sigma ladder (0.30/0.35 -> 0.15/0.20) adds overhead and interferes with CMA-ES covariance learning. sigma_scheduling EXHAUSTED."
    },
    {
      "id": "EXP_BOUNDARY_HANDLING_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "constraint_handling",
      "note": "BoundPenalty not valid pycma string option. Baseline uses BoundTransform (default). constraint_handling EXHAUSTED."
    },
    {
      "id": "EXP_TWO_SOURCE_SEQUENTIAL_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "two_source_strategy",
      "note": "Already tested as sequential_2src. Listed in 'Don't Retry': high variance."
    },
    {
      "id": "EXP_PARALLEL_NM_POLISH_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "parallelization",
      "note": "Invalid premise. NM polish is only on BEST candidate, not all candidates."
    },
    {
      "id": "EXP_NM_MULTI_START_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "polish_strategy",
      "note": "Prior evidence: 8 NM iters optimal. Multi-start would need 3-4x polish budget."
    },
    {
      "id": "EXP_RANDOM_SEED_ENSEMBLE_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "ensemble",
      "note": "Same as more_inits_select_best which FAILED. Multiple seeds = more overhead."
    },
    {
      "id": "EXP_COVARIANCE_INIT_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "cmaes_init",
      "note": "CMA-ES designed to LEARN covariance from fitness landscape. Pre-init interferes."
    },
    {
      "id": "EXP_POLISH_TOP3_001",
      "worker": "W3",
      "score": 1.0743,
      "time_min": 114.2,
      "result": "FAILED",
      "family": "polish_strategy_v2",
      "note": "Polishing 3 candidates (4 NM each) is 1.9x over budget with 8% worse score. Baseline (8 NM on best) is optimal."
    },
    {
      "id": "EXP_COORDINATE_POLISH_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "FAILED",
      "family": "polish_method_v2",
      "note": "Powell coordinate descent 5-8x slower than NM. Line search overhead prohibitive for expensive sims."
    },
    {
      "id": "EXP_SPLIT_POLISH_FINE_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "fidelity_polish",
      "note": "Same as progressive_polish_fidelity. Truncated polish overfits to proxy noise."
    },
    {
      "id": "EXP_TV_REGULARIZATION_001",
      "worker": "W3",
      "score": 1.1614,
      "time_min": 83.7,
      "result": "FAILED",
      "family": "regularization",
      "note": "TV regularization adds penalty that biases away from true RMSE optimum. 0.6% worse score, 43% over budget."
    },
    {
      "id": "EXP_NM_DIM_ADAPTIVE_001",
      "worker": "W3",
      "score": 1.165,
      "time_min": 66.5,
      "result": "FAILED",
      "family": "polish_tuning_v2",
      "note": "Scipy adaptive NM (Gao-Han 2012): -0.0038 score, +14% time. Designed for high-dim (n>10), not n=3-6. Fixed iterations override convergence benefit."
    },
    {
      "id": "EXP_ADAPTIVE_NM_COEFFICIENTS_001",
      "worker": "W2",
      "score": 1.1493,
      "time_min": 85.0,
      "result": "FAILED",
      "family": "polish_tuning",
      "note": "Adaptive NM coefficients: -0.0195 score, +45% time. Default coefficients optimal."
    },
    {
      "id": "EXP_HYBRID_RESTART_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "restart_v3",
      "note": "All polish modifications FAIL. NM x8 with defaults is OPTIMAL."
    },
    {
      "id": "EXP_ASYMMETRIC_POLISH_001",
      "worker": "W1",
      "score": 1.1639,
      "time_min": 79.3,
      "result": "FAILED",
      "family": "polish_method_v2",
      "note": "Implementation overhead prevents testing hypothesis. 8/8 config runs 35% slower than baseline. polish_method_v2 family EXHAUSTED."
    },
    {
      "id": "EXP_FINAL_POP_SELECTION_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "candidate_selection_v2",
      "note": "Baseline already implements dissimilarity filtering (tau=0.2). Experiment is redundant."
    },
    {
      "id": "EXP_MOMENT_INVERSION_001",
      "worker": "W1",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "direct_inversion_v2",
      "note": "Moment-based inversion theoretically unsound for sparse sensors. direct_inversion_v2 family EXHAUSTED."
    },
    "mini_batch_sensor_eval",
    "extended_kalman_filter_inversion",
    "ensemble_weighted_solution",
    {
      "id": "EXP_EXTENDED_VERIFICATION_001",
      "worker": "W2",
      "score": 1.1432,
      "time_min": 40.0,
      "result": "FAILED",
      "family": "verification_v3",
      "note": "Extended verification to ALL candidates gives same score (~1.1432) as verifying just best. verify_top_n=1 is optimal. Overhead not justified."
    }
  ],
  "workers": {
    "W1": {
      "focus_area": "Experiment Executor",
      "status": "active",
      "registered_at": 1769488065.2773347,
      "last_seen": 1769488065.2773347,
      "experiments_run": 0
    },
    "W2": {
      "focus_area": "Experiment Executor",
      "status": "active",
      "registered_at": 1769488065.2942352,
      "last_seen": 1769488065.2942352,
      "experiments_run": 0
    },
    "W3": {
      "status": "idle",
      "current_experiment": null,
      "last_completed": "EXP_NM_DIM_ADAPTIVE_001",
      "directive": "Queue empty. All experiments completed. Waiting for W0 to add more experiments."
    },
    "W4": {
      "status": "idle",
      "current_experiment": null,
      "last_completed": null,
      "directive": "5 NEW experiments available: polish_top3_reduced (P2), coordinate_descent_polish (P3), asymmetric_polish_budget (P3), early_polish_trigger (P4), split_polish_fidelity (P4). Claim highest priority."
    }
  },
  "research_findings": [
    "larger_candidate_pool: FAILED",
    "  - Score 1.1726 @ 85.6 min (MASSIVELY OVER BUDGET)",
    "  - Pool size 15 vs 10 - marginal score gain but +27 min overhead",
    "  - Baseline pool_size=10 is already optimal",
    "NEW EXPERIMENTS ADDED: 2 more for queue health",
    "------- W0 CYCLE 38 \u2605\u2605\u2605 SUCCESS \u2605\u2605\u2605 (2026-01-26T07:17Z) -------",
    "*** NEW BEST: solution_verification_pass = 1.1373 @ 42.6 min ***",
    "rerun_solution_verification CONFIRMS GENUINE IMPROVEMENT:",
    "  - Mean score: 1.1373 (vs old baseline 1.1246)",
    "  - Improvement: +0.0127 (+1.0%)",
    "  - Time: 42.6 min (within budget)",
    "  - Verified with 3 seeds: [1.1255, 1.1420, 1.1443]",
    "  - 2 out of 3 runs above old 95% CI (1.1342)",
    "WHY IT WORKS:",
    "  - Gradient verification catches cases where CMA-ES+NM didn't fully converge",
    "  - Small gradient descent steps push borderline solutions to better accuracy",
    "  - ~45% of samples benefit from verification",
    "RECOMMENDATION: ADOPT solution_verification_pass as production optimizer",
    "greedy_diversity_selection: FAILED",
    "  - Score 1.1499 @ 68.5 min (over budget, worse)",
    "  - Simple sort+filter is actually optimal for scoring formula",
    "  - Greedy selection sometimes picks higher-RMSE candidates for diversity",
    "2026-01-27 W0 Research: candidate_weighted_ensemble - Diversity already saturated at 2.75/3. Scoring formula penalizes diverse but worse candidates. Ensemble approaches CANNOT help.",
    "2026-01-27 W0 Research: higher_population_density - 2x population with fixed fevals = fewer generations = worse convergence (-1%). Population tuning EXHAUSTED.",
    "2026-01-27 W0 Research: improved_triangulation_init - Acoustic formulas WRONG for heat diffusion. Current triangulation already uses correct physics (r ~ sqrt(t)). Initialization family EXHAUSTED.",
    "2026-01-27 W0 Research: position_bounds_from_init - Sigma control already achieves local search. Hard bounds add failure mode without benefit. NOT_FEASIBLE.",
    "2026-01-27 W0 Research: reduced_fevals_more_polish - CMA-ES needs all fevals (continues improving >1%). NM has diminishing returns after iter 8. Budget allocation already optimal.",
    "2026-01-27 W0 Web Research: Top DFO algorithms 2025 - EA4eig, EBOwithCMAR, APGSK-IMODE, DIRMIN are state-of-art",
    "2026-01-27 W0 Web Research: LR-CMA-ES (local refinement hybrid) shows promise - synergy of evolutionary learning and self-learning",
    "2026-01-27 W0 Web Research: Tikhonov regularization with novel optimal parameter selection is state-of-art for inverse heat source",
    "2026-01-27 W2: extended_verification_all_candidates - FAILED. verify_top_n=1,2,3 all give same score (~1.1432). Verification overhead (10 min/candidate) not justified. BEST candidate verification is optimal, verifying MORE doesn't help MORE.",
    "2026-01-27 W0: extended_verification_all_candidates FAILED (1.1432 @ 40 min vs true baseline 1.1688). Verifying more candidates adds overhead without improving score. verify_top_n=1 is optimal. verification_v3 family EXHAUSTED.",
    "2026-01-27 W2: double_nm_polish_round FAILED. All configs (6+4, 4+3, 3+3 iter) WORSE than baseline. Best: 1.1278 vs 1.1373. Additional NM polish rounds hurt accuracy. Hypothesis disproved: NM doesn't get stuck in local basin. polish_v3 family EXHAUSTED.",
    "2026-01-27 W0: double_nm_polish_round FAILED (best 1.1278 @ 31.4 min). Double NM polish HURTS accuracy. 3-iteration NM is already optimal. polish_v3 family EXHAUSTED.",
    "2026-01-27 W0: intensity_refinement_only NOT_FEASIBLE. CRITICAL INSIGHT: Baseline uses Variable Projection (VarPro) - CMA-ES only optimizes position, intensity computed analytically via closed-form least-squares. This is ALREADY globally optimal for intensity. Any intensity tuning experiments are redundant."
  ],
  "research_sources": [
    "https://github.com/CMA-ES/pycma - pycma with lq-CMA-ES support",
    "https://github.com/CMA-ES/lq-cma - Official lq-CMA-ES implementation",
    "https://arxiv.org/html/2402.09638v1 - Multi-Fidelity Methods Survey",
    "https://www.sciencedirect.com/science/article/abs/pii/S0378778811000533 - Rapid PDE optimization",
    "https://www.sciencedirect.com/science/article/abs/pii/S0045782521001468 - Tutorial on adjoint method for inverse problems",
    "https://www.researchgate.net/publication/26354266 - Heat source estimation with conjugate gradient",
    "https://github.com/deepmodeling/jax-fem - JAX-FEM differentiable FEM solver",
    "https://www.sciencedirect.com/science/article/abs/pii/S0010465523001479 - JAX-FEM paper",
    "https://arxiv.org/abs/2402.11722 - Invertible FNO for forward and inverse problems",
    "https://openai.com/index/evolution-strategies/ - OpenAI Evolution Strategies",
    "https://www.sciencedirect.com/science/article/abs/pii/S0735193325002490 - Heat source field inversion with PINN (2025)",
    "https://github.com/maziarraissi/PINNs - Original PINN framework",
    "https://arxiv.org/abs/2506.14792 - Fast automated adjoints for spectral PDE solvers (2025)",
    "https://hess.copernicus.org/articles/28/3051/2024/ - Discretize-then-optimize adjoint for implicit schemes (HESS 2024)",
    "https://www.researchgate.net/publication/220739886 - BIPOP vs IPOP benchmarking (BBOB)",
    "https://www.mdpi.com/1424-8220/25/16/4984 - Gappy C-POD for thermal field reconstruction (Sensors 2025)",
    "https://www.sciencedirect.com/science/article/abs/pii/S0017931025008439 - D-PBCS for inverse heat source detection (July 2025)"
  ],
  "notes": "QUEUE EXHAUSTED. All experiments completed. Baseline 1.1688 @ 58.4 min is proven optimal for 60-min budget.",
  "research_mandate": {
    "status": "EMERGENCY_COMPLETED",
    "reason": "Queue was EMPTY. Deep research cycle conducted. 4 last-resort experiments added.",
    "research_performed": [
      "Searched: frequency domain inverse heat source identification Fourier transform 2024 2025",
      "Searched: multi-fidelity CMA-ES expensive optimization 2024 2025",
      "Searched: inverse heat source localization machine learning sensor data 2025 state of the art",
      "Searched: compressive sensing heat source reconstruction sparse measurements 2025",
      "Searched: CMA-ES restart strategy BIPOP IPOP performance comparison 2024",
      "Searched: thermal inverse problem sensor optimization observation operator 2025",
      "Searched: CMA-ES population diversity preservation multi-candidate optimization 2024 2025",
      "Searched: adjoint method implicit time stepping ADI heat equation automatic differentiation 2024 2025"
    ],
    "key_findings": [
      "Checkpointed adjoint (arXiv 2025): Memory-efficient adjoint via checkpointing can work WITH implicit ADI schemes",
      "BIPOP vs IPOP (BBOB): BIPOP alternates large/small populations, can solve problems IPOP misses",
      "Gappy C-POD (Sensors 2025): Clustering-based POD handles heterogeneous samples with varying physics",
      "D-PBCS (ScienceDirect July 2025): Physics-based compressive sensing for inverse heat source detection",
      "Discretize-then-optimize (HESS 2024): Adjoint method that works with implicit numerical schemes"
    ],
    "key_insight": "ALL major algorithm families are now EXHAUSTED. The baseline 1.1688 @ 58.4 min may be near-optimal. The 4 new experiments are theoretically grounded but have LOW probability of beating baseline. The gap to target (0.0812) may require fundamentally different problem formulation or longer time budget."
  },
  "updated_at": 1769488065.2942352
}