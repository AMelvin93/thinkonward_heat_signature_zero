{
  "version": "19.1",
  "last_updated": "2026-01-28T15:34:25.645513",
  "mode": "ACTIVE",
  "leaderboard": {
    "updated": "2026-01-28",
    "our_best": 1.1464,
    "our_rank_estimate": "~11th",
    "competition_standings": [
      {
        "rank": 1,
        "team": "Matt Motoki",
        "score": 1.23902
      },
      {
        "rank": 2,
        "team": "vellin",
        "score": 1.229405
      },
      {
        "rank": 3,
        "team": "kjc",
        "score": 1.227493
      },
      {
        "rank": 4,
        "team": "Jonas M",
        "score": 1.226773
      },
      {
        "rank": 5,
        "team": "olbap",
        "score": 1.216828
      },
      {
        "rank": 6,
        "team": "Yahu",
        "score": 1.193261
      },
      {
        "rank": 7,
        "team": "Th204c",
        "score": 1.18947
      },
      {
        "rank": 8,
        "team": "Ti41e7",
        "score": 1.174324
      },
      {
        "rank": 9,
        "team": "nacumaria00",
        "score": 1.171643
      },
      {
        "rank": 10,
        "team": "MG\u00f6ksu",
        "score": 1.158469
      },
      {
        "rank": 11,
        "team": "bobatea",
        "score": 1.129488
      }
    ],
    "gap_analysis": {
      "to_top_10": "+0.012 needed (beat 1.158)",
      "to_top_5": "+0.071 needed (beat 1.217)",
      "to_top_3": "+0.081 needed (beat 1.227)",
      "to_1st": "+0.093 needed (beat 1.239)"
    },
    "note": "TOP 10 IS WITHIN REACH - need only +0.012 improvement. Focus on accuracy, not speed."
  },
  "resume_tracking": {
    "summaries_analyzed": [
      "ensemble_voting",
      "cluster_transfer",
      "lq_cma_es_builtin",
      "bayesian_optimization_gp",
      "cmaes_to_nm_sequential",
      "multi_fidelity_pyramid",
      "fast_source_count_detection",
      "early_timestep_filtering",
      "adaptive_sample_budget",
      "warm_start_cmaes",
      "niching_cmaes_diversity",
      "extended_nm_polish",
      "temporal_40pct_higher_sigma",
      "ipop_cmaes_temporal",
      "adaptive_timestep_fraction",
      "physics_informed_init",
      "pod_reduced_order_model",
      "larger_cmaes_population",
      "two_source_specialized",
      "adaptive_sigma_schedule",
      "active_cmaes_covariance",
      "progressive_polish_fidelity",
      "boundary_aware_initialization",
      "jax_differentiable_solver",
      "levenberg_marquardt_inverse",
      "cmaes_then_gradient_refinement",
      "conjugate_gradient_adjoint",
      "openai_evolution_strategy",
      "differential_evolution",
      "variable_projection_separable",
      "adaptive_simulated_annealing",
      "pretrained_nn_surrogate",
      "pinn_inverse_heat_source",
      "early_rejection_partial_sim",
      "cmaes_restart_from_best",
      "weighted_sensor_loss",
      "multistart_elite_selection",
      "solution_injection_cmaes",
      "polar_parameterization",
      "log_rmse_loss",
      "adaptive_nm_iterations",
      "random_timestep_selection",
      "adaptive_population_size",
      "diagonal_decoding_cmaes",
      "separable_cmaes_diagonal",
      "powell_polish_instead_nm",
      "learning_rate_adapted_cmaes",
      "coordinate_wise_sigma",
      "bfgs_polish_after_cmaes",
      "cmaes_early_stopping",
      "lower_sigma_baseline",
      "checkpointed_adjoint_method",
      "gappy_clustering_pod",
      "physics_compressive_sensing",
      "bipop_cmaes_restart",
      "frequency_domain_optimization",
      "greens_function_inversion",
      "modal_identification_method",
      "informative_sensor_subset",
      "landscape_adaptive_cmaes",
      "rbf_meshless_inverse",
      "cmaes_rbf_surrogate",
      "scipy_slsqp_optimizer",
      "direct_algorithm_search",
      "temporal_fidelity_sweep",
      "more_inits_select_best",
      "correct_temporal_sweep",
      "reduced_cmaes_more_nm",
      "ica_seeded_init",
      "sigma_ladder",
      "weighted_centroid_nm",
      "boundary_handling_methods",
      "confidence_based_early_exit",
      "genetic_algorithm_optimizer",
      "growth_optimizer",
      "mae_polish_loss",
      "multi_objective_pareto",
      "parallel_nm_polish",
      "polish_top3_reduced",
      "coordinate_descent_polish",
      "split_polish_fidelity",
      "tv_regularization_sparse",
      "asymmetric_polish_budget",
      "micro_restart_polish",
      "adaptive_nm_coefficients",
      "moment_based_inversion",
      "nm_dimension_adaptive",
      "adaptive_temporal_fidelity",
      "intensity_prior_from_peak",
      "larger_popsize_exploration",
      "reduced_nm_polish_4iter",
      "sigma_restart_on_stagnation",
      "bobyqa_polish",
      "simulation_result_caching",
      "kriging_local_infill",
      "pso_then_cmaes_hybrid",
      "learned_sampling_policy",
      "condition_number_init",
      "center_spread_parameterization",
      "full_sim_reranking",
      "scaled_parameter_space",
      "laplace_domain_initialization",
      "spsa_gradient_optimizer",
      "vectorized_batch_evaluation",
      "smart_early_termination",
      "mini_batch_sensor_eval",
      "sacobra_rbf_optimizer",
      "extended_kalman_filter_inversion",
      "particle_filter_inversion",
      "variance_reduced_cmaes",
      "cmaes_quasi_newton_polish",
      "separable_position_intensity",
      "ensemble_weighted_solution",
      "parallel_cmaes_info_sharing",
      "greedy_coordinate_refinement",
      "intensity_first_then_position",
      "confidence_weighted_candidate_selection",
      "lightweight_ensemble_postprocessing",
      "adaptive_polish_per_sample",
      "cmaes_restart_inject_best",
      "median_ensemble_solution",
      "top3_ensemble_averaging",
      "multi_restart_nm_polish",
      "pinn_initialization",
      "ensemble_on_40pct_temporal_baseline",
      "simple_position_average_best2",
      "fcmaes_speed_test",
      "adei_heat_source",
      "coarse_to_fine_temporal",
      "stratified_sample_processing",
      "solution_verification_pass",
      "baseline_consistency_test",
      "larger_candidate_pool",
      "tighter_intensity_range",
      "adjusted_dissimilarity_threshold",
      "multi_seed_best_selection",
      "rerun_solution_verification",
      "greedy_diversity_selection",
      "candidate_weighted_ensemble",
      "higher_population_density",
      "improved_triangulation_init",
      "position_bounds_from_init",
      "reduced_fevals_more_polish",
      "extended_verification_all_candidates",
      "double_nm_polish_round",
      "intensity_refinement_only",
      "perturbed_local_restart",
      "reproduce_w2_best_config",
      "perturbed_extended_polish",
      "higher_temporal_45pct",
      "perturbation_plus_verification",
      "multi_round_perturbation",
      "w2_config_with_gradient_verify",
      "adaptive_perturbation_scale",
      "reduced_1src_evals",
      "tikhonov_regularized_rmse"
    ],
    "last_cycle_timestamp": "2026-01-28T13:32:00.000000",
    "notes": "W0: tikhonov_regularized_rmse analyzed - CATASTROPHIC (8x budget, worse score). Regularization incompatible with CMA-ES."
  },
  "best_scores": {
    "in_budget": {
      "score": 1.1468,
      "time_min": 54.2,
      "experiment": "perturbation_plus_verification (W2)",
      "algorithm": "CMA-ES + NM + perturbation + verification + sigma 0.18/0.22",
      "date": "2026-01-28",
      "note": "Marginal +0.0004 vs perturbed_extended_polish - may be noise"
    },
    "previous_best": {
      "score": 1.1464,
      "time_min": 51.2,
      "experiment": "perturbed_extended_polish (W1)",
      "note": "Previous best"
    },
    "simple_baseline": {
      "score": 1.1373,
      "time_min": 42.6,
      "experiment": "solution_verification_pass",
      "note": "Verified baseline with gradient verification only"
    }
  },
  "queue_status": {
    "available_experiments": 0,
    "claimed_experiments": 0,
    "status": "EMPTY",
    "note": "Queue exhausted by W1. Need W0 to add new experiments based on research."
  },
  "exploration_progress": {
    "total_experiments": 60,
    "experiments_per_family": {
      "evolutionary_cmaes": 34,
      "evolutionary_other": 3,
      "gradient_based": 4,
      "gradient_free_local": 6,
      "surrogate": 2,
      "surrogate_lq": 1,
      "ensemble": 1,
      "decomposition": 2,
      "meta_learning": 3,
      "hybrid": 1,
      "bayesian_opt": 1,
      "multi_fidelity": 1,
      "temporal_fidelity": 1,
      "budget_allocation": 1
    },
    "families_exhausted": [
      "evolutionary_cmaes",
      "gradient_based",
      "evolutionary_other",
      "ensemble",
      "decomposition",
      "meta_learning",
      "surrogate_lq",
      "bayesian_opt",
      "multi_fidelity_spatial",
      "hybrid",
      "preprocessing",
      "budget_allocation",
      "diversity",
      "temporal_fidelity_extended",
      "initialization",
      "surrogate_pod",
      "cmaes_accuracy",
      "source_specific",
      "sigma_scheduling",
      "cmaes_variants",
      "problem_specific",
      "initialization_v2",
      "loss_reformulation",
      "polish_strategy_v2",
      "polish_method_v2",
      "fidelity_polish",
      "regularization",
      "polish_tuning_v2",
      "direct_inversion_v2",
      "restart_v3",
      "nm_adaptive",
      "local_search_v3",
      "temporal_fidelity_v2",
      "physics_init_v2",
      "polish_budget_v2",
      "cmaes_tuning_v2",
      "restart_strategy_v2",
      "surrogate_v2",
      "hybrid_v2",
      "meta_v2",
      "initialization_v4",
      "engineering",
      "surrogate_optimization",
      "state_estimation",
      "evaluation_v2",
      "temporal_v4",
      "regularization_v2",
      "polish_efficiency",
      "polish_allocation",
      "multi_start_v2",
      "cmaes_efficiency"
    ],
    "families_to_explore": [
      "gradient_approx_v2 - SPSA dimension-independent gradient",
      "evaluation_v2 - Mini-batch sensor evaluation",
      "initialization_v5 - Laplace domain analysis",
      "problem_transform - Parameter scaling",
      "candidate_selection_v2 - Full-sim reranking",
      "parameterization_v2 - Center-spread for 2-source"
    ],
    "research_directions": [
      "The leaderboard top scores (1.22+) prove higher accuracy IS achievable",
      "L-BFGS-B achieved excellent accuracy but was slow due to finite differences - adjoint gradients could fix this",
      "40% temporal fidelity worked - what about frequency domain which naturally captures temporal structure?",
      "Neural operators (FNO) can learn PDE solution operators with high accuracy",
      "The problem may benefit from reformulation - different loss functions, constraints, or parameterizations"
    ]
  },
  "experiments_in_progress": {
    "EXP_COORDINATE_POLISH_001": {
      "worker": "W1",
      "started": "2026-01-25T02:29:10.536783Z",
      "name": "coordinate_descent_polish"
    },
    "EXP_NM_DIM_ADAPTIVE_001": {
      "worker": "W3",
      "started_at": "2026-01-25T03:16:34.054315Z"
    },
    "EXP_45PCT_TEMPORAL_001": {
      "worker": "W3",
      "started": "2026-01-28T02:53:08.816632Z",
      "name": "higher_temporal_45pct"
    }
  },
  "experiments_completed": [
    {
      "id": "EXP_WEIGHTED_LOSS_001",
      "worker": "W1",
      "score": 1.0131,
      "time_min": 90.7,
      "result": "FAILED",
      "family": "loss_function",
      "note": "Weighted RMSE optimization finds different optimum than unweighted (scored). Diversity destroyed."
    },
    {
      "id": "EXP_CMAES_RESTART_BEST_001",
      "worker": "W1",
      "score": 1.0986,
      "time_min": 175.7,
      "result": "FAILED",
      "family": "cmaes_improvement",
      "note": "Two-phase restart WORSE than single phase. Phase 2 doubles sims without improving accuracy. Diversity lost due to small sigma."
    },
    {
      "id": "EXP_POD_SURROGATE_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "surrogate_pod",
      "note": "POD not viable - sample-specific physics prevents universal basis. Temporal fidelity already achieves same speedup."
    },
    {
      "id": "EXP_PHYSICS_INIT_001",
      "worker": "W1",
      "score": 1.1593,
      "time_min": 71.9,
      "result": "FAILED",
      "family": "initialization",
      "note": "Gradient init WORSE than hottest-sensor (-0.0046). Temperature gradients corrupted by diffusion. Simple heuristics win."
    },
    {
      "id": "EXP_ADAPTIVE_TIMESTEP_001",
      "worker": "W2",
      "score": 1.1635,
      "time_min": 69.9,
      "result": "FAILED",
      "family": "temporal_fidelity_extended",
      "note": "Adaptive 25%->40% WORSE than fixed 40%. CMA-ES covariance needs consistent landscape. Switching fidelity mid-run is counterproductive."
    },
    {
      "id": "EXP_IPOP_TEMPORAL_001",
      "worker": "W2",
      "score": 1.1687,
      "time_min": 75.7,
      "result": "FAILED",
      "family": "temporal_fidelity_extended",
      "note": "IPOP FAILED. Splitting fevals across restarts reduces convergence. Problem doesn't have local optima - restarts don't help."
    },
    {
      "id": "EXP_TEMPORAL_HIGHER_SIGMA_001",
      "worker": "W1",
      "score": 1.1745,
      "time_min": 63.0,
      "best_in_budget_score": 1.1584,
      "best_in_budget_time": 52.1,
      "result": "FAILED",
      "family": "temporal_fidelity_extended",
      "note": "Higher sigma (0.25/0.30) +0.0057 score but +5min. Best in-budget 1.1584 is WORSE than W2 baseline 1.1688. W2's sigma 0.18/0.22 is optimal."
    },
    {
      "id": "EXP_EXTENDED_POLISH_001",
      "worker": "W2",
      "score": 1.1703,
      "time_min": 82.3,
      "result": "FAILED",
      "family": "refinement",
      "note": "12 iterations OVER BUDGET (82.3 min). +0.0015 score not worth +24 min. 8 iterations is optimal."
    },
    {
      "id": "EXP_NICHING_CMAES_001",
      "worker": "W2",
      "score": 1.0622,
      "time_min": 46.9,
      "result": "FAILED",
      "family": "diversity",
      "note": "Niching FAILED. Scoring averages accuracy over candidates - worse diverse candidates hurt. Baseline already at 2.75/3 N_valid."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_001",
      "worker": "W2",
      "score": 1.1688,
      "time_min": 58.4,
      "result": "SUCCESS",
      "family": "temporal_fidelity",
      "note": "NEW BEST! 40% timesteps + 8 NM polish (full): +0.0441 vs baseline. 2-src RMSE dropped 33%."
    },
    {
      "id": "EXP_ADAPTIVE_BUDGET_001",
      "worker": "W1",
      "score": 1.1143,
      "time_min": 56.2,
      "result": "FAILED",
      "family": "budget_allocation",
      "note": "Early termination hurts CMA-ES accuracy. Fixed-budget baseline is optimal."
    },
    {
      "id": "EXP_WS_CMAES_001",
      "worker": "W1",
      "score": 0.276,
      "time_min": 65.8,
      "result": "FAILED",
      "family": "meta_learning",
      "note": "WS-CMA-ES causes divergence. Probing wastes budget. meta_learning family EXHAUSTED."
    },
    {
      "id": "EXP_FAST_SOURCE_DETECT_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "preprocessing",
      "note": "INVALID PREMISE - n_sources already in sample data."
    },
    {
      "id": "EXP_SEQUENTIAL_HANDOFF_001",
      "worker": "W2",
      "score": 1.1132,
      "time_min": 56.6,
      "result": "FAILED",
      "family": "hybrid",
      "note": "Sequential CMA-ES to NM handoff FAILED. Best in-budget 1.1132 is WORSE than baseline."
    },
    {
      "id": "EXP_MULTIFID_OPT_001",
      "worker": "W1",
      "score": 0.259,
      "time_min": 44.9,
      "result": "FAILED",
      "family": "multi_fidelity",
      "note": "Coarse grid RMSE landscape differs from fine grid."
    },
    {
      "id": "EXP_BAYESIAN_OPT_001",
      "worker": "W1",
      "score": 0.31,
      "time_min": 57.6,
      "result": "FAILED",
      "family": "bayesian_opt",
      "note": "GP surrogate doesn't model thermal RMSE landscape."
    },
    {
      "id": "EXP_LQ_CMAES_001",
      "worker": "W1",
      "score": 0.253,
      "time_min": 64.1,
      "result": "FAILED",
      "family": "surrogate_lq",
      "note": "fmin_lq_surr API mismatch - returns ONE solution but we need MULTIPLE candidates."
    },
    {
      "id": "EXP_TRANSFER_LEARN_001",
      "worker": "W2",
      "score": 1.0804,
      "time_min": 59.7,
      "result": "FAILED",
      "family": "meta_learning",
      "note": "Cluster transfer FAILED - sensor features don't predict solution similarity."
    },
    {
      "id": "EXP_ENSEMBLE_001",
      "worker": "W1",
      "score": 1.0972,
      "time_min": 347.9,
      "result": "FAILED",
      "family": "ensemble",
      "note": "Ensemble 5.8x over budget."
    },
    {
      "id": "CMAES_TUNING_BATCH",
      "worker": "ALL",
      "score": 1.1247,
      "time_min": 57.2,
      "result": "EXHAUSTED",
      "family": "evolutionary_cmaes",
      "note": "32+ experiments. Best in-budget: 1.1247."
    },
    {
      "id": "EXP_ADAPTIVE_NM_POLISH_001",
      "worker": "W2",
      "score": 1.1607,
      "time_min": 78.3,
      "result": "FAILED",
      "family": "refinement",
      "note": "Adaptive NM iterations does NOT improve. Fixed 8 is optimal. refinement family EXHAUSTED."
    },
    {
      "id": "EXP_SOLUTION_INJECTION_001",
      "worker": "W1",
      "score": 0.9929,
      "time_min": 122.5,
      "result": "FAILED",
      "family": "population_enhancement",
      "note": "Solution injection requires SEQUENTIAL CMA-ES (to share solutions). Destroys parallelism - 2x over budget, -15% score."
    },
    {
      "id": "EXP_LOG_RMSE_LOSS_001",
      "worker": "W2",
      "score": 1.1677,
      "time_min": 70.5,
      "result": "FAILED",
      "family": "loss_reformulation",
      "note": "Log(1+RMSE) objective: same accuracy but +12 min overhead. Monotonic transformations don't help CMA-ES."
    },
    {
      "id": "EXP_RANDOM_TIMESTEPS_001",
      "worker": "W2",
      "score": null,
      "time_min": 350,
      "result": "FAILED",
      "family": "temporal_sampling",
      "note": "Random timesteps requires FULL simulation (3-4x slower). Projected 350 min (5.8x over budget). Efficiency comes from running FEWER timesteps, not DIFFERENT timesteps."
    },
    {
      "id": "EXP_SEP_CMAES_001",
      "worker": "W2",
      "score": 1.1501,
      "time_min": 312.0,
      "result": "FAILED",
      "family": "cmaes_variants",
      "note": "Diagonal covariance HURTS both accuracy (-0.0187) and time (5.3x over budget). Position correlations along heat gradient are essential. Full covariance is optimal."
    },
    {
      "id": "EXP_EARLY_STOP_CMA_001",
      "worker": "W2",
      "score": 1.1378,
      "time_min": 313.3,
      "result": "FAILED",
      "family": "efficiency",
      "note": "Early stopping NEVER triggered (0 saved fevals). CMA-ES continues improving >1% throughout its full budget. 5.4x over budget due to overhead. efficiency family EXHAUSTED."
    },
    {
      "id": "EXP_CHECKPOINTED_ADJOINT_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "FAILED",
      "family": "gradient_revisited",
      "note": "Single-timestep adjoint correct (0.5% error), but full gradient has 5x error due to chain rule through optimal q(x,y). gradient_revisited family EXHAUSTED."
    },
    {
      "id": "EXP_GAPPY_CPOD_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "surrogate_v2",
      "note": "Physics clustering works (4 clusters), BUT 100% unique sensor locations (80/80) defeats Gappy C-POD. surrogate_v2 family EXHAUSTED."
    },
    {
      "id": "EXP_PHYSICS_CS_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "inverse_method",
      "note": "Observation matrix needs 97 min/sample (budget 9 sec). 646x over budget. Coarse grids 6x over. Sample-specific sensors prevent pre-computation. inverse_method family EXHAUSTED."
    },
    {
      "id": "EXP_FREQUENCY_DOMAIN_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "frequency_domain",
      "note": "RMSE computation is 0.082ms, simulation is 1207ms (14,721x slower). Frequency domain addresses wrong bottleneck. frequency_domain family EXHAUSTED."
    },
    {
      "id": "EXP_GREENS_FUNCTION_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "direct_inversion",
      "note": "Green's function is 4.3x SLOWER than ADI (5151ms vs 1189ms). Sensor heterogeneity (100% unique) prevents pre-computation. Inversion is nonlinear. direct_inversion family EXHAUSTED."
    },
    {
      "id": "EXP_MODAL_IDENTIFICATION_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "model_reduction",
      "note": "Only 2 sensors = 2 modes max. Simulation is bottleneck (1167ms), SVD is 0.02ms. Modal space doesn't reduce simulations. model_reduction family EXHAUSTED."
    },
    {
      "id": "EXP_SENSOR_SUBSET_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "sensor_optimization",
      "note": "Subset RMSE has r=0.68 correlation with full RMSE. This is proxy optimization which failed in weighted loss (-14%). sensor_optimization family EXHAUSTED."
    },
    {
      "id": "EXP_BIPOP_CMAES_001",
      "worker": "W1",
      "score": 1.1609,
      "time_min": 54.9,
      "result": "FAILED",
      "family": "cmaes_restart_v2",
      "note": "BIPOP restarts add 8 min overhead without accuracy gain. Confirms IPOP finding: problem lacks local optima."
    },
    {
      "id": "EXP_LANDSCAPE_ADAPTIVE_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "meta_optimization",
      "note": "ELA probing (10 evals \u00d7 1 sec = 10 sec) exceeds per-sample budget (9 sec). All prior adaptive strategies failed. meta_optimization family EXHAUSTED."
    },
    {
      "id": "EXP_RBF_MESHLESS_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "meshless_direct",
      "note": "Matrix A requires n_rbf_points simulations. Even coarsest 5\u00d73=15 pts = 15 sec exceeds budget (9 sec). 100% unique sensors prevent pre-computation. Same issue as D-PBCS (646x over). meshless_direct family EXHAUSTED."
    },
    {
      "id": "EXP_CMAES_RBF_SURROGATE_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "surrogate_hybrid",
      "note": "CMA-ES covariance adaptation is already implicit surrogate modeling. ~10% rejection rate (same as early rejection), small population (6-8) mean minimal benefit (~6% sim reduction). surrogate_hybrid family EXHAUSTED."
    },
    {
      "id": "EXP_SCIPY_SLSQP_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "gradient_numerical",
      "note": "SLSQP uses finite diff gradients = same O(n) overhead as L-BFGS-B which FAILED. NM x8 is optimal polish method. gradient_numerical family EXHAUSTED."
    },
    {
      "id": "EXP_DIRECT_ALGORITHM_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "deterministic_global",
      "note": "DIRECT uses 441-443 evals vs CMA-ES 20-36 (12-22x more). Projected 236 min vs 60 min budget (4x over). deterministic_global family EXHAUSTED."
    },
    {
      "id": "EXP_GENETIC_ALGORITHM_001",
      "worker": "W1",
      "score": 1.1615,
      "time_min": 64.8,
      "result": "FAILED",
      "family": "alternative_optimizer",
      "note": "GA fundamentally inferior to CMA-ES. All runs over budget with worse scores. GA lacks CMA-ES covariance adaptation."
    },
    {
      "id": "EXP_MULTI_OBJECTIVE_001",
      "worker": "W1",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "multi_objective",
      "note": "Theoretically unsound - scoring formula is already multi-objective. NSGA-II also 6-10x over eval budget."
    },
    {
      "id": "EXP_GROWTH_OPTIMIZER_001",
      "worker": "W1",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "alternative_metaheuristic",
      "note": "Growth Optimizer lacks covariance adaptation. Pattern of failures (GA, DE, OpenAI ES, SA) proves CMA-ES is optimal."
    },
    {
      "id": "EXP_EARLY_EXIT_THRESHOLD_001",
      "worker": "W1",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "early_exit",
      "note": "Same as prior failed experiments. Early exit hurts CMA-ES covariance adaptation."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_SWEEP_001",
      "worker": "W2",
      "score": 1.1656,
      "time_min": 67.4,
      "result": "FAILED",
      "family": "temporal_tuning",
      "note": "Sigma 0.18/0.22 is WORSE than 0.15/0.20. All runs over budget. Temporal tuning family EXHAUSTED."
    },
    {
      "id": "EXP_MORE_INITS_SELECT_BEST_001",
      "worker": "W2",
      "score": 1.159,
      "time_min": 66.9,
      "result": "FAILED",
      "family": "candidate_selection",
      "note": "More inits adds overhead (10-12 min) without accuracy benefit. Baseline 2-init is optimal."
    },
    {
      "id": "EXP_CORRECT_TEMPORAL_SWEEP_001",
      "worker": "W1",
      "score": 1.1641,
      "time_min": 71.2,
      "result": "FAILED",
      "family": "temporal_tuning_v2",
      "note": "40% baseline is OPTIMAL. All other fractions worse. temporal_tuning EXHAUSTED."
    },
    {
      "id": "EXP_REDUCED_CMAES_MORE_NM_001",
      "worker": "W2",
      "score": 1.1584,
      "time_min": 71.1,
      "result": "FAILED",
      "family": "budget_reallocation",
      "note": "Reducing CMA-ES fevals (15/30 vs 20/36) + more NM (10 vs 8) = over budget, worse score. Baseline budget allocation is optimal."
    },
    {
      "id": "EXP_SIGMA_LADDER_001",
      "worker": "W2",
      "score": 1.1658,
      "time_min": 70.0,
      "result": "FAILED",
      "family": "sigma_scheduling_v2",
      "note": "Sigma ladder (0.30/0.35 -> 0.15/0.20) adds overhead and interferes with CMA-ES covariance learning. sigma_scheduling EXHAUSTED."
    },
    {
      "id": "EXP_BOUNDARY_HANDLING_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "constraint_handling",
      "note": "BoundPenalty not valid pycma string option. Baseline uses BoundTransform (default). constraint_handling EXHAUSTED."
    },
    {
      "id": "EXP_TWO_SOURCE_SEQUENTIAL_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "two_source_strategy",
      "note": "Already tested as sequential_2src. Listed in 'Don't Retry': high variance."
    },
    {
      "id": "EXP_PARALLEL_NM_POLISH_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "parallelization",
      "note": "Invalid premise. NM polish is only on BEST candidate, not all candidates."
    },
    {
      "id": "EXP_NM_MULTI_START_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "polish_strategy",
      "note": "Prior evidence: 8 NM iters optimal. Multi-start would need 3-4x polish budget."
    },
    {
      "id": "EXP_RANDOM_SEED_ENSEMBLE_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "ensemble",
      "note": "Same as more_inits_select_best which FAILED. Multiple seeds = more overhead."
    },
    {
      "id": "EXP_COVARIANCE_INIT_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "cmaes_init",
      "note": "CMA-ES designed to LEARN covariance from fitness landscape. Pre-init interferes."
    },
    {
      "id": "EXP_POLISH_TOP3_001",
      "worker": "W3",
      "score": 1.0743,
      "time_min": 114.2,
      "result": "FAILED",
      "family": "polish_strategy_v2",
      "note": "Polishing 3 candidates (4 NM each) is 1.9x over budget with 8% worse score. Baseline (8 NM on best) is optimal."
    },
    {
      "id": "EXP_COORDINATE_POLISH_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "FAILED",
      "family": "polish_method_v2",
      "note": "Powell coordinate descent 5-8x slower than NM. Line search overhead prohibitive for expensive sims."
    },
    {
      "id": "EXP_SPLIT_POLISH_FINE_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "fidelity_polish",
      "note": "Same as progressive_polish_fidelity. Truncated polish overfits to proxy noise."
    },
    {
      "id": "EXP_TV_REGULARIZATION_001",
      "worker": "W3",
      "score": 1.1614,
      "time_min": 83.7,
      "result": "FAILED",
      "family": "regularization",
      "note": "TV regularization adds penalty that biases away from true RMSE optimum. 0.6% worse score, 43% over budget."
    },
    {
      "id": "EXP_NM_DIM_ADAPTIVE_001",
      "worker": "W3",
      "score": 1.165,
      "time_min": 66.5,
      "result": "FAILED",
      "family": "polish_tuning_v2",
      "note": "Scipy adaptive NM (Gao-Han 2012): -0.0038 score, +14% time. Designed for high-dim (n>10), not n=3-6. Fixed iterations override convergence benefit."
    },
    {
      "id": "EXP_ADAPTIVE_NM_COEFFICIENTS_001",
      "worker": "W2",
      "score": 1.1493,
      "time_min": 85.0,
      "result": "FAILED",
      "family": "polish_tuning",
      "note": "Adaptive NM coefficients: -0.0195 score, +45% time. Default coefficients optimal."
    },
    {
      "id": "EXP_HYBRID_RESTART_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "restart_v3",
      "note": "All polish modifications FAIL. NM x8 with defaults is OPTIMAL."
    },
    {
      "id": "EXP_ASYMMETRIC_POLISH_001",
      "worker": "W1",
      "score": 1.1639,
      "time_min": 79.3,
      "result": "FAILED",
      "family": "polish_method_v2",
      "note": "Implementation overhead prevents testing hypothesis. 8/8 config runs 35% slower than baseline. polish_method_v2 family EXHAUSTED."
    },
    {
      "id": "EXP_FINAL_POP_SELECTION_001",
      "worker": "W2",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "candidate_selection_v2",
      "note": "Baseline already implements dissimilarity filtering (tau=0.2). Experiment is redundant."
    },
    {
      "id": "EXP_MOMENT_INVERSION_001",
      "worker": "W1",
      "score": null,
      "time_min": null,
      "result": "ABORTED",
      "family": "direct_inversion_v2",
      "note": "Moment-based inversion theoretically unsound for sparse sensors. direct_inversion_v2 family EXHAUSTED."
    },
    "mini_batch_sensor_eval",
    "extended_kalman_filter_inversion",
    "ensemble_weighted_solution",
    {
      "id": "EXP_EXTENDED_VERIFICATION_001",
      "worker": "W2",
      "score": 1.1432,
      "time_min": 40.0,
      "result": "FAILED",
      "family": "verification_v3",
      "note": "Extended verification to ALL candidates gives same score (~1.1432) as verifying just best. verify_top_n=1 is optimal. Overhead not justified."
    },
    {
      "id": "EXP_REPRODUCE_W2_BEST_001",
      "worker": "W2",
      "score": 1.1599,
      "time_min": 71.5,
      "result": "FAILED",
      "family": "baseline_verification",
      "note": "Claimed W2 baseline (1.1688 @ 58.4 min) CANNOT be reproduced. Actual: 1.1599 @ 71.5 min (22% slower). Use solution_verification_pass (1.1373 @ 42.6 min) instead."
    },
    {
      "id": "EXP_PERTURB_PLUS_VERIFY_001",
      "worker": "W2",
      "score": 1.1468,
      "time_min": 54.2,
      "result": "SUCCESS",
      "family": "hybrid_v3",
      "note": "NEW BEST - Combined perturbation + verification achieves +0.0004 vs previous best. Both techniques contribute (21% perturbed, 14% verified)"
    },
    {
      "id": "EXP_MULTI_ROUND_PERTURB_001",
      "worker": "W1",
      "score": 1.1372,
      "time_min": 50.9,
      "result": "FAILED",
      "family": "perturbation_v2",
      "note": "Multi-round perturbation WORSE than single-round. Single-round optimal."
    },
    {
      "id": "EXP_W2_CONFIG_WITH_VERIFICATION_001",
      "worker": "W1",
      "score": 1.1355,
      "time_min": 313.9,
      "result": "FAILED",
      "family": "hybrid_v2",
      "note": "W2 config + gradient verification is 5x over budget. Verification overhead prohibitive."
    },
    {
      "id": "EXP_ADAPTIVE_PERTURB_SCALE_001",
      "worker": "W1",
      "score": 1.1419,
      "time_min": 57.4,
      "result": "FAILED",
      "family": "perturbation_v2",
      "note": "Adaptive perturbation scale FAILS. Best in-budget (1.1419) is WORSE than baseline (1.1464). Fixed scale 0.05 is optimal."
    },
    {
      "id": "EXP_W2_CONFIG_WITH_VERIFICATION_001",
      "worker": "W?",
      "score": null,
      "time_min": null,
      "result": "FAILED",
      "family": "verification_v2",
      "note": "5-6x over budget"
    },
    {
      "id": "EXP_ADAPTIVE_PERTURB_SCALE_001",
      "worker": "W?",
      "score": 1.1419,
      "time_min": 57.4,
      "result": "FAILED",
      "family": "perturbation_v2",
      "note": "Worse than baseline"
    },
    {
      "id": "EXP_REDUCED_1SRC_EVALS_001",
      "worker": "W1",
      "score": 1.1398,
      "time_min": 49.0,
      "result": "FAILED",
      "family": "efficiency_v2",
      "note": "Reducing 1-src fevals (15 vs 20) hurts accuracy. Best in-budget 1.1398 is WORSE than baseline 1.1464."
    },
    {
      "id": "EXP_REDUCED_1SRC_EVALS_001",
      "worker": "W?",
      "score": 1.1398,
      "time_min": 49.0,
      "result": "FAILED",
      "family": "efficiency_v2",
      "note": "1-source needs full 20 fevals"
    },
    {
      "id": "EXP_TIKHONOV_REGULARIZED_001",
      "worker": "W1",
      "score": 1.1388,
      "time_min": 491.2,
      "result": "FAILED",
      "family": "regularization_v2",
      "note": "Tikhonov regularization CATASTROPHIC - 8x over budget (491.2 min), -0.0076 score. Regularization penalty fundamentally breaks CMA-ES optimization."
    },
    {
      "id": "EXP_EARLY_STOP_POLISH_001",
      "worker": "W1",
      "score": 1.1203,
      "time_min": 445.6,
      "result": "FAILED",
      "family": "polish_efficiency",
      "note": "Per-iteration NM has ~10x overhead vs single call. 90% early stopped but 7.4x over budget. Fundamental flaw."
    }
  ],
  "workers": {
    "W1": {
      "focus_area": "Experiment Executor",
      "status": "active",
      "registered_at": 1769488065.2773347,
      "last_seen": 1769488065.2773347,
      "experiments_run": 0
    },
    "W2": {
      "focus_area": "Experiment Executor",
      "status": "active",
      "registered_at": 1769488065.2942352,
      "last_seen": 1769488065.2942352,
      "experiments_run": 0
    },
    "W3": {
      "status": "active",
      "current_experiment": "EXP_45PCT_TEMPORAL_001",
      "last_started": "2026-01-28T02:53:08.816667Z"
    },
    "W4": {
      "status": "idle",
      "current_experiment": null,
      "last_completed": null,
      "directive": "5 NEW experiments available: polish_top3_reduced (P2), coordinate_descent_polish (P3), asymmetric_polish_budget (P3), early_polish_trigger (P4), split_polish_fidelity (P4). Claim highest priority."
    }
  },
  "research_findings": [
    "larger_candidate_pool: FAILED",
    "  - Score 1.1726 @ 85.6 min (MASSIVELY OVER BUDGET)",
    "  - Pool size 15 vs 10 - marginal score gain but +27 min overhead",
    "  - Baseline pool_size=10 is already optimal",
    "NEW EXPERIMENTS ADDED: 2 more for queue health",
    "------- W0 CYCLE 38 \u2605\u2605\u2605 SUCCESS \u2605\u2605\u2605 (2026-01-26T07:17Z) -------",
    "*** NEW BEST: solution_verification_pass = 1.1373 @ 42.6 min ***",
    "rerun_solution_verification CONFIRMS GENUINE IMPROVEMENT:",
    "  - Mean score: 1.1373 (vs old baseline 1.1246)",
    "  - Improvement: +0.0127 (+1.0%)",
    "  - Time: 42.6 min (within budget)",
    "  - Verified with 3 seeds: [1.1255, 1.1420, 1.1443]",
    "  - 2 out of 3 runs above old 95% CI (1.1342)",
    "WHY IT WORKS:",
    "  - Gradient verification catches cases where CMA-ES+NM didn't fully converge",
    "  - Small gradient descent steps push borderline solutions to better accuracy",
    "  - ~45% of samples benefit from verification",
    "RECOMMENDATION: ADOPT solution_verification_pass as production optimizer",
    "greedy_diversity_selection: FAILED",
    "  - Score 1.1499 @ 68.5 min (over budget, worse)",
    "  - Simple sort+filter is actually optimal for scoring formula",
    "  - Greedy selection sometimes picks higher-RMSE candidates for diversity",
    "2026-01-27 W0 Research: candidate_weighted_ensemble - Diversity already saturated at 2.75/3. Scoring formula penalizes diverse but worse candidates. Ensemble approaches CANNOT help.",
    "2026-01-27 W0 Research: higher_population_density - 2x population with fixed fevals = fewer generations = worse convergence (-1%). Population tuning EXHAUSTED.",
    "2026-01-27 W0 Research: improved_triangulation_init - Acoustic formulas WRONG for heat diffusion. Current triangulation already uses correct physics (r ~ sqrt(t)). Initialization family EXHAUSTED.",
    "2026-01-27 W0 Research: position_bounds_from_init - Sigma control already achieves local search. Hard bounds add failure mode without benefit. NOT_FEASIBLE.",
    "2026-01-27 W0 Research: reduced_fevals_more_polish - CMA-ES needs all fevals (continues improving >1%). NM has diminishing returns after iter 8. Budget allocation already optimal.",
    "2026-01-27 W0 Web Research: Top DFO algorithms 2025 - EA4eig, EBOwithCMAR, APGSK-IMODE, DIRMIN are state-of-art",
    "2026-01-27 W0 Web Research: LR-CMA-ES (local refinement hybrid) shows promise - synergy of evolutionary learning and self-learning",
    "2026-01-27 W0 Web Research: Tikhonov regularization with novel optimal parameter selection is state-of-art for inverse heat source",
    "2026-01-27 W2: extended_verification_all_candidates - FAILED. verify_top_n=1,2,3 all give same score (~1.1432). Verification overhead (10 min/candidate) not justified. BEST candidate verification is optimal, verifying MORE doesn't help MORE.",
    "2026-01-27 W0: extended_verification_all_candidates FAILED (1.1432 @ 40 min vs true baseline 1.1688). Verifying more candidates adds overhead without improving score. verify_top_n=1 is optimal. verification_v3 family EXHAUSTED.",
    "2026-01-27 W2: double_nm_polish_round FAILED. All configs (6+4, 4+3, 3+3 iter) WORSE than baseline. Best: 1.1278 vs 1.1373. Additional NM polish rounds hurt accuracy. Hypothesis disproved: NM doesn't get stuck in local basin. polish_v3 family EXHAUSTED.",
    "2026-01-27 W0: double_nm_polish_round FAILED (best 1.1278 @ 31.4 min). Double NM polish HURTS accuracy. 3-iteration NM is already optimal. polish_v3 family EXHAUSTED.",
    "2026-01-27 W0: intensity_refinement_only NOT_FEASIBLE. CRITICAL INSIGHT: Baseline uses Variable Projection (VarPro) - CMA-ES only optimizes position, intensity computed analytically via closed-form least-squares. This is ALREADY globally optimal for intensity. Any intensity tuning experiments are redundant.",
    "------- W0 CYCLE 39 \u2605\u2605\u2605 NEW BEST \u2605\u2605\u2605 (2026-01-28T00:45Z) -------",
    "*** perturbed_local_restart: 1.1452 @ 47.7 min (+0.0079 vs 1.1373) ***",
    "KEY INSIGHT: Basin hopping works! Perturbing top candidate and re-optimizing escapes local minima in ~25% of samples.",
    "Optimal config: perturb_top_n=1, n_perturbations=2, perturbation_scale=0.05, perturb_nm_iters=3",
    "Critical bug fix: Use coarse grid (50x25) for perturbation NM, not fine grid (100x50) - 4x speedup",
    "NEXT: Try combining perturbation with W2's 40% temporal config (1.1688) - could stack improvements",
    "2026-01-28 W2: reproduce_w2_best_config FAILED. Claimed W2 baseline (1.1688 @ 58.4 min) CANNOT be reproduced. Actual results: sigma 0.18/0.22 = 1.1599 @ 71.5 min, sigma 0.15/0.20 = 1.1559 @ 71.1 min. Both 22% slower than claimed. RECOMMENDATION: Use solution_verification_pass (1.1373 @ 42.6 min) as reliable baseline.",
    "------- W0 CYCLE 43 \u2605\u2605\u2605 CRITICAL FINDING \u2605\u2605\u2605 (2026-01-28T01:40Z) -------",
    "*** CRITICAL: W2's 1.1688 @ 58.4 min CANNOT BE REPRODUCED ***",
    "Verification results:",
    "  - sigma 0.18/0.22: 1.1599 @ 71.5 min (19% over budget)",
    "  - sigma 0.15/0.20: 1.1559 @ 71.1 min (19% over budget)",
    "IMPACT: The TRUE best in-budget score is perturbed_local_restart (1.1452 @ 47.7 min)",
    "Possible causes: different machine, different code path, or incorrect original measurement",
    "RECOMMENDATION: Use solution_verification_pass (1.1373 @ 42.6 min) or perturbed_local_restart (1.1452 @ 47.7 min) as reliable baselines",
    "------- W0 CYCLE 44 \u2605\u2605\u2605 NEW BEST \u2605\u2605\u2605 (2026-01-28T01:50Z) -------",
    "*** perturbed_extended_polish: 1.1464 @ 51.2 min (+0.0012 vs 1.1452) ***",
    "KEY INSIGHT: Higher sigma (0.18/0.22) + perturbation is the winning combination",
    "Optimal config: sigma 0.18/0.22, 8 NM polish, perturb_top_n=1, n_perturbations=2",
    "More polish (10 vs 8) HURTS score - overfitting to coarse grid",
    "More perturbations (3 vs 2) add time without benefit",
    "Budget utilization: 85% (8.8 min remaining)",
    "------- W3 CYCLE: perturbation_plus_verification -------",
    "*** NEW BEST: perturbation_plus_verification = 1.1468 @ 54.2 min ***",
    "Combined approach works: 21% samples improved by perturbation, 14% by verification",
    "Improvement: +0.0004 vs perturbed_extended_polish (1.1464)",
    "------- W0 CYCLE 4 PROACTIVE RESEARCH (2026-01-28T02:56Z) -------",
    "Queue was LOW (4 available). Added 2 perturbation experiments:",
    "  - adaptive_perturbation_scale: Scale perturbation based on solution quality",
    "  - multi_round_perturbation: Two rounds of perturb \u2192 optimize",
    "Web research: LR-CMA-ES (local refinement), KbP-LaF-CMAES (knowledge-based perturbation)",
    "------- W0 CYCLE 5 COMPLETION ANALYSIS (2026-01-28T03:06Z) -------",
    "*** higher_temporal_45pct: FAILED (1.1443 @ 51.3 min) ***",
    "  - 45% temporal: 1.1335 (WORSE - higher temporal adds noise)",
    "  - 35% temporal: 1.1443 (comparable to 40%)",
    "  - 40% temporal: 1.1430 (confirms optimal)",
    "  \u2192 CONCLUSION: temporal_v4 family EXHAUSTED. 40% is optimal.",
    "------- W0 CYCLE 6 DEEP RESEARCH (2026-01-28T05:10Z) -------",
    "*** perturbation_plus_verification: MARGINAL (1.1468 @ 54.2 min) ***",
    "  - +0.0004 vs baseline 1.1464 - not worth the extra 3 min",
    "  - KEY INSIGHT: Perturbation (21%) and verification (14%) help DIFFERENT samples",
    "  - But the improvements DON'T COMPOUND - each mechanism provides small per-sample gains",
    "  - Combined approach helps 35% of samples but net gain is only 0.03%",
    "  - RECOMMENDATION: Use perturbed_extended_polish (1.1464 @ 51.2 min) as production",
    "WEB RESEARCH CONDUCTED:",
    "  - Tikhonov regularization with GCV automatic parameter selection (Preprints July 2025)",
    "  - MODE/CMA-ES (April 2025): 61% reduction in evaluations, 77% efficiency gain - no public impl yet",
    "  - NEWUOA 5x faster than CMA-ES on smooth problems, but BOBYQA already tested (NM optimal)",
    "  - iFNO/SC-FNO for inverse problems: 440x speedup but training cost prohibitive per-sample",
    "  - Distributed Tikhonov with component-wise penalties (Springer March 2025)",
    "QUEUE STATUS: HEALTHY (6 available)",
    "  - tikhonov_regularized_rmse [P3] aligns with research on automatic regularization",
    "  - Keep perturbation experiments as they're the current winning approach",
    "------- W0 CYCLE 12 (2026-01-28T06:47Z) -------",
    "multi_round_perturbation: FAILED",
    "  - Best in-budget: 1.1372 @ 50.9 min (WORSE than 1.1468 baseline)",
    "  - Multi-round perturbation adds overhead without improving accuracy",
    "  - Single-round perturbation is sufficient and optimal",
    "  - perturbation_v2 family EXHAUSTED",
    "------- W0 CYCLE 26 (2026-01-28T10:25Z) -------",
    "w2_config_with_gradient_verify: FAILED",
    "  - All runs 5-6x over budget (309-392 min)",
    "  - Gradient verification adds ~5x overhead without accuracy benefit",
    "  - verification_v2 family EXHAUSTED",
    "adaptive_perturbation_scale: FAILED",
    "  - Best in-budget: 1.1419 @ 57.4 min (WORSE than baseline 1.1464)",
    "  - RMSE-based scaling doesn't predict optimization difficulty",
    "  - Fixed perturbation scale (0.05) is optimal",
    "  - perturbation_v2 family EXHAUSTED",
    "QUEUE STATUS: LOW (3 available) - Need proactive research",
    "PROACTIVE RESEARCH: Added 2 experiments to restore queue:",
    "  - position_only_perturbation [P2]: Only perturb position, keep VarPro-optimal intensity",
    "  - early_stop_polish_convergence [P2]: Early-stop NM when converged, reallocate budget",
    "Queue restored from 3 to 5 available",
    "------- W0 CYCLE 32 (2026-01-28T11:55Z) -------",
    "reduced_1src_evals: FAILED",
    "  - Best in-budget: 1.1398 @ 49.0 min (WORSE than baseline 1.1464)",
    "  - 1-source problems need full 20 fevals, not fewer",
    "  - Baseline 20/36 fevals is already optimized",
    "  - efficiency_v2 family showing diminishing returns",
    "------- W1 CYCLE: tikhonov_regularized_rmse -------",
    "tikhonov_regularized_rmse: CATASTROPHIC FAILURE",
    "  - Score: 1.1388 (WORSE than baseline 1.1464)",
    "  - Time: 491.2 min (8x over 60 min budget)",
    "  - Regularization toward triangulation prior biases away from true optimum",
    "  - CMA-ES covariance adaptation is implicit surrogate - explicit regularization interferes",
    "  - regularization_v2 family EXHAUSTED",
    "------- W0 CYCLE 36 (2026-01-28T13:32Z) -------",
    "QUEUE RESTORED: Was 4 available (LOW), now 6 available (HEALTHY)",
    "NEW EXPERIMENTS ADDED:",
    "  - best_of_2_seeds [P2]: Run 2 CMA-ES with different seeds, pick best before polish",
    "  - multi_candidate_perturbation [P2]: Perturb top 2 candidates instead of just top 1",
    "WEB RESEARCH CONDUCTED:",
    "  - HR-CMA-ES (history-assisted restart): Uses BSP tree to identify regions of interest",
    "  - KbP-LaF-CMAES: Knowledge-based perturbation with Leaders and Followers strategy",
    "  - B2B inverse neural operator: Decouples function representation from inverse map (too expensive per-sample)",
    "  - Spatio-temporal regularization for heat inverse problems (but regularization already proven to fail)",
    "WORKERS STATUS:",
    "  - W1: claimed position_only_perturbation",
    "  - W2: claimed perturbation_with_40pct_temporal",
    "  - W3/W4: idle (6 experiments available)",
    "------- W1 CYCLE: early_stop_polish_convergence -------",
    "early_stop_polish_convergence: FAILED",
    "  - 90% of samples triggered early stopping (3.46 iters saved avg)",
    "  - BUT: 7.4x over budget (445.6 min vs 60 min)",
    "  - FUNDAMENTAL FLAW: scipy minimize with maxiter=1 has ~10x init overhead",
    "  - Fixed 8 NM iterations is OPTIMAL",
    "  - polish_efficiency family EXHAUSTED",
    "------- W1 CYCLE: asymmetric_nm_iterations -------",
    "asymmetric_nm_iterations: INCONCLUSIVE",
    "  - Hypothesis: 2-src problems need more NM polish than 1-src",
    "  - Run 2 (6/10): Score 1.1479, 1-src RMSE=0.137, 2-src RMSE=0.216",
    "  - Run 3 (8/8): Score 1.1355, 1-src RMSE=0.140, 2-src RMSE=0.203",
    "  - Asymmetric scored BETTER but 2-src RMSE was WORSE with more iterations",
    "  - All runs over budget on this slow machine (140-164 min)",
    "  - CONCLUSION: Fixed 8 NM iterations remains optimal",
    "  - polish_allocation family EXHAUSTED",
    "------- W1 CYCLE: best_of_2_seeds -------",
    "best_of_2_seeds: FAILED",
    "  - Score: 1.1162 (vs baseline 1.1468, delta -0.0306)",
    "  - Time: 133.9 min (2.2x over budget)",
    "  - Sample 7 had catastrophic RMSE=0.5374",
    "  - ROOT CAUSE: Budget splitting prevents CMA-ES covariance learning",
    "  - multi_start_v2 family EXHAUSTED",
    "------- W1 CYCLE: multi_candidate_perturbation -------",
    "multi_candidate_perturbation: INCONCLUSIVE",
    "  - Score: 1.1526 (vs baseline 1.1468, delta +0.0058)",
    "  - Time: 150.4 min (2.5x over budget)",
    "  - Shows promise: outperforms multi-seed (1.1526 vs 1.1162)",
    "  - Sample 7 improved from 0.5374 to 0.3242",
    "  - Cannot validate production viability due to slow machine",
    "------- W1 CYCLE: sigma_fine_tune_around_w2 -------",
    "sigma_fine_tune_around_w2: INCONCLUSIVE",
    "  - Baseline sigma 0.18/0.22: Score 1.1505 @ 148.8 min (2.5x over budget)",
    "  - Machine too slow for production-relevant conclusions",
    "  - Prior evidence already established sigma 0.18/0.22 as optimal",
    "------- W1 SESSION SUMMARY -------",
    "All experiments ran over budget on this slow machine (2-2.5x slower than baseline):",
    "  - asymmetric_nm_iterations: INCONCLUSIVE (142.9 min)",
    "  - best_of_2_seeds: FAILED (133.9 min)",
    "  - multi_candidate_perturbation: INCONCLUSIVE (150.4 min, shows promise)",
    "  - sigma_fine_tune_around_w2: INCONCLUSIVE (148.8 min)",
    "RECOMMENDATION: Use faster machine for further experiments",
    "------- W1 CYCLE: population_warmup -------",
    "population_warmup: ABORTED",
    "  - Prior evidence (sigma_ladder) shows sigma scheduling interferes with CMA-ES",
    "  - This experiment would repeat a known failure",
    "  - cmaes_efficiency family EXHAUSTED",
    "------- W1 SESSION COMPLETE -------",
    "QUEUE EXHAUSTED - No more experiments available",
    "W1 completed 4 experiments + 1 aborted:",
    "  1. asymmetric_nm_iterations: INCONCLUSIVE (machine too slow)",
    "  2. best_of_2_seeds: FAILED (multi-seed hurts CMA-ES)",
    "  3. multi_candidate_perturbation: INCONCLUSIVE (shows promise, needs validation)",
    "  4. sigma_fine_tune_around_w2: INCONCLUSIVE (machine too slow)",
    "  5. population_warmup: ABORTED (sigma scheduling known to fail)",
    "FAMILIES EXHAUSTED THIS SESSION: polish_allocation, multi_start_v2, cmaes_efficiency",
    "RECOMMENDATION: Need W0 proactive research to add new experiments to queue"
  ],
  "research_sources": [
    "https://github.com/CMA-ES/pycma - pycma with lq-CMA-ES support",
    "https://github.com/CMA-ES/lq-cma - Official lq-CMA-ES implementation",
    "https://arxiv.org/html/2402.09638v1 - Multi-Fidelity Methods Survey",
    "https://www.sciencedirect.com/science/article/abs/pii/S0378778811000533 - Rapid PDE optimization",
    "https://www.sciencedirect.com/science/article/abs/pii/S0045782521001468 - Tutorial on adjoint method for inverse problems",
    "https://www.researchgate.net/publication/26354266 - Heat source estimation with conjugate gradient",
    "https://github.com/deepmodeling/jax-fem - JAX-FEM differentiable FEM solver",
    "https://www.sciencedirect.com/science/article/abs/pii/S0010465523001479 - JAX-FEM paper",
    "https://arxiv.org/abs/2402.11722 - Invertible FNO for forward and inverse problems",
    "https://openai.com/index/evolution-strategies/ - OpenAI Evolution Strategies",
    "https://www.sciencedirect.com/science/article/abs/pii/S0735193325002490 - Heat source field inversion with PINN (2025)",
    "https://github.com/maziarraissi/PINNs - Original PINN framework",
    "https://arxiv.org/abs/2506.14792 - Fast automated adjoints for spectral PDE solvers (2025)",
    "https://hess.copernicus.org/articles/28/3051/2024/ - Discretize-then-optimize adjoint for implicit schemes (HESS 2024)",
    "https://www.researchgate.net/publication/220739886 - BIPOP vs IPOP benchmarking (BBOB)",
    "https://www.mdpi.com/1424-8220/25/16/4984 - Gappy C-POD for thermal field reconstruction (Sensors 2025)",
    "https://www.sciencedirect.com/science/article/abs/pii/S0017931025008439 - D-PBCS for inverse heat source detection (July 2025)"
  ],
  "notes": "NEW BEST: perturbed_extended_polish 1.1464 @ 51.2 min. Higher sigma + perturbation is optimal.",
  "research_mandate": {
    "status": "EMERGENCY_COMPLETED",
    "reason": "Queue was EMPTY. Deep research cycle conducted. 4 last-resort experiments added.",
    "research_performed": [
      "Searched: frequency domain inverse heat source identification Fourier transform 2024 2025",
      "Searched: multi-fidelity CMA-ES expensive optimization 2024 2025",
      "Searched: inverse heat source localization machine learning sensor data 2025 state of the art",
      "Searched: compressive sensing heat source reconstruction sparse measurements 2025",
      "Searched: CMA-ES restart strategy BIPOP IPOP performance comparison 2024",
      "Searched: thermal inverse problem sensor optimization observation operator 2025",
      "Searched: CMA-ES population diversity preservation multi-candidate optimization 2024 2025",
      "Searched: adjoint method implicit time stepping ADI heat equation automatic differentiation 2024 2025"
    ],
    "key_findings": [
      "Checkpointed adjoint (arXiv 2025): Memory-efficient adjoint via checkpointing can work WITH implicit ADI schemes",
      "BIPOP vs IPOP (BBOB): BIPOP alternates large/small populations, can solve problems IPOP misses",
      "Gappy C-POD (Sensors 2025): Clustering-based POD handles heterogeneous samples with varying physics",
      "D-PBCS (ScienceDirect July 2025): Physics-based compressive sensing for inverse heat source detection",
      "Discretize-then-optimize (HESS 2024): Adjoint method that works with implicit numerical schemes"
    ],
    "key_insight": "ALL major algorithm families are now EXHAUSTED. The baseline 1.1688 @ 58.4 min may be near-optimal. The 4 new experiments are theoretically grounded but have LOW probability of beating baseline. The gap to target (0.0812) may require fundamentally different problem formulation or longer time budget."
  },
  "updated_at": 1769488065.2942352
}