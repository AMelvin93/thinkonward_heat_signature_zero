{
  "version": "4.0",
  "last_updated": "2026-01-19T02:45:00Z",
  "notes": "W1 completed EXP_WS_CMAES_001 (FAILED). WS-CMA-ES causes divergence, 62-170% worse accuracy. meta_learning family EXHAUSTED.",

  "experiment_queue": [
    {
      "id": "EXP_LQ_CMAES_001",
      "priority": 99,
      "family": "surrogate_lq",
      "status": "completed",
      "result": "FAILED - API mismatch. fmin_lq_surr returns ONE solution, we need MULTIPLE candidates. 29-71% worse RMSE, 7-17% slower.",
      "score": 0.253,
      "time_min": 64.1,
      "worker": "W1",
      "name": "lq_cma_es_builtin",
      "description": "Use pycma's built-in lq-CMA-ES (linear-quadratic surrogate model)",
      "finding": "fmin_lq_surr is designed for single-objective returning ONE best solution. Our scoring needs MULTIPLE diverse candidates. Fundamental mismatch."
    },
    {
      "id": "EXP_SEQUENTIAL_HANDOFF_001",
      "priority": 99,
      "family": "hybrid",
      "status": "completed",
      "result": "FAILED - Best in-budget 1.1132 is WORSE than baseline 1.1247. Best overall 1.1439 @ 126.5 min (2x over budget).",
      "score": 1.1132,
      "time_min": 56.6,
      "worker": "W2",
      "name": "cmaes_to_nm_sequential",
      "description": "CMA-ES exploration then hand off best to Nelder-Mead run",
      "finding": "Sequential handoff fundamentally doesn't work. NM improves score (+0.0192) when given time, but time overhead is prohibitive. When reduced to fit budget, score drops BELOW baseline. CMA-ES already uses full time budget."
    },
    {
      "id": "EXP_BAYESIAN_OPT_001",
      "priority": 99,
      "family": "bayesian_opt",
      "status": "completed",
      "result": "FAILED - GP surrogate doesn't model thermal RMSE landscape. 31-91% worse accuracy, or 45% over budget if tuned.",
      "score": 0.31,
      "time_min": 57.6,
      "worker": "W1",
      "name": "bayesian_optimization_gp",
      "description": "Use Bayesian Optimization with Gaussian Process surrogate as alternative to CMA-ES",
      "finding": "Bayesian Optimization is NOT suitable for thermal inverse problem. GP surrogate fails to capture the complex RMSE landscape. CMA-ES's covariance adaptation is essential."
    },
    {
      "id": "EXP_MULTIFID_OPT_001",
      "priority": 99,
      "family": "multi_fidelity",
      "status": "completed",
      "result": "FAILED - Coarse grid RMSE landscape differs from fine grid. Best in-budget: RMSE 0.259 @ 45 min (45% worse than baseline).",
      "score": 0.259,
      "time_min": 44.9,
      "worker": "W1",
      "name": "multi_fidelity_proper_ratio",
      "actual_folder": "multi_fidelity_pyramid",
      "description": "Multi-fidelity optimization with literature-recommended grid ratios",
      "finding": "Multi-fidelity via grid coarsening doesn't work for inverse problems. RMSE landscape is fundamentally different at different resolutions. Solutions don't transfer."
    },
    {
      "id": "EXP_FAST_SOURCE_DETECT_001",
      "priority": 99,
      "family": "preprocessing",
      "status": "completed",
      "result": "ABORTED - Invalid premise. n_sources already in sample data. Detection accuracy only 67% (target 95%).",
      "worker": "W2",
      "name": "fast_source_count_detection",
      "description": "Fast detection of 1-source vs 2-source BEFORE optimization",
      "finding": "Baseline already uses sample['n_sources']. Detection not needed. Sensor-based features give only 67% accuracy, far below 95% target."
    },
    {
      "id": "EXP_WS_CMAES_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - WS-CMA-ES causes divergence. Best: RMSE 0.276 @ 66 min (62% worse than baseline). Probing wastes budget.",
      "score": 0.276,
      "time_min": 65.8,
      "worker": "W1",
      "name": "warm_start_cmaes",
      "description": "Use CyberAgentAILab's WS-CMA-ES to transfer solutions between similar samples",
      "finding": "WS-CMA-ES doesn't work for thermal inverse problems. Each sample is unique with no shared optimization landscape structure. Probing phase wastes budget. meta_learning family EXHAUSTED."
    },
    {
      "id": "EXP_TRANSFER_LEARN_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - Sensor feature clustering doesn't predict solution similarity",
      "score": 1.0804,
      "time_min": 59.7,
      "worker": "W2",
      "name": "sample_clustering_transfer",
      "actual_folder": "cluster_transfer",
      "description": "Cluster similar samples, transfer solutions between them",
      "finding": "Clustering by sensor features doesn't predict solution similarity. Best in-budget score 1.0804 is 4% below baseline. Transfer learning fundamentally doesn't work for this problem."
    },
    {
      "id": "EXP_POD_SURROGATE_001",
      "priority": 3,
      "family": "surrogate_pod",
      "status": "paused_by_W2",
      "paused_reason": "Feasibility confirmed (10-20 modes needed), but coefficient mapping is complex. Pivoting to higher-priority EXP_TEMPORAL_FIDELITY_001.",
      "claimed_at": "2026-01-19T02:35:00Z",
      "name": "pod_reduced_order_model",
      "description": "Use POD (Proper Orthogonal Decomposition) as fast surrogate for simulation",
      "hypothesis": "POD-based reduced order model can be 70-100x faster than full simulation",
      "research_source": "ResearchGate: 'POD-based inverse algorithms more than 83 times faster than CFD'",
      "why_different": "POD learns optimal basis functions from simulation snapshots, preserving accuracy while reducing DOFs",
      "implementation": [
        "1. Collect snapshots from existing CMA-ES runs (T(x,y) fields for various source configs)",
        "2. Compute SVD/POD to extract dominant modes (5-10 modes should capture 99% energy)",
        "3. Build POD surrogate: project any new source config to get approximate T field",
        "4. Use POD for CMA-ES candidate filtering (like NN surrogate but physics-based)",
        "5. Only full-simulate candidates with low POD RMSE",
        "6. Compare speed and accuracy"
      ],
      "success_criteria": "Score >= 1.12 AND time <= 40 min (40% speedup)",
      "abort_criteria": "POD fails to capture 2-source thermal fields accurately",
      "complexity": "HIGH - requires collecting and processing snapshot data"
    },
    {
      "id": "EXP_ADAPTIVE_BUDGET_001",
      "priority": 1,
      "family": "budget_allocation",
      "status": "available",
      "name": "adaptive_sample_budget",
      "description": "Dynamically allocate fevals per sample based on convergence speed",
      "hypothesis": "Some samples converge faster than others. Redistributing budget from easy to hard samples improves overall score without increasing total time.",
      "research_source": "W0 observation: Baseline uses fixed fevals (15/28) but some samples need fewer iterations to converge. Early convergence detection could free budget for harder samples.",
      "why_different": "Unlike other approaches, this doesn't change CMA-ES or add overhead. It just redistributes existing budget more efficiently.",
      "implementation": [
        "1. Track CMA-ES convergence metrics per sample (sigma shrinkage, fitness stagnation)",
        "2. If sample converges early (sigma < threshold OR fitness stable for N generations), terminate",
        "3. Bank saved fevals into a 'bonus pool'",
        "4. For samples with high initial RMSE or slow convergence, draw from bonus pool",
        "5. Use two-pass approach: first pass identifies easy/hard samples, second pass reallocates",
        "6. Alternative: single-pass with running average to predict difficulty"
      ],
      "success_criteria": "Score >= 1.13 AND time <= 57 min (same budget, better allocation)",
      "abort_criteria": "Early termination hurts more than bonus helps"
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_001",
      "priority": 1,
      "family": "temporal_fidelity",
      "status": "claimed_by_W2",
      "claimed_at": "2026-01-19T03:00:00Z",
      "name": "early_timestep_filtering",
      "description": "Use reduced timesteps (25-50%) for candidate filtering, same spatial grid",
      "hypothesis": "Simulating first 25-50% of timesteps maintains spatial optimization landscape while reducing simulation cost by 2-4x",
      "research_source": "W0 deep research 2026-01-19: Multi-fidelity via spatial coarsening FAILED because landscape changes. Temporal fidelity maintains spatial structure.",
      "why_different": "Unlike spatial coarsening (which failed), temporal reduction keeps 100x50 grid intact. RMSE landscape at t=50% should correlate with t=100%.",
      "implementation": [
        "1. For CMA-ES candidate evaluation: simulate only first 25-50% of timesteps",
        "2. Compute RMSE over available timesteps (truncated time series)",
        "3. Use truncated RMSE for CMA-ES population ranking",
        "4. For final candidates: run full simulation (100% timesteps) to get accurate RMSE",
        "5. Test correlation between truncated and full RMSE",
        "6. Tune timestep fraction (25%, 50%, 75%) for best speed/accuracy tradeoff"
      ],
      "success_criteria": "Score >= 1.12 AND time <= 45 min (20% speedup)",
      "abort_criteria": "Truncated RMSE ranking poorly correlated with full RMSE (<0.7 correlation)"
    },
    {
      "id": "EXP_COARSE_SURROGATE_001",
      "priority": 99,
      "family": "surrogate",
      "status": "deprioritized",
      "name": "coarse_grid_as_surrogate",
      "description": "Use 40x20 coarse grid as cheap surrogate for candidate filtering",
      "note": "DEPRIORITIZED - Multi-fidelity via spatial coarsening PROVEN to fail (EXP_MULTIFID_OPT_001). Different resolution = different landscape.",
      "hypothesis": "40x20 grid is 6x faster than 100x50; use to filter before fine evaluation",
      "implementation": [
        "1. For each CMA-ES generation: evaluate all candidates on 40x20 grid",
        "2. Filter to top 50% by coarse RMSE",
        "3. Evaluate filtered candidates on 50x25 grid",
        "4. Use 50x25 results for CMA-ES update",
        "5. Final evaluation on 100x50"
      ],
      "success_criteria": "Score >= 1.11 AND time <= 50 min",
      "abort_criteria": "Coarse filtering removes good candidates consistently"
    },
    {
      "id": "EXP_ENSEMBLE_001",
      "priority": 99,
      "family": "ensemble",
      "status": "completed",
      "result": "FAILED - 347.9 min (5.8x over budget), score 1.0972 (WORSE than baseline 1.1247)",
      "score": 1.0972,
      "time_min": 347.9,
      "worker": "W1",
      "name": "optimizer_ensemble_voting",
      "description": "Run multiple optimizers in parallel, take best result per sample",
      "finding": "Nelder-Mead won 74% but running both optimizers doubles simulation count. Key: REDUCE simulations, not add optimizers."
    },
    {
      "id": "EXP_SURROGATE_NN_001",
      "priority": 99,
      "family": "surrogate",
      "status": "completed",
      "result": "FAILED - Online learning doesn't work with parallel processing",
      "score": 1.1203,
      "time_min": 75.6,
      "worker": "W1",
      "name": "neural_network_surrogate_prefilter"
    },
    {
      "id": "EXP_COBYLA_REFINE_001",
      "priority": 99,
      "family": "gradient_free_local",
      "status": "completed",
      "result": "FAILED - 88.8 min over budget, COBYLA uses 20+ extra evals vs NM's 3-6",
      "score": 1.1235,
      "time_min": 88.8,
      "worker": "W2",
      "name": "cobyla_trust_region_refinement"
    },
    {
      "id": "EXP_FAST_ICA_001",
      "priority": 99,
      "family": "decomposition",
      "status": "completed",
      "result": "FAILED - 151.1 min projected (91 min over budget), coarse-to-fine adds overhead",
      "score": 1.1046,
      "time_min": 151.1,
      "worker": "W2",
      "name": "accelerated_ica_decomposition",
      "finding": "Coarse-to-fine adds massive overhead. ICA itself is fast but extra refinement stage doubles simulation count."
    }
  ],

  "algorithm_families": {
    "evolutionary_cmaes": "EXHAUSTED - 34 experiments, no more tuning",
    "evolutionary_other": "PSO failed, may try genetic algorithms",
    "gradient_based": "ABANDONED - finite differences too slow (adjoint method could change this)",
    "gradient_free_local": "Nelder-Mead OK for refinement, COBYLA too slow",
    "surrogate": "Custom NN failed. lq-CMA-ES FAILED (API mismatch). Try POD",
    "surrogate_lq": "FAILED - fmin_lq_surr returns ONE solution, we need MULTIPLE candidates for diversity scoring",
    "surrogate_pod": "IN PROGRESS (W2) - POD-based reduced order model. HIGH potential but complex",
    "decomposition": "ABANDONED - Fast ICA failed due to refinement overhead",
    "ensemble": "ABANDONED - 5.8x over budget, worse score. Fundamentally unworkable.",
    "hybrid": "FAILED - Sequential handoff doesn't fit in budget. NM adds value but time cost prohibitive.",
    "bayesian_opt": "FAILED - GP surrogate doesn't model thermal RMSE landscape. 31-91% worse accuracy than CMA-ES.",
    "multi_fidelity": "FAILED - Coarse grid (SPATIAL) RMSE landscape differs from fine grid. Solutions don't transfer.",
    "temporal_fidelity": "NEW - Use fewer TIMESTEPS (not coarser grid) for candidate filtering. W0 research 2026-01-19.",
    "meta_learning": "EXHAUSTED - Both cluster_transfer and WS-CMA-ES FAILED. No shared landscape structure between samples.",
    "preprocessing": "ABORTED - n_sources already in sample data. Detection not needed."
  },

  "completed_experiments": [
    "CMAES_TUNING_BATCH",
    "EXP_PSO_001",
    "EXP_BASINHOPPING_001",
    "EXP_DIFFEVO_001",
    "EXP_NM_MULTISTART_001",
    "EXP_HYBRID_TWOSTAGE_001",
    "EXP_ULTRACOARSE_001",
    "EXP_ADAPTIVE_SIGMA_001",
    "EXP_OUTLIER_FOCUS_001",
    "EXP_TARGETED_2SRC_001",
    "EXP_SURROGATE_NN_001",
    "EXP_COBYLA_REFINE_001",
    "EXP_FAST_ICA_001",
    "EXP_ENSEMBLE_001",
    "EXP_TRANSFER_LEARN_001",
    "EXP_LQ_CMAES_001",
    "EXP_BAYESIAN_OPT_001",
    "EXP_MULTIFID_OPT_001",
    "EXP_SEQUENTIAL_HANDOFF_001",
    "EXP_FAST_SOURCE_DETECT_001",
    "EXP_WS_CMAES_001"
  ]
}
