{
  "version": "7.0",
  "last_updated": "2026-01-25T03:29:43.328165Z",
  "notes": "V7.3: W0 DEEP RESEARCH 2026-01-25T03:07. 3 polish experiments FAILED. Added 3 new experiments.",
  "queue_status": "OK",
  "experiment_queue": [
    {
      "id": "EXP_LQ_CMAES_001",
      "priority": 99,
      "family": "surrogate_lq",
      "status": "completed",
      "result": "FAILED - API mismatch. fmin_lq_surr returns ONE solution, we need MULTIPLE candidates. 29-71% worse RMSE, 7-17% slower.",
      "score": 0.253,
      "time_min": 64.1,
      "worker": "W1",
      "name": "lq_cma_es_builtin",
      "description": "Use pycma's built-in lq-CMA-ES (linear-quadratic surrogate model)",
      "finding": "fmin_lq_surr is designed for single-objective returning ONE best solution. Our scoring needs MULTIPLE diverse candidates. Fundamental mismatch."
    },
    {
      "id": "EXP_SEQUENTIAL_HANDOFF_001",
      "priority": 99,
      "family": "hybrid",
      "status": "completed",
      "result": "FAILED - Best in-budget 1.1132 is WORSE than baseline 1.1247. Best overall 1.1439 @ 126.5 min (2x over budget).",
      "score": 1.1132,
      "time_min": 56.6,
      "worker": "W2",
      "name": "cmaes_to_nm_sequential",
      "description": "CMA-ES exploration then hand off best to Nelder-Mead run",
      "finding": "Sequential handoff fundamentally doesn't work. NM improves score (+0.0192) when given time, but time overhead is prohibitive."
    },
    {
      "id": "EXP_BAYESIAN_OPT_001",
      "priority": 99,
      "family": "bayesian_opt",
      "status": "completed",
      "result": "FAILED - GP surrogate doesn't model thermal RMSE landscape. 31-91% worse accuracy, or 45% over budget if tuned.",
      "score": 0.31,
      "time_min": 57.6,
      "worker": "W1",
      "name": "bayesian_optimization_gp",
      "description": "Use Bayesian Optimization with Gaussian Process surrogate as alternative to CMA-ES",
      "finding": "Bayesian Optimization is NOT suitable for thermal inverse problem. GP surrogate fails to capture the complex RMSE landscape."
    },
    {
      "id": "EXP_MULTIFID_OPT_001",
      "priority": 99,
      "family": "multi_fidelity",
      "status": "completed",
      "result": "FAILED - Coarse grid RMSE landscape differs from fine grid. Best in-budget: RMSE 0.259 @ 45 min (45% worse than baseline).",
      "score": 0.259,
      "time_min": 44.9,
      "worker": "W1",
      "name": "multi_fidelity_proper_ratio",
      "actual_folder": "multi_fidelity_pyramid",
      "description": "Multi-fidelity optimization with literature-recommended grid ratios",
      "finding": "Multi-fidelity via grid coarsening doesn't work for inverse problems. RMSE landscape is fundamentally different at different resolutions."
    },
    {
      "id": "EXP_FAST_SOURCE_DETECT_001",
      "priority": 99,
      "family": "preprocessing",
      "status": "completed",
      "result": "ABORTED - Invalid premise. n_sources already in sample data.",
      "worker": "W2",
      "name": "fast_source_count_detection",
      "description": "Fast detection of 1-source vs 2-source BEFORE optimization",
      "finding": "Baseline already uses sample['n_sources']. Detection not needed."
    },
    {
      "id": "EXP_WS_CMAES_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - WS-CMA-ES causes divergence. Best: RMSE 0.276 @ 66 min (62% worse than baseline).",
      "score": 0.276,
      "time_min": 65.8,
      "worker": "W1",
      "name": "warm_start_cmaes",
      "description": "Use CyberAgentAILab's WS-CMA-ES to transfer solutions between similar samples",
      "finding": "WS-CMA-ES doesn't work for thermal inverse problems. Each sample is unique with no shared optimization landscape structure."
    },
    {
      "id": "EXP_TRANSFER_LEARN_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - Sensor feature clustering doesn't predict solution similarity",
      "score": 1.0804,
      "time_min": 59.7,
      "worker": "W2",
      "name": "sample_clustering_transfer",
      "actual_folder": "cluster_transfer",
      "description": "Cluster similar samples, transfer solutions between them",
      "finding": "Clustering by sensor features doesn't predict solution similarity. Transfer learning doesn't work for this problem."
    },
    {
      "id": "EXP_ADAPTIVE_BUDGET_001",
      "priority": 99,
      "family": "budget_allocation",
      "status": "completed",
      "result": "FAILED - Early termination hurts accuracy. Best in-budget: 1.1143 @ 56 min (1% worse than baseline).",
      "score": 1.1143,
      "time_min": 56.2,
      "worker": "W1",
      "name": "adaptive_sample_budget",
      "description": "Dynamically allocate fevals per sample based on convergence speed",
      "finding": "Early termination based on sigma/stagnation hurts accuracy. CMA-ES needs full budget."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity",
      "status": "completed",
      "result": "SUCCESS - NEW BEST! Score 1.1362 @ 39 min (+0.0115 vs baseline, 32% faster).",
      "score": 1.1362,
      "time_min": 39.0,
      "worker": "W2",
      "name": "early_timestep_filtering",
      "description": "Use reduced timesteps (40%) for candidate filtering, same spatial grid",
      "finding": "40% timesteps is optimal. Maintains 100x50 spatial grid, RMSE correlation 0.95+. Counterintuitive: more fevals HURT."
    },
    {
      "id": "EXP_TEMPORAL_HIGHER_SIGMA_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Higher sigma increases time disproportionately. Cannot beat W2 baseline within budget.",
      "score": 1.1745,
      "time_min": 63.0,
      "best_in_budget_score": 1.1584,
      "best_in_budget_time": 52.1,
      "worker": "W1",
      "name": "temporal_40pct_higher_sigma",
      "description": "Combine 40% temporal fidelity with higher sigma (0.25/0.30) for better accuracy",
      "finding": "Higher sigma (0.25/0.30) improves score by +0.0057 but adds 5+ min. Reducing polish to stay in budget loses the accuracy gain. W2's sigma 0.18/0.22 is optimal."
    },
    {
      "id": "EXP_NICHING_CMAES_001",
      "priority": 99,
      "family": "diversity",
      "status": "completed",
      "result": "FAILED - Scoring formula AVERAGES accuracy over candidates. Adding worse diverse candidates hurts score. Baseline already at 2.75/3 N_valid.",
      "score": 1.0622,
      "time_min": 46.9,
      "worker": "W2",
      "name": "niching_cmaes_diversity",
      "description": "Use Niching CMA-ES to maintain population diversity for multiple candidate solutions",
      "finding": "Diversity is NOT the bottleneck. Baseline already achieves 80% 3-candidate samples. Taboo-based niching hurts accuracy more than diversity helps."
    },
    {
      "id": "EXP_IPOP_TEMPORAL_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - IPOP adds time overhead (75.7 min) without improving accuracy. Splitting fevals across restarts reduces convergence.",
      "score": 1.1687,
      "time_min": 75.7,
      "worker": "W2",
      "name": "ipop_cmaes_temporal",
      "description": "IPOP-CMA-ES (increasing population) with 40% temporal fidelity",
      "finding": "IPOP divides feval budget across restarts, each restart gets insufficient fevals. Problem doesn't have local optima - restarts don't help."
    },
    {
      "id": "EXP_PHYSICS_INIT_001",
      "priority": 99,
      "family": "initialization",
      "status": "completed",
      "result": "FAILED - Gradient init is WORSE than smart init: -0.0046 score, +2.3 min",
      "score": 1.1639,
      "time_min": 69.6,
      "worker": "W1",
      "name": "physics_informed_init",
      "description": "Use sensor temperature gradients to initialize source location estimates",
      "finding": "Temperature gradients at sensors don't accurately point to sources. Heat diffusion corrupts gradient signal. Simple hottest-sensor init is already optimal."
    },
    {
      "id": "EXP_ADAPTIVE_TIMESTEP_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Adaptive timestep switching during CMA-ES is counterproductive. Best: 1.1635 @ 69.9 min (both worse than baseline).",
      "score": 1.1635,
      "time_min": 69.9,
      "worker": "W2",
      "name": "adaptive_timestep_fraction",
      "description": "Start with 25% timesteps, increase to 40% as CMA-ES converges",
      "finding": "CMA-ES covariance adaptation requires consistent fitness landscape. Switching fidelity mid-run disrupts learning and leads to worse performance."
    },
    {
      "id": "EXP_POD_SURROGATE_001",
      "priority": 99,
      "family": "surrogate_pod",
      "status": "completed",
      "result": "ABORTED - POD not viable for this problem due to sample-specific physics (kappa varies). Can't pre-build universal basis.",
      "worker": "W2",
      "name": "pod_reduced_order_model",
      "description": "Use POD (Proper Orthogonal Decomposition) as fast surrogate for simulation",
      "finding": "Each sample has unique physics (kappa, bc, T0). POD requires pre-computed snapshots from SAME system. Online POD would need simulations, defeating purpose. Temporal fidelity already provides similar speedup."
    },
    {
      "id": "EXP_EXTENDED_POLISH_001",
      "priority": 99,
      "family": "refinement",
      "status": "completed",
      "result": "FAILED - 12 iterations takes 82.3 min (37% over budget) for only +0.0015 score. 8 iterations is optimal.",
      "score": 1.1703,
      "time_min": 82.3,
      "worker": "W2",
      "name": "extended_nm_polish",
      "description": "Increase NM polish iterations from 8 to 12-16 for better accuracy",
      "finding": "More polish iterations exceeds time budget. Each additional iteration adds ~6 min on 80 samples. 8 is already optimal."
    },
    {
      "id": "EXP_LARGER_POPSIZE_001",
      "priority": 99,
      "family": "cmaes_accuracy",
      "status": "completed",
      "result": "FAILED - Popsize=12 adds 14 min without improving accuracy. Default popsize is optimal.",
      "score": 1.1666,
      "time_min": 73.0,
      "worker": "W1",
      "name": "larger_cmaes_population",
      "description": "Use larger CMA-ES population size for better accuracy in inverse problem",
      "finding": "Larger popsize reduces generations with fixed feval budget. CMA-ES needs multiple generations to adapt covariance. Default popsize (4+3*ln(n)) is already optimal."
    },
    {
      "id": "EXP_2SOURCE_FOCUS_001",
      "priority": 99,
      "family": "source_specific",
      "status": "completed",
      "result": "FAILED - Specialized feval allocation hurts performance. Reducing 1-src hurts accuracy, increasing 2-src doesn't help.",
      "score": 1.162,
      "time_min": 69.3,
      "worker": "W2",
      "name": "two_source_specialized",
      "description": "Specialized optimization for 2-source samples (60% of data, higher RMSE)",
      "finding": "Baseline 20/36 feval split is already optimal. 2-source is harder due to 4D search space, not under-optimization. More fevals don't help."
    },
    {
      "id": "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
      "priority": 99,
      "family": "sigma_scheduling",
      "status": "completed",
      "result": "ABORTED - Prior evidence shows this approach will fail. CMA-ES already adapts sigma naturally.",
      "worker": "W1",
      "name": "adaptive_sigma_schedule",
      "description": "Start with higher sigma (0.25/0.30), decay to lower sigma (0.18/0.22) during optimization",
      "finding": "EXP_TEMPORAL_HIGHER_SIGMA_001 showed high sigma adds time without in-budget accuracy gain. EXP_ADAPTIVE_TIMESTEP_001 showed changing conditions mid-run disrupts CMA-ES. No point testing a known-to-fail approach."
    },
    {
      "id": "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "ABORTED - Prior experiment showed truncated polish HURTS accuracy. 40% polish gave -0.0346 score. 60% would similarly hurt.",
      "worker": "W2",
      "name": "progressive_polish_fidelity",
      "description": "Use progressive timestep fidelity during NM polish: 60% early, 100% final",
      "finding": "NM polish requires full timesteps for accurate refinement. Truncated polish 'overfits to proxy noise'. Full timestep polish is optimal."
    },
    {
      "id": "EXP_ACTIVE_CMAES_001",
      "priority": 99,
      "family": "cmaes_variants",
      "status": "completed",
      "result": "ABORTED - Wrong premise. CMA_active already defaults to True in pycma. Baseline already uses active CMA-ES.",
      "worker": "W2",
      "name": "active_cmaes_covariance",
      "description": "Use Active CMA-ES variant with enhanced negative covariance update",
      "finding": "pycma's CMA_active option defaults to True. The baseline already uses active covariance update. No experiment needed."
    },
    {
      "id": "EXP_PARAMETER_SCALING_001",
      "priority": 99,
      "family": "problem_specific",
      "status": "completed",
      "result": "ABORTED - Wrong premise. CMA-ES only optimizes positions (x,y). Intensity (q) is computed analytically via least squares. No scaling needed.",
      "worker": "W1",
      "name": "weighted_parameter_scaling",
      "description": "Different scaling/weights for position (x,y) vs intensity (q) parameters",
      "finding": "Optimizer already separates position (CMA-ES) from intensity (analytical). CMA-ES only sees (x,y) or (x1,y1,x2,y2), not q."
    },
    {
      "id": "EXP_BOUNDARY_AWARE_INIT_001",
      "priority": 99,
      "family": "initialization_v2",
      "status": "completed",
      "result": "ABORTED - Data analysis shows 24% of samples have boundary hotspots. Biasing away would hurt these cases.",
      "worker": "W1",
      "name": "boundary_aware_initialization",
      "description": "Initialize heat sources away from domain boundaries where physics differs",
      "finding": "24% of samples have hottest sensor near boundary (<10% margin). Abort criteria met: biasing away hurts boundary source cases. initialization_v2 family should be marked EXHAUSTED."
    },
    {
      "id": "EXP_COARSE_SURROGATE_001",
      "priority": 99,
      "family": "surrogate",
      "status": "deprioritized",
      "name": "coarse_grid_as_surrogate",
      "description": "Use 40x20 coarse grid as cheap surrogate for candidate filtering",
      "note": "DEPRIORITIZED - Multi-fidelity via spatial coarsening PROVEN to fail (EXP_MULTIFID_OPT_001). Different resolution = different landscape."
    },
    {
      "id": "EXP_ENSEMBLE_001",
      "priority": 99,
      "family": "ensemble",
      "status": "completed",
      "result": "FAILED - 347.9 min (5.8x over budget), score 1.0972 (WORSE than baseline 1.1247)",
      "score": 1.0972,
      "time_min": 347.9,
      "worker": "W1",
      "name": "optimizer_ensemble_voting",
      "description": "Run multiple optimizers in parallel, take best result per sample",
      "finding": "Nelder-Mead won 74% but running both optimizers doubles simulation count. Key: REDUCE simulations, not add optimizers."
    },
    {
      "id": "EXP_SURROGATE_NN_001",
      "priority": 99,
      "family": "surrogate",
      "status": "completed",
      "result": "FAILED - Online learning doesn't work with parallel processing",
      "score": 1.1203,
      "time_min": 75.6,
      "worker": "W1",
      "name": "neural_network_surrogate_prefilter"
    },
    {
      "id": "EXP_COBYLA_REFINE_001",
      "priority": 99,
      "family": "gradient_free_local",
      "status": "completed",
      "result": "FAILED - 88.8 min over budget, COBYLA uses 20+ extra evals vs NM's 3-6",
      "score": 1.1235,
      "time_min": 88.8,
      "worker": "W2",
      "name": "cobyla_trust_region_refinement"
    },
    {
      "id": "EXP_FAST_ICA_001",
      "priority": 99,
      "family": "decomposition",
      "status": "completed",
      "result": "FAILED - 151.1 min projected (91 min over budget), coarse-to-fine adds overhead",
      "score": 1.1046,
      "time_min": 151.1,
      "worker": "W2",
      "name": "accelerated_ica_decomposition",
      "finding": "Coarse-to-fine adds massive overhead. ICA itself is fast but extra refinement stage doubles simulation count."
    },
    {
      "id": "EXP_CONJUGATE_GRADIENT_001",
      "priority": 99,
      "family": "adjoint_gradient",
      "status": "completed",
      "result": "FAILED - Adjoint gradient is 5-6 orders of magnitude too small. Manual derivation errors cause L-BFGS-B to see near-zero gradient and not iterate.",
      "score": 0.9006,
      "time_min": 56.5,
      "worker": "W2",
      "name": "conjugate_gradient_adjoint",
      "description": "Conjugate Gradient Method with adjoint equation for O(1) gradient computation",
      "finding": "Gradient verification: adjoint=[-0.000017, 0.000004], finite_diff=[-9.079, 1.980]. Ratio ~0.000002. Manual adjoint derivation for ADI time-stepping is error-prone. Use JAX autodiff instead."
    },
    {
      "id": "EXP_JAX_AUTODIFF_001",
      "priority": 99,
      "family": "differentiable_simulation",
      "status": "completed",
      "result": "ABORTED - Fundamental incompatibility. Explicit Euler stability constraint requires 4x more timesteps than implicit ADI.",
      "worker": "W1",
      "name": "jax_differentiable_solver",
      "description": "Rewrite thermal simulator in JAX for automatic differentiation",
      "finding": "JAX autodiff requires explicit time-stepping. Stability constraint dt < 0.002061 vs original dt=0.004 means 4x more timesteps needed. Running fewer timesteps gives wrong physics (5.7x temperature error). Implicit ADI is essential for efficiency."
    },
    {
      "id": "EXP_HYBRID_CMAES_LBFGSB_001",
      "priority": 99,
      "family": "hybrid_gradient",
      "status": "completed",
      "result": "FAILED - L-BFGS-B polish NOT better than NM polish. Finite diff overhead (O(n) extra sims/iter) outweighs gradient advantage. Best in-budget L-BFGS-B: 1.1174 @ 42 min (WORSE than NM x8: 1.1415 @ 38 min).",
      "score": 1.1174,
      "time_min": 42.4,
      "worker": "W1",
      "name": "cmaes_then_gradient_refinement",
      "description": "Use CMA-ES for global search, then L-BFGS-B with finite differences for final polish",
      "finding": "L-BFGS-B finite diff gradient requires O(n) extra sims per iteration. For 4D (2-src): ~250-350 sims vs NM's ~180. NM x8 is optimal polish method. hybrid_gradient family FAILED."
    },
    {
      "id": "EXP_OPENAI_ES_001",
      "priority": 99,
      "family": "alternative_es",
      "status": "completed",
      "result": "FAILED - OpenAI ES cannot beat CMA-ES. Diagonal covariance loses correlation information. Best: 1.1204 @ 43.3 min (-0.0158 score, +4.3 min vs baseline).",
      "score": 1.1204,
      "time_min": 43.3,
      "worker": "W1",
      "name": "openai_evolution_strategy",
      "description": "Try OpenAI's Evolution Strategy as alternative to CMA-ES",
      "finding": "OpenAI ES's diagonal covariance approximation loses critical parameter correlations that CMA-ES captures. For low-dim expensive black-box optimization, CMA-ES is optimal. alternative_es family FAILED."
    },
    {
      "id": "EXP_PRETRAINED_SURROGATE_001",
      "priority": 99,
      "family": "neural_operator",
      "status": "completed",
      "name": "pretrained_nn_surrogate",
      "description": "Train neural network on many simulations OFFLINE, then use as fast surrogate",
      "hypothesis": "Previous NN surrogate failed due to online learning + parallel issues. Pre-training OFFLINE on 10K+ simulations creates a fixed surrogate that works with parallel processing.",
      "research_source": "FNO inverse problems paper (AISTATS 2025). Neural operators can learn PDE solution operators with high accuracy.",
      "why_different": "Previous approach: online learning during optimization (failed). New approach: offline pre-training, fixed surrogate during optimization.",
      "implementation": [
        "1. Generate training data: 10K random source configs -> RMSE values",
        "2. Train small MLP: (x, y, [x2, y2]) -> predicted_RMSE",
        "3. Use trained surrogate for candidate filtering in CMA-ES",
        "4. Only full-simulate candidates with low surrogate RMSE",
        "5. Compare total sims needed vs baseline"
      ],
      "success_criteria": "Reduce simulations by 50%+ while maintaining score >= 1.16",
      "abort_criteria": "Surrogate accuracy too poor or training data generation too expensive",
      "result": "ABORTED - RMSE landscape is sample-specific (avg correlation -0.167). Universal surrogate cannot predict RMSE without sample information.",
      "worker": "W2",
      "finding": "Tested RMSE landscape correlation across 5 samples. Average Spearman r = -0.167 (anti-correlated). Each sample has unique physics (kappa, bc, Y_obs). Surrogate needs sample info, defeating purpose."
    },
    {
      "id": "EXP_PINN_DIRECT_001",
      "priority": 99,
      "family": "neural_operator",
      "status": "completed",
      "name": "pinn_inverse_heat_source",
      "description": "Use Physics-Informed Neural Network to directly solve the inverse heat source problem",
      "hypothesis": "PINNs can directly optimize for heat source parameters by encoding physics constraints in the loss. Recent 2025 paper specifically addresses heat source field inversion.",
      "research_source": "Heat source field inversion with PINN (ScienceDirect 2025). ASME Journal review on PINNs for heat transfer.",
      "why_different": "Direct approach: NN takes sensor readings, outputs source parameters. No iterative optimization loop.",
      "implementation": [
        "1. Define PINN architecture: input=sensor_temps, output=source_params",
        "2. Loss = data_mismatch + physics_residual (heat equation)",
        "3. Train on sensor observations to find source parameters",
        "4. Use PyTorch or TensorFlow with autodiff",
        "5. Compare accuracy and inference speed"
      ],
      "success_criteria": "Score >= 1.15 AND inference time < 1 sec per sample",
      "abort_criteria": "PINN requires too much training time per sample",
      "result": "ABORTED - PINN requires efficient gradients. Adjoint failed (1e-6 error), JAX failed (stability), finite diff too slow (99 min vs 60 budget).",
      "worker": "W2",
      "finding": "L-BFGS-B achieves 15% better RMSE than NM but takes 2.3x longer due to finite diff. ADI solver blocks autodiff. Gradient-free methods (CMA-ES + NM) remain optimal."
    },
    {
      "id": "EXP_LEVENBERG_MARQUARDT_001",
      "priority": 99,
      "family": "nonlinear_least_squares",
      "status": "completed",
      "result": "FAILED - LM is local optimizer that gets stuck in local minima. Finite diff Jacobian needs 3-5 sims/iter. Best in-budget: 1.0350 @ 56 min (-9% vs baseline).",
      "score": 1.035,
      "time_min": 56.0,
      "worker": "W1",
      "name": "levenberg_marquardt_inverse",
      "description": "Use Levenberg-Marquardt algorithm via scipy.optimize.least_squares for inverse heat problem",
      "finding": "LM fundamentally unsuitable: (1) local optimizer vs multi-modal landscape, (2) expensive finite diff Jacobian (3-5 sims/iter), (3) CMA-ES is 3x more sample-efficient. nonlinear_least_squares family should be marked FAILED."
    },
    {
      "id": "EXP_TRUST_REGION_REFLECTIVE_001",
      "priority": 99,
      "family": "nonlinear_least_squares",
      "status": "deprioritized",
      "name": "trust_region_reflective_bounded",
      "description": "Use Trust-Region Reflective method for bounded inverse heat problem",
      "deprioritize_reason": "DEPRIORITIZED based on EXP_LEVENBERG_MARQUARDT_001 results. TRF is also a local optimizer like LM - will have same local minima issues on multi-modal RMSE landscape. Same expensive finite-diff Jacobian problem. Not worth testing.",
      "original_hypothesis": "TRF handles bounded constraints naturally (source positions have box constraints). Our x,y are bounded to domain [0,Lx]x[0,Ly]. TRF respects bounds without penalty methods, potentially more accurate."
    },
    {
      "id": "EXP_SEPARABLE_VP_001",
      "priority": 99,
      "family": "variable_projection",
      "status": "completed",
      "result": "FAILED - VP's Gauss-Newton is local optimizer that gets stuck. Baseline already uses VP implicitly (analytical q). Best: 1.0025 @ 54.3 min (-0.1337 score).",
      "score": 1.0025,
      "time_min": 54.3,
      "worker": "W1",
      "name": "variable_projection_separable",
      "description": "Exploit separable structure via Variable Projection: optimize positions, solve intensity analytically",
      "finding": "Baseline already uses VP (analytical q for each CMA-ES candidate). Gauss-Newton on positions gets stuck in local minima. CMA-ES + analytical q is optimal for exploiting separability."
    },
    {
      "id": "EXP_ADAPTIVE_SA_001",
      "priority": 99,
      "family": "simulated_annealing",
      "status": "completed",
      "name": "adaptive_simulated_annealing",
      "description": "Use Adaptive Simulated Annealing (ASA) for heat source inverse problem",
      "hypothesis": "2024 Nature paper shows ASA successfully reconstructs internal heat sources in biological tissue. ASA adaptively adjusts temperature schedule based on search state, improving global optimization for complex inverse problems.",
      "research_source": "Noninvasive reconstruction of internal heat source in biological tissue using ASA (Nature Scientific Reports, July 2024). ASA avoids complex regularization by treating inverse problem as optimization.",
      "why_different": "SA is a global optimizer like CMA-ES but with different exploration mechanism (probabilistic uphill moves). ASA adapts cooling schedule dynamically. May find different optima than CMA-ES. Very different from local methods (LM) that failed.",
      "implementation": [
        "1. Use scipy.optimize.dual_annealing (generalized SA with local search)",
        "2. Set bounds for source positions",
        "3. Use same objective function as baseline (RMSE after q optimization)",
        "4. Compare global search efficiency vs CMA-ES",
        "5. If promising, implement full ASA with adaptive cooling"
      ],
      "success_criteria": "Score >= 1.16 AND time <= 55 min",
      "abort_criteria": "SA converges slower than CMA-ES OR gets stuck in same local minima",
      "result": "FAILED - dual_annealing is fundamentally unsuitable. With local search: 2-5x over budget. Without: 23% worse accuracy.",
      "score": 0.8666,
      "time_min": 28.2,
      "worker": "W2",
      "finding": "SA local search uses L-BFGS-B finite differences = slow. Pure SA = fast but inaccurate. CMA-ES covariance adaptation is more sample-efficient."
    },
    {
      "id": "EXP_SURROGATE_CMAES_001",
      "priority": 99,
      "family": "surrogate_assisted",
      "status": "completed",
      "name": "surrogate_assisted_cmaes",
      "description": "Use kriging/GP surrogate to pre-filter CMA-ES candidates, reducing simulation count",
      "hypothesis": "SCR algorithm (2025) shows surrogate-assisted CMA-ES works well for expensive black-box functions. Build kriging surrogate from initial samples, use it to filter unpromising candidates before full simulation.",
      "research_source": "SCR: Surrogate-CMAES-RQLIF for expensive black-box optimization (Optimization and Engineering, March 2025). Automotive crashworthiness optimization with surrogate-based hyperparameter tuning.",
      "why_different": "Previous surrogate (NN online) failed due to parallel processing. This approach: (1) builds surrogate from existing samples, (2) uses kriging not NN, (3) only filters obviously bad candidates. More conservative than full surrogate replacement.",
      "implementation": [
        "1. Run baseline CMA-ES on first 10 samples, collect (params, RMSE) pairs",
        "2. Fit kriging/GP model to collected data",
        "3. For remaining samples: use surrogate to filter CMA-ES population",
        "4. Only simulate candidates with low predicted RMSE (top 50%)",
        "5. Update surrogate periodically with new data"
      ],
      "success_criteria": "Reduce simulations by 30%+ while maintaining score >= 1.15",
      "abort_criteria": "Surrogate filtering removes good candidates OR overhead exceeds savings",
      "result": "ABORTED - Prior finding: RMSE landscapes are sample-specific (avg correlation -0.167). Kriging surrogate from samples 1-10 will NOT generalize to samples 11-80.",
      "worker": "W2",
      "finding": "Based on EXP_PRETRAINED_SURROGATE_001 results: RMSE = f(x, y, Y_obs, kappa, bc) is sample-specific. Surrogate needs sample info to work, defeating the purpose."
    },
    {
      "id": "EXP_DIFFERENTIAL_EVOLUTION_001",
      "priority": 99,
      "family": "differential_evolution",
      "status": "completed",
      "result": "FAILED - DE cannot match CMA-ES accuracy. Best: 1.1325 @ 35.2 min (-0.0037 score, -3.8 min vs baseline). CMA-ES covariance adaptation is essential.",
      "score": 1.1325,
      "time_min": 35.2,
      "worker": "W1",
      "name": "differential_evolution_inverse",
      "description": "Use scipy.optimize.differential_evolution for inverse heat problem",
      "finding": "DE's mutation/crossover is less efficient than CMA-ES's covariance adaptation. DE is 10% faster but sacrifices accuracy. differential_evolution family FAILED.",
      "success_criteria": "Score >= 1.16 AND time <= 55 min",
      "abort_criteria": "DE converges slower than CMA-ES OR same local minima issue"
    },
    {
      "id": "EXP_WEIGHTED_LOSS_001",
      "priority": 99,
      "family": "loss_function",
      "status": "completed",
      "result": "FAILED - Optimizing weighted RMSE finds different optimum than unweighted (scored). Score 1.0131 (-0.1557). Diversity destroyed.",
      "score": 1.0131,
      "time_min": 90.7,
      "worker": "W1",
      "name": "weighted_sensor_loss",
      "description": "Weight sensor contributions by their informativeness or reliability",
      "finding": "CRITICAL: Do NOT change loss function - optimize exactly what you are scored on. loss_function family FAILED."
    },
    {
      "id": "EXP_POLAR_PARAM_001",
      "priority": 99,
      "family": "problem_reformulation",
      "status": "completed",
      "result": "FAILED - Rectangular domain incompatible with polar coords. Score 1.1472 @ 72.7 min vs baseline.",
      "score": 1.1472,
      "time_min": 72.7,
      "worker": "W2",
      "name": "polar_parameterization",
      "description": "Reparameterize source positions using polar coordinates centered on domain centroid",
      "finding": "Cartesian is optimal for rectangular domains. problem_reformulation family FAILED."
    },
    {
      "id": "EXP_EARLY_REJECTION_001",
      "priority": 99,
      "family": "efficiency",
      "status": "completed",
      "result": "FAILED - Two-stage evaluation ADDS overhead (191 sims vs 100 baseline). Only 8.6% rejection rate - need >25% to break even. Efficiency family EXHAUSTED.",
      "score": 1.1598,
      "time_min": 146.7,
      "worker": "W1",
      "name": "early_rejection_partial_sim",
      "description": "Reject clearly bad candidates early using partial simulation (first 10% timesteps)",
      "finding": "Early rejection fundamentally flawed: filter cost not offset by savings. CMA-ES candidates cluster near optima, most pass filter. Baseline 40% temporal is optimal."
    },
    {
      "id": "EXP_ADAPTIVE_NM_POLISH_001",
      "priority": 4,
      "family": "refinement",
      "status": "completed",
      "name": "adaptive_nm_iterations",
      "description": "Dynamically adjust NM polish iterations based on convergence rate",
      "hypothesis": "Some samples converge quickly (4-5 NM iters), others need more (10+). Fixed 8 iters is wasteful for easy samples, insufficient for hard ones.",
      "research_source": "Adaptive algorithms adjust parameters based on progress. Early stopping when converged saves compute.",
      "why_different": "Current baseline uses fixed 8 NM iterations. Adaptive could be more efficient.",
      "implementation": [
        "1. Start NM polish with initial iterations (e.g., 4)",
        "2. Check convergence: delta_rmse < threshold?",
        "3. If not converged and iterations < max (e.g., 12), continue",
        "4. If converged early, stop and save time",
        "5. Track time savings and accuracy"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (save time on easy samples)",
      "abort_criteria": "Adaptive logic overhead exceeds savings OR accuracy drops",
      "worker": "W2",
      "result": "FAILED - Adaptive NM iterations does NOT improve. Best: 1.1607 @ 78.3 min (-0.0081 score, +19.9 min vs baseline).",
      "score": 1.1607,
      "time_min": 78.3,
      "finding": "Fixed 8 NM iterations is already optimal. Adaptive batching adds overhead. Source-count based (6/10) is worse. refinement family EXHAUSTED."
    },
    {
      "id": "EXP_CMAES_RESTART_BEST_001",
      "priority": 99,
      "family": "cmaes_improvement",
      "status": "completed",
      "result": "FAILED - Two-phase restart doubles sims (175.7 min), causes diversity loss (1.7 vs 3 candidates), no accuracy improvement. Score 1.0986 vs baseline 1.1688.",
      "score": 1.0986,
      "time_min": 175.7,
      "worker": "W1",
      "name": "cmaes_restart_from_best",
      "description": "Run short CMA-ES, restart from best solution found with tighter sigma",
      "finding": "CMA-ES converges well in single phase. Restart wastes budget re-converging to same solutions. Small sigma in phase 2 destroys diversity."
    },
    {
      "id": "EXP_GRADIENT_INIT_001",
      "priority": 99,
      "family": "initialization",
      "status": "invalid",
      "name": "gradient_informed_init",
      "description": "Use temperature gradient direction to inform initial source position guess",
      "hypothesis": "Heat sources create temperature gradients pointing away from them. Tracing gradient backward could give better init than just hottest sensor.",
      "research_source": "Physics-based initialization. Heat equation has predictable gradient structure.",
      "why_different": "Current smart init uses hottest sensor position. Gradient-informed could be more accurate.",
      "implementation": [
        "1. Compute temperature gradient at each sensor location",
        "2. Trace gradient backward (uphill in temp) from hot sensors",
        "3. Estimate source position as intersection point or max temp region",
        "4. Use as initial guess for CMA-ES",
        "5. Compare with hottest-sensor init"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Gradient-based init is not consistently better than hottest-sensor",
      "invalidation_reason": "DUPLICATE of EXP_PHYSICS_INIT_001 which FAILED. That experiment tested 'sensor temperature gradients to initialize source location estimates' and found gradient init WORSE than hottest-sensor (-0.0046 score, +2.3 min). This experiment proposes essentially the same thing. initialization family already marked EXHAUSTED."
    },
    {
      "id": "EXP_MULTISTART_ELITE_001",
      "priority": 99,
      "family": "multi_start",
      "status": "completed",
      "name": "multistart_elite_selection",
      "description": "Run multiple short CMA-ES from different inits, continue only the best 1-2",
      "hypothesis": "Instead of running N full optimizations, run N short ones and continue only the most promising. More efficient budget allocation.",
      "research_source": "Elite selection in multi-start optimization. Racing algorithms.",
      "why_different": "Current approach runs all inits to completion. Elite selection may be more efficient.",
      "implementation": [
        "1. Start 4-5 CMA-ES runs from different smart inits",
        "2. After 5 generations, evaluate all",
        "3. Continue only top 1-2 performers",
        "4. Run to full convergence",
        "5. Compare with baseline multi-start"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (better budget allocation)",
      "abort_criteria": "Elite selection kills good runs OR overhead exceeds savings",
      "worker": "W2",
      "result": "FAILED - Multi-start overhead dominates. Early fitness not predictive. Score 1.1529 @ 67 min vs baseline 1.1688 @ 58.4 min.",
      "score": 1.1529,
      "time_min": 67.0,
      "finding": "Single-start with good initialization is already optimal. multi_start family FAILED."
    },
    {
      "id": "EXP_LOG_RMSE_LOSS_001",
      "priority": 99,
      "family": "loss_reformulation",
      "status": "completed",
      "result": "FAILED - Monotonic transformation adds overhead without benefit. Score 1.1677 @ 70.5 min vs baseline.",
      "score": 1.1677,
      "time_min": 70.5,
      "worker": "W2",
      "name": "log_rmse_loss",
      "description": "Use log(1+RMSE) instead of RMSE as optimization objective",
      "finding": "loss_reformulation family FAILED. RMSE is optimal objective."
    },
    {
      "id": "EXP_SOLUTION_INJECTION_001",
      "priority": 99,
      "family": "population_enhancement",
      "status": "completed",
      "result": "FAILED - Sequential execution kills parallelism. Score 0.9929 @ 122.5 min vs baseline 1.1688 @ 58.4 min.",
      "score": 0.9929,
      "time_min": 122.5,
      "worker": "W1",
      "name": "solution_injection_cmaes",
      "description": "Inject best solutions from early inits into later CMA-ES populations (GloMPO-inspired)",
      "finding": "Information sharing between inits not worth losing parallelism. population_enhancement family FAILED."
    },
    {
      "id": "EXP_TIKHONOV_REG_001",
      "priority": 99,
      "family": "regularization",
      "status": "completed",
      "worker": "W2",
      "name": "tikhonov_regularized_loss",
      "description": "Add Tikhonov regularization penalty to RMSE for smoother solutions",
      "result": "ABORTED - Prior evidence: EXP_WEIGHTED_LOSS_001 showed changing loss function is catastrophic (score 1.0131 vs 1.1688). Any modification to objective finds DIFFERENT optimum than what we're scored on. Tikhonov would have same issue.",
      "finding": "regularization family EXHAUSTED. Optimizing modified objective finds different optimum."
    },
    {
      "id": "EXP_ADAPTIVE_POPSIZE_001",
      "priority": 99,
      "family": "cmaes_enhancement",
      "status": "completed",
      "worker": "W1",
      "name": "adaptive_population_size",
      "description": "Start with larger population for exploration, reduce for exploitation",
      "result": "FAILED - Two-phase CMA-ES massively over budget (124.6 min, 2.1x). Score 1.0026 vs baseline 1.1688. Phases break covariance adaptation.",
      "score": 1.0026,
      "time_min": 124.6,
      "finding": "Two-phase CMA-ES creates 8 CMA-ES runs per 2-src sample (4 inits x 2 phases). CMA-ES covariance adaptation requires continuous updates - phases discard learning. cmaes_enhancement family EXHAUSTED."
    },
    {
      "id": "EXP_COORD_SIGMA_001",
      "priority": 99,
      "family": "cmaes_tuning",
      "status": "completed",
      "worker": "W1",
      "name": "coordinate_wise_sigma",
      "result": "ABORTED - Flawed premise. CMA-ES only optimizes positions (not intensity). dd-CMA-ES already tested coordinate scaling and failed (2:1 domain ratio too mild).",
      "finding": "Intensity is computed analytically, not by CMA-ES. Prior EXP_DD_CMAES_001 showed coordinate-wise scaling doesn't help."
    },
    {
      "id": "EXP_EARLY_STOP_CMA_001",
      "priority": 99,
      "family": "efficiency",
      "status": "completed",
      "result": "FAILED - Early stopping NEVER triggered (0 saved fevals). CMA-ES continues improving >1% throughout its budget. 5.4x over budget.",
      "score": 1.1378,
      "time_min": 313.3,
      "worker": "W2",
      "name": "cmaes_early_stopping",
      "description": "Stop CMA-ES early if best solution hasn't improved for K generations",
      "finding": "The hypothesis 'many samples converge quickly' is NOT validated. CMA-ES makes >1% improvements throughout. efficiency family EXHAUSTED."
    },
    {
      "id": "EXP_NOISE_INJECTION_001",
      "priority": 99,
      "family": "exploration",
      "status": "completed",
      "worker": "W2",
      "name": "noise_injection_escape",
      "description": "Inject small noise into CMA-ES solutions when convergence stagnates",
      "result": "ABORTED - Prior evidence: EXP_EARLY_STOP_CMA_001 showed CMA-ES continues improving >1% throughout - it does NOT stagnate. If sigma doesn't drop (no stagnation), noise injection would never trigger. No benefit possible.",
      "finding": "exploration/escape family EXHAUSTED. CMA-ES doesn't get stuck in local optima for this problem."
    },
    {
      "id": "EXP_POWELL_POLISH_001",
      "priority": 99,
      "family": "local_search",
      "status": "completed",
      "worker": "W1",
      "name": "powell_polish_instead_nm",
      "description": "Use Powell's method (coordinate-wise search) instead of Nelder-Mead for final polish",
      "result": "FAILED - Powell uses 5-17x more function evaluations (1000-3500 sims for 2-src). 4.2x over budget (244.9 min), score WORSE (-0.0275).",
      "score": 1.1413,
      "time_min": 244.9,
      "finding": "Powell's coordinate-wise line searches require O(n) 1D optimizations per iteration. For 4D: 40-120 evals/iter vs NM's ~5. Nelder-Mead is optimal for 2-4D polish. local_search family should use NM."
    },
    {
      "id": "EXP_RANDOM_TIMESTEPS_001",
      "priority": 4,
      "family": "temporal_sampling",
      "status": "completed",
      "worker": "W2",
      "name": "random_timestep_selection",
      "description": "Use random 40% of timesteps instead of first 40% for CMA-ES evaluation",
      "hypothesis": "First 40% timesteps may miss important late-time dynamics. Random sampling captures different parts of temporal evolution, potentially providing more informative RMSE signal.",
      "research_source": "Stochastic sampling in reduced-order models. Random subsets can capture global behavior better than contiguous subsets.",
      "why_different": "Baseline uses first 40% timesteps (early dynamics only). Random samples entire time window.",
      "implementation": [
        "1. Pre-select random 40% of timestep indices (once per sample)",
        "2. Use same indices for all CMA-ES candidates in that sample",
        "3. Compare RMSE correlation with full-timestep RMSE",
        "4. Compare final optimization quality"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Random timesteps give worse RMSE correlation than contiguous (current method)",
      "result": "FAILED - Random timesteps requires full simulation (3-4x slower). Projected 350 min (5.8x over budget). No accuracy benefit.",
      "score": null,
      "time_min": 350,
      "finding": "Efficiency comes from running FEWER timesteps, not DIFFERENT timesteps. Early timesteps are most informative. temporal_sampling family FAILED."
    },
    {
      "id": "EXP_SEP_CMAES_001",
      "priority": 2,
      "family": "cmaes_variants",
      "status": "completed",
      "name": "separable_cmaes_diagonal",
      "description": "Use Separable CMA-ES (diagonal-only covariance) for faster convergence",
      "hypothesis": "Separable CMA-ES uses diagonal covariance matrix only, enabling higher learning rates. For low-dim problems (2-4D), the correlation information loss may be acceptable while gaining speed. Research shows sep-CMA-ES converges faster on separable/nearly-separable functions.",
      "research_source": "dd-CMA paper (Akimoto & Hansen, Evolutionary Computation 2020). pycma CMA_diagonal option.",
      "why_different": "Baseline uses full covariance CMA-ES. Sep-CMA-ES trades correlation modeling for faster adaptation. Our problem may be sufficiently separable (x, y positions somewhat independent).",
      "implementation": [
        "1. Set CMA option: CMA_diagonal=True (always diagonal)",
        "2. This enables higher learning rate for covariance update",
        "3. Compare convergence speed and final accuracy",
        "4. Check if position correlations matter for this problem"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (faster with comparable accuracy)",
      "abort_criteria": "Diagonal covariance loses critical correlations, accuracy drops significantly",
      "worker": "W2",
      "result": "FAILED - Diagonal covariance hurts both accuracy (-0.0187) and time (5.3x over budget). Full covariance is essential.",
      "score": 1.1501,
      "time_min": 312.0,
      "finding": "Position correlations along heat gradient are essential. Diagonal covariance causes worse convergence AND more iterations."
    },
    {
      "id": "EXP_DD_CMAES_001",
      "priority": 99,
      "family": "cmaes_variants",
      "status": "completed",
      "worker": "W1",
      "name": "diagonal_decoding_cmaes",
      "description": "Use dd-CMA-ES (Diagonal Acceleration) for coordinate-wise scaling",
      "result": "FAILED - dd-CMA-ES is neutral (dd=1.0 vs dd=0 gives same score). 2:1 domain ratio too mild for diagonal decoding.",
      "score": 1.1466,
      "time_min": 50.3,
      "finding": "dd-CMA-ES provides no improvement. Best: 1.1466 @ 50.3 min vs baseline 1.1688 @ 58.4 min. Low-dim problems (2D/4D) don't benefit."
    },
    {
      "id": "EXP_BFGS_POLISH_001",
      "priority": 99,
      "family": "local_search",
      "status": "completed",
      "worker": "W1",
      "name": "bfgs_polish_after_cmaes",
      "result": "ABORTED - Prior evidence. L-BFGS-B polish (EXP_HYBRID_CMAES_LBFGSB_001) and Powell polish (EXP_POWELL_POLISH_001) both FAILED. All alternative polish methods worse than NM.",
      "finding": "BFGS requires finite diff gradient (2n+1 extra evals/iter). L-BFGS-B already tested this approach and failed. NM is optimal for 2-4D polish. local_search family EXHAUSTED."
    },
    {
      "id": "EXP_LRA_CMAES_001",
      "priority": 99,
      "family": "cmaes_advanced",
      "status": "completed",
      "worker": "W1",
      "name": "learning_rate_adapted_cmaes",
      "result": "FAILED - cmaes library underperforms pycma. lr_adapt=True: 1.1440 @ 53 min, lr_adapt=False: 1.1457 @ 52 min. Both worse than baseline 1.1688 @ 58.4 min.",
      "score": 1.1457,
      "time_min": 52.0,
      "finding": "LRA hurts slightly (-0.0017). Main issue is cmaes library vs pycma, not LRA. Stay with pycma.",
      "description": "Use CMA-ES with Learning Rate Adaptation (LRA-CMA-ES) for better handling of multimodal/noisy landscapes",
      "hypothesis": "LRA-CMA-ES maintains constant signal-to-noise ratio in learning. May improve convergence on difficult samples where standard CMA-ES struggles. Based on 2024 paper on adaptive learning rates.",
      "research_source": "CMA-ES with Learning Rate Adaptation (arXiv 2024). cmaes Python library supports LRA.",
      "why_different": "Standard CMA-ES has fixed learning rates. LRA adapts them based on fitness landscape characteristics. May help with multimodal samples.",
      "implementation": [
        "1. Use cmaes library with lr_adapt=True option",
        "2. Compare convergence on difficult 2-source samples",
        "3. Check if improved handling of noisy RMSE landscape",
        "4. Compare total time and final accuracy"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 58 min",
      "abort_criteria": "LRA overhead exceeds any accuracy benefit"
    },
    {
      "id": "EXP_LOWER_SIGMA_001",
      "priority": 99,
      "family": "sigma_tuning",
      "status": "completed",
      "worker": "W1",
      "name": "lower_sigma_baseline",
      "description": "Test lower initial sigma (0.15/0.18) vs baseline (0.15/0.20)",
      "result": "FAILED - Both lower (0.15/0.18) and higher (0.18/0.22) sigma are WORSE than baseline (0.15/0.20). Baseline sigma is locally optimal.",
      "score": 1.1335,
      "time_min": 39.6,
      "finding": "Lower sigma: 1.1335 @ 39.6 min (-0.0353). Higher sigma: 1.1379 @ 38.5 min (-0.0309). Baseline sigma 0.15/0.20 is optimal. sigma_tuning family EXHAUSTED."
    },
    {
      "id": "EXP_FEVAL_POLISH_TRADE_001",
      "priority": 99,
      "family": "budget_reallocation",
      "status": "completed",
      "worker": "W2",
      "result": "ABORTED - Prior evidence: (1) EXP_EARLY_STOP_CMA_001 showed CMA-ES improves >1% throughout - reducing fevals hurts accuracy. (2) EXP_EXTENDED_POLISH_001 showed 12 polish = +24 min for only +0.0015 score. Trading fevals for polish is not viable.",
      "finding": "budget_reallocation family EXHAUSTED. CMA-ES fevals are not tradeable - they're essential for covariance learning.",
      "name": "reduced_fevals_more_polish",
      "description": "Trade CMA-ES fevals for extended NM polish (fevals 15/28, polish 12)",
      "hypothesis": "CMA-ES may be converging before using all fevals. Reallocating budget to NM polish (12 iters instead of 8) may improve final accuracy. Extended polish failed when adding budget; this keeps total budget constant.",
      "research_source": "Extended polish (12 iters) improved accuracy but went over budget. Reducing CMA-ES fevals may recover time.",
      "why_different": "Previous extended polish added time. This experiment trades CMA-ES budget for polish budget, keeping total constant.",
      "implementation": [
        "1. Reduce fevals: 1-src from 20 to 15, 2-src from 36 to 28",
        "2. Increase polish from 8 to 12 iterations",
        "3. Track CMA-ES convergence (sigma at termination)",
        "4. Compare final accuracy and total time"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 58 min",
      "abort_criteria": "Reduced CMA-ES fevals hurt more than extended polish helps"
    },
    {
      "id": "EXP_SIGMA_CONVERGENCE_001",
      "priority": 99,
      "family": "termination_criteria",
      "status": "completed",
      "worker": "W2",
      "name": "sigma_based_termination",
      "description": "Stop CMA-ES when sigma drops below threshold instead of fixed fevals",
      "result": "ABORTED - Prior evidence: EXP_EARLY_STOP_CMA_001 showed CMA-ES doesn't converge early (0 saved fevals). EXP_ADAPTIVE_BUDGET_001 showed early termination hurts accuracy.",
      "finding": "termination_criteria family EXHAUSTED. CMA-ES needs its full budget - any early termination approach will fail."
    },
    {
      "id": "EXP_SEED_ENSEMBLE_001",
      "priority": 99,
      "family": "robustness",
      "status": "completed",
      "worker": "W2",
      "name": "random_seed_ensemble",
      "description": "Run CMA-ES with 2-3 different random seeds, pick best result per sample",
      "result": "ABORTED - Same as more_inits_select_best which FAILED. Multiple seeds = more overhead without benefit.",
      "finding": "robustness/ensemble approaches multiply simulation count. Not viable within 60 min budget.",
      "completed_by": "W2",
      "completed_at": "2026-01-24T23:40:31.752360Z"
    },
    {
      "id": "EXP_SAMPLE_TIMEOUT_001",
      "priority": 99,
      "family": "adaptive_timeout",
      "status": "completed",
      "worker": "W2",
      "name": "per_sample_timeout",
      "description": "Set per-sample timeout, reallocate budget from fast samples to hard ones",
      "result": "ABORTED - Prior evidence: EXP_EARLY_STOP_CMA_001 showed CMA-ES improves >1% throughout. Samples take their full budget for good reason. Reallocating time won't help. EXP_ADAPTIVE_BUDGET_001 also showed early termination hurts.",
      "finding": "adaptive_timeout family EXHAUSTED. CMA-ES needs its full budget per sample."
    },
    {
      "id": "EXP_INTENSITY_BOUNDS_001",
      "priority": 99,
      "family": "problem_bounds",
      "status": "completed",
      "worker": "W2",
      "name": "tighter_intensity_bounds",
      "description": "Use analysis of solved samples to tighten intensity bounds for remaining samples",
      "result": "ABORTED - Flawed premise: Intensity q is computed ANALYTICALLY via least squares, not optimized by CMA-ES. The bounds [0.5, 2.0] are just clipping constraints on the analytical solution. Tighter bounds cannot improve convergence - they would only risk clipping valid solutions.",
      "finding": "problem_bounds family EXHAUSTED. Intensity is not a CMA-ES search dimension."
    },
    {
      "id": "EXP_CHECKPOINTED_ADJOINT_001",
      "priority": 99,
      "family": "gradient_revisited",
      "status": "completed",
      "result": "FAILED - Single-timestep adjoint is CORRECT (0.5% error), but full gradient has 5x error due to missing chain rule through optimal q(x,y). Manual adjoint too complex.",
      "worker": "W2",
      "name": "checkpointed_adjoint_method",
      "description": "Memory-efficient adjoint via checkpointing for implicit ADI time-stepping",
      "hypothesis": "Previous adjoint failed due to manual derivation errors. 2025 research shows checkpointing-based adjoint can work with implicit schemes by computing VJPs step-by-step with minimal memory.",
      "research_source": "Fast automated adjoints for spectral PDE solvers (arXiv 2025), Discretize-then-optimize adjoint (HESS 2024)",
      "why_different": "Previous adjoint: manual derivation (error-prone). This: automatic via checkpointing + implicit differentiation. Avoids JAX stability issues by keeping ADI solver.",
      "implementation": [
        "1. Keep existing ADI solver (no stability issues)",
        "2. Implement checkpointing: store state every N timesteps",
        "3. Compute adjoint backward using AD between checkpoints",
        "4. Use scipy.optimize.minimize with gradient",
        "5. Compare gradient accuracy vs finite differences"
      ],
      "success_criteria": "Gradients within 1% of finite diff AND runtime < 60 min",
      "abort_criteria": "Implementation complexity exceeds 2 days OR gradient errors > 10%"
    },
    {
      "id": "EXP_BIPOP_CMAES_001",
      "priority": 3,
      "family": "cmaes_restart_v2",
      "status": "completed",
      "name": "bipop_cmaes_restart",
      "description": "BIPOP-CMA-ES alternates between large and small population restarts",
      "hypothesis": "IPOP failed because it only increases population (wasting budget). BIPOP alternates: large pop for global search, small pop for local refinement. May find better balance.",
      "research_source": "BBOB benchmarking IPOP vs BIPOP (ResearchGate), Alternative restart strategies for CMA-ES (arXiv 2012)",
      "why_different": "IPOP (tested): always increases population. BIPOP: alternates large/small. Small population can solve problems IPOP misses (Gallagher, Katsuuras functions).",
      "implementation": [
        "1. Use pycma with bipop=True option",
        "2. Set total budget same as baseline",
        "3. Let BIPOP choose population sizes automatically",
        "4. Track which regime finds best solutions",
        "5. Compare final score and diversity"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 60 min",
      "abort_criteria": "BIPOP wastes budget on small pop restarts OR same issues as IPOP",
      "worker": "W1",
      "result": "FAILED - Restarts add 8 min overhead without sufficient accuracy gain. Best in-budget: 1.1609 @ 54.9 min (-0.0079 vs baseline).",
      "score": 1.1609,
      "time_min": 54.9,
      "finding": "BIPOP and IPOP both fail for same reason: problem lacks local optima. Restarts are wasteful. cmaes_restart_v2 family EXHAUSTED."
    },
    {
      "id": "EXP_GAPPY_CPOD_001",
      "priority": 99,
      "family": "surrogate_v2",
      "status": "completed",
      "worker": "W2",
      "name": "gappy_clustering_pod",
      "description": "Gappy Clustering-POD for thermal field reconstruction with heterogeneous samples",
      "hypothesis": "Standard POD failed due to sample-specific physics (varying kappa). Gappy C-POD clusters similar samples first, then builds separate POD bases per cluster. May handle heterogeneity.",
      "research_source": "Optimization of Sparse Sensor Layouts with Gappy C-POD (Sensors 2025)",
      "why_different": "Standard POD: one basis for all samples (failed). Gappy C-POD: cluster by physics params (kappa, bc), build cluster-specific bases. Online: identify cluster, use its basis.",
      "implementation": [
        "1. Analyze samples: cluster by (kappa, bc_type, T0_stats)",
        "2. For each cluster: generate training snapshots",
        "3. Build POD basis per cluster",
        "4. Online: identify sample cluster, use appropriate basis",
        "5. Compare speedup vs accuracy tradeoff"
      ],
      "success_criteria": "Speedup >= 2x AND score loss <= 0.01",
      "abort_criteria": "Clusters too small for reliable POD OR kappa variation too continuous",
      "result": "ABORTED - Physics clustering works (only 4 clusters), BUT 100% unique sensor locations (80 configs for 80 samples) defeats Gappy C-POD.",
      "finding": "Gappy POD requires consistent observation points. With unique sensors per sample, each needs individual treatment - same as rejected online POD. surrogate_v2 family EXHAUSTED."
    },
    {
      "id": "EXP_PHYSICS_CS_001",
      "priority": 99,
      "family": "inverse_method",
      "status": "completed",
      "worker": "W2",
      "name": "physics_compressive_sensing",
      "description": "Decomposed Physics-Based Compressive Sensing (D-PBCS) for heat source detection",
      "hypothesis": "D-PBCS combines compressed sensing sparsity constraints with heat equation physics. Can reconstruct heat sources from sparse measurements without iterative optimization.",
      "research_source": "D-PBCS for inverse heat source detection (ScienceDirect July 2025)",
      "why_different": "CMA-ES: iterative black-box optimization (many simulations). D-PBCS: single linear solve with sparsity constraints. Physics-informed, not data-driven like failed surrogates.",
      "implementation": [
        "1. Formulate heat source as sparse vector on grid",
        "2. Build observation matrix (sensor locations)",
        "3. Add physics constraints (heat equation residual)",
        "4. Solve: min ||source||_1 s.t. ||Y_obs - H*source||_2 < epsilon",
        "5. Compare accuracy and speed vs CMA-ES"
      ],
      "success_criteria": "Score >= 1.16 AND time <= 30 min (direct solve should be fast)",
      "abort_criteria": "Heat source not actually sparse (continuous distribution) OR physics constraints don't match problem",
      "result": "ABORTED - Observation matrix requires 97 min/sample (budget 9 sec). 646x over budget. Coarse grids still 6x over. Sample-specific sensors prevent pre-computation.",
      "finding": "D-PBCS fundamentally incompatible with time budget. Building A needs 5000 sims. inverse_method family EXHAUSTED."
    },
    {
      "id": "EXP_FREQUENCY_DOMAIN_001",
      "priority": 99,
      "family": "frequency_domain",
      "status": "completed",
      "worker": "W2",
      "name": "frequency_domain_optimization",
      "description": "Heat equation optimization in Fourier/frequency domain for faster RMSE computation",
      "hypothesis": "Heat equation simplifies in frequency domain. Fourier transform can decouple spatial modes, potentially enabling faster RMSE evaluation and more efficient optimization.",
      "research_source": "Frequency Domain Thermal Analysis (AIAA 2024), Small perturbation frequency domain method (ScienceDirect Feb 2026)",
      "why_different": "All prior experiments use time-domain simulation. Frequency domain could offer faster RMSE computation by exploiting heat equation's spectral structure.",
      "implementation": [
        "1. Transform temperature observations Y_obs to frequency domain",
        "2. Compute heat source response in frequency domain (Green's function)",
        "3. Formulate RMSE as frequency-domain residual",
        "4. Optimize source parameters using spectral RMSE",
        "5. Compare speed vs accuracy with time-domain baseline"
      ],
      "success_criteria": "Score >= 1.15 AND time <= 40 min (frequency domain should be faster)",
      "abort_criteria": "Spectral RMSE doesn't correlate with time-domain RMSE OR boundary conditions complicate transform",
      "result": "ABORTED - RMSE computation is 0.082ms, simulation is 1207ms (14,721x slower). Frequency domain addresses wrong bottleneck.",
      "finding": "Frequency domain still requires full ADI simulation. RMSE is already negligible. frequency_domain family EXHAUSTED."
    },
    {
      "id": "EXP_RBF_MESHLESS_001",
      "priority": 99,
      "family": "meshless_direct",
      "status": "completed",
      "worker": "W2",
      "name": "rbf_meshless_inverse",
      "description": "Radial Basis Function meshless method for direct inverse problem solving",
      "result": "ABORTED - Matrix A computation requires n_rbf_points simulations. Even coarsest grid (5\u00d73=15 pts = 15 sec) exceeds budget (9 sec). 100% unique sensors prevent pre-computation.",
      "finding": "RBF meshless has same fundamental issue as D-PBCS (646x over budget). Building observation matrix requires O(n_points) simulations per sample. meshless_direct family EXHAUSTED."
    },
    {
      "id": "EXP_CMAES_RBF_SURROGATE_001",
      "priority": 99,
      "family": "surrogate_hybrid",
      "status": "completed",
      "worker": "W2",
      "name": "cmaes_rbf_surrogate",
      "description": "CMA-ES with local RBF surrogate for expensive function evaluations",
      "result": "ABORTED - CMA-ES covariance adaptation is already implicit surrogate modeling. ~10% rejection rate, ~6% potential sim reduction. Not worth complexity.",
      "finding": "Low rejection rate (same as early rejection 8.6%) and small population (6-8) mean minimal benefit. surrogate_hybrid family EXHAUSTED."
    },
    {
      "id": "EXP_SCIPY_SLSQP_001",
      "priority": 99,
      "family": "gradient_numerical",
      "status": "completed",
      "worker": "W2",
      "name": "scipy_slsqp_optimizer",
      "description": "SLSQP optimizer with numerical gradients for local refinement",
      "result": "ABORTED - SLSQP uses finite diff gradients = same O(n) overhead as L-BFGS-B which FAILED. Abort criteria met.",
      "finding": "L-BFGS-B polish: 1.1174 @ 42 min (worse than NM x8: 1.1415 @ 38 min). SLSQP has identical gradient overhead. gradient_numerical family EXHAUSTED."
    },
    {
      "id": "EXP_GENETIC_ALGORITHM_001",
      "priority": 6,
      "family": "evolutionary_ga",
      "status": "done",
      "name": "genetic_algorithm_optimizer",
      "description": "Standard genetic algorithm as alternative to CMA-ES",
      "hypothesis": "CMA-ES may be over-engineering for this simple 2-4D problem. A simpler GA with good initialization might achieve similar results with less overhead.",
      "research_source": "scipy.optimize.differential_evolution, DEAP library",
      "why_different": "CMA-ES uses expensive covariance adaptation. GA uses simpler crossover/mutation. For low-dimensional problems, GA overhead may be lower.",
      "implementation": [
        "1. Use same initialization as baseline (4 smart inits)",
        "2. Run GA instead of CMA-ES for global search",
        "3. Use same Nelder-Mead polish",
        "4. Compare convergence speed and final score",
        "5. Track function evaluations"
      ],
      "success_criteria": "Score >= 1.16 AND time <= 50 min",
      "abort_criteria": "GA converges slower than CMA-ES OR requires more function evaluations",
      "claimed_by": "W1",
      "result": "FAILED - GA slower and less accurate than CMA-ES. All runs over budget with worse scores."
    },
    {
      "id": "EXP_DIRECT_ALGORITHM_001",
      "priority": 99,
      "family": "deterministic_global",
      "status": "completed",
      "worker": "W2",
      "name": "direct_algorithm_search",
      "description": "DIRECT (DIviding RECTangles) deterministic global optimization",
      "result": "ABORTED - DIRECT uses 441-443 evals vs CMA-ES 20-36 (12-22x more). Projected 236 min vs 60 min budget (4x over).",
      "finding": "Space partitioning doesn't scale. No covariance adaptation. All alternative global optimizers failed. deterministic_global family EXHAUSTED."
    },
    {
      "id": "EXP_GREENS_FUNCTION_001",
      "priority": 99,
      "family": "direct_inversion",
      "status": "completed",
      "worker": "W2",
      "name": "greens_function_inversion",
      "description": "Use heat equation Green's function for direct source localization without iteration",
      "hypothesis": "2026 research shows Green function method can directly localize heat sources via Volterra integral equation. Unlike CMA-ES iteration, this provides closed-form solution. Privacy-preserving and computationally minimal.",
      "research_source": "Indoor person localization via inverse heat source (Wiley 2026), Green function heat distribution modeling",
      "why_different": "CMA-ES: iterative optimization (60-90 sims). Green's function: analytical solution via integral equation. May achieve similar accuracy with single computation.",
      "implementation": [
        "1. Compute Green's function for 2D heat equation with given BCs",
        "2. Formulate temperature at sensors as convolution integral",
        "3. Invert the Volterra integral equation",
        "4. Extract source position and intensity from inversion",
        "5. Compare speed and accuracy vs CMA-ES baseline"
      ],
      "success_criteria": "Score >= 1.10 AND time <= 30 min",
      "abort_criteria": "Green's function requires sample-specific computation negating speed benefit OR inversion is ill-posed without regularization",
      "result": "ABORTED - Green's function is 4.3x SLOWER than ADI (5151ms vs 1189ms). Sensor heterogeneity (100% unique) prevents pre-computation. Inversion is nonlinear. No advantage.",
      "finding": "Green's function series requires 2500+ terms per evaluation. ADI implicit solver is highly optimized. direct_inversion family EXHAUSTED."
    },
    {
      "id": "EXP_MODAL_IDENTIFICATION_001",
      "priority": 99,
      "family": "model_reduction",
      "status": "completed",
      "worker": "W2",
      "name": "modal_identification_method",
      "description": "Build low-order modal model from sensor data for fast source estimation",
      "hypothesis": "Modal Identification Method (MIM) builds reduced-order model directly from temperature measurements. Can estimate time-varying heat sources through inversion of low-order model without full simulation.",
      "research_source": "Estimation of time-varying heat sources through inversion of low order model (ResearchGate)",
      "result": "ABORTED - Only 2 sensors = 2 modes max. Simulation is bottleneck (1167ms), SVD is 0.02ms. Modal space doesn't reduce simulations.",
      "finding": "MIM requires dense sensors and known modal basis. We have 2 sensors and sample-specific physics. model_reduction family EXHAUSTED.",
      "why_different": "Instead of simulating full PDE, extract dominant modes from observation data and solve inverse problem in modal space.",
      "implementation": [
        "1. Identify dominant thermal modes from sensor time series",
        "2. Build low-order state space model (few modes)",
        "3. Formulate source estimation as modal coefficient problem",
        "4. Solve reduced inverse problem (low dimensional)",
        "5. Compare accuracy and speed vs full simulation"
      ],
      "success_criteria": "Score >= 1.10 AND time <= 25 min",
      "abort_criteria": "Sensor sparsity prevents reliable mode identification OR modal truncation loses source information"
    },
    {
      "id": "EXP_SENSOR_SUBSET_001",
      "priority": 99,
      "family": "sensor_optimization",
      "status": "completed",
      "worker": "W2",
      "name": "informative_sensor_subset",
      "description": "Use only most informative sensors for optimization to reduce noise and computation",
      "hypothesis": "Not all sensors contribute equally to source localization. Using a subset of most informative sensors (highest temperature variance or gradient) may improve signal-to-noise ratio and reduce computation.",
      "research_source": "A-optimal sensor placement (arXiv Sept 2025), Sparse sensor allocation for source detection (arXiv Sept 2025)",
      "result": "ABORTED - Subset RMSE has r=0.68 correlation with full RMSE. This is proxy optimization which failed in weighted loss (-14%).",
      "finding": "Optimizing subset RMSE finds different optimum than full RMSE. No computational benefit. sensor_optimization family EXHAUSTED.",
      "why_different": "Baseline uses all sensors equally. Selecting optimal subset may improve both accuracy (less noise) and speed (fewer comparisons).",
      "implementation": [
        "1. Analyze sensor informativeness (variance, SNR, gradient magnitude)",
        "2. Select top K sensors per sample (K=3-4)",
        "3. Run CMA-ES optimization using only selected sensors",
        "4. Compare RMSE on full sensor set for fair comparison",
        "5. Track accuracy vs sensor count tradeoff"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (maintain accuracy, save compute)",
      "abort_criteria": "Sensor selection overhead exceeds savings OR subset loses critical source information"
    },
    {
      "id": "EXP_LANDSCAPE_ADAPTIVE_001",
      "priority": 99,
      "family": "meta_optimization",
      "status": "completed",
      "worker": "W2",
      "result": "ABORTED - ELA probing (10 evals \u00d7 1 sec = 10 sec) exceeds per-sample budget (9 sec).",
      "finding": "Prior adaptive strategies all failed. meta_optimization family EXHAUSTED.",
      "name": "landscape_adaptive_cmaes",
      "description": "Use Exploratory Landscape Analysis (ELA) to auto-configure CMA-ES per sample",
      "hypothesis": "2025 research shows ELA features can identify problem characteristics (multimodality, separability) enabling optimal CMA-ES configuration. Different samples may benefit from different sigma/population settings.",
      "research_source": "Surrogate-based automated HPO for crashworthiness (Springer 2025), BBOB landscape analysis",
      "why_different": "Baseline uses fixed CMA-ES config. ELA could detect sample difficulty (1-src vs 2-src, landscape shape) and adapt parameters accordingly.",
      "implementation": [
        "1. Run cheap ELA probes (10-20 random evaluations)",
        "2. Extract landscape features (y-distribution, local optima indicators)",
        "3. Map features to CMA-ES configuration (sigma, popsize)",
        "4. Run configured CMA-ES optimization",
        "5. Compare vs fixed configuration baseline"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "ELA probe cost exceeds benefit OR samples too homogeneous for adaptation"
    },
    {
      "id": "EXP_MULTI_OBJECTIVE_001",
      "priority": 6,
      "family": "multi_objective",
      "status": "done",
      "name": "multi_objective_pareto",
      "description": "Multi-objective optimization trading off accuracy vs computation time",
      "hypothesis": "Instead of hard budget constraint, treat accuracy and speed as two objectives. NSGA-II or similar can find Pareto front, revealing if better tradeoffs exist.",
      "research_source": "NSGA-II multi-objective optimization, pymoo library",
      "why_different": "Baseline optimizes accuracy with time constraint. MO can explore full tradeoff space, potentially finding better operating points.",
      "implementation": [
        "1. Define objectives: minimize RMSE, minimize simulation count",
        "2. Use NSGA-II or MOEA/D for multi-objective optimization",
        "3. Generate Pareto front of solutions",
        "4. Analyze tradeoff curve",
        "5. Compare best in-budget solution vs baseline"
      ],
      "success_criteria": "Score >= 1.17 within 60 min OR discover better accuracy-time tradeoff",
      "abort_criteria": "Pareto front reveals no improvements OR MO overhead exceeds benefit",
      "claimed_by": "W1",
      "claimed_at": "2026-01-24T16:07:34.673756Z",
      "result": "ABORTED - Multi-objective optimization theoretically unsound. Scoring formula is already multi-objective."
    },
    {
      "id": "EXP_GROWTH_OPTIMIZER_001",
      "priority": 7,
      "family": "metaheuristic_go",
      "status": "done",
      "name": "growth_optimizer",
      "description": "Growth Optimizer (GO) - 2022 metaheuristic competitive with 50 algorithms",
      "hypothesis": "GO uses learning/reflection mechanisms inspired by human growth. Tested against 50 metaheuristics including CMA-ES. May offer different convergence characteristics.",
      "research_source": "Growth Optimizer: A powerful metaheuristic (Knowledge-Based Systems 2022)",
      "why_different": "CMA-ES uses Gaussian covariance adaptation. GO uses learning/reflection mechanisms. Different search dynamics may find different solutions.",
      "implementation": [
        "1. Implement GO or find Python library (pyGO)",
        "2. Use same initialization and budget as baseline",
        "3. Apply same Nelder-Mead polish",
        "4. Compare convergence and final score",
        "5. Track function evaluations"
      ],
      "success_criteria": "Score >= 1.16 AND time <= 55 min",
      "abort_criteria": "GO requires more evaluations than CMA-ES OR no library available",
      "claimed_by": "W1",
      "claimed_at": "2026-01-24T16:11:01.807013Z",
      "result": "ABORTED - Another metaheuristic without covariance adaptation. Same failure mode as GA, DE, OpenAI ES, SA."
    },
    {
      "id": "EXP_EARLY_EXIT_THRESHOLD_001",
      "priority": 7,
      "family": "termination_v2",
      "status": "done",
      "name": "confidence_based_early_exit",
      "description": "Exit CMA-ES early when confidence in solution is high",
      "hypothesis": "When CMA-ES sigma becomes very small (high confidence), additional iterations have diminishing returns. Early exit could save time without accuracy loss.",
      "research_source": "CMA-ES termination criteria, standard deviation convergence",
      "why_different": "Previous early termination based on improvement threshold. This uses sigma/stddev directly as confidence measure.",
      "implementation": [
        "1. Monitor CMA-ES sigma during optimization",
        "2. Exit early when sigma < threshold (e.g., 0.01)",
        "3. Use saved budget for additional smart initializations",
        "4. Compare accuracy-time tradeoff vs baseline",
        "5. Find optimal sigma threshold"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min",
      "abort_criteria": "Early exit hurts accuracy OR threshold is sample-dependent",
      "claimed_by": "W1",
      "claimed_at": "2026-01-24T16:13:14.304525Z",
      "result": "ABORTED - Same idea as prior failed experiments (ADAPTIVE_BUDGET, CMAES_EARLY_STOPPING). Early exit hurts CMA-ES."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_SWEEP_001",
      "priority": 2,
      "family": "temporal_tuning",
      "status": "completed",
      "name": "temporal_fidelity_sweep",
      "description": "Fine-tune temporal fidelity: test 35%, 38%, 42%, 45% to find exact optimum around 40%",
      "hypothesis": "40% timesteps was found good but nearby fractions weren't tested. 38% or 42% might be marginally better.",
      "research_source": "Internal: baseline found 40% optimal but didn't explore neighborhood",
      "implementation": [
        "1. Copy early_timestep_filtering optimizer",
        "2. Parameterize temporal_fraction",
        "3. Run grid search: [0.35, 0.38, 0.40, 0.42, 0.45]",
        "4. Keep sigma 0.18/0.22 and 8 NM polish constant",
        "5. Compare scores and times"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-24T20:37:46.749817Z",
      "result": "FAILED - Sigma 0.18/0.22 is WORSE than baseline 0.15/0.20. All runs over budget (67-73 min vs 60 min). No improvement over 40% baseline.",
      "score": 1.1671,
      "time_min": 68.7,
      "worker": "W1/W2",
      "finding": "temporal_tuning family EXHAUSTED. Baseline configuration is optimal.",
      "completed_at": "2026-01-24T21:52:52.546373Z"
    },
    {
      "id": "EXP_MORE_INITS_SELECT_BEST_001",
      "priority": 2,
      "family": "candidate_selection",
      "status": "completed",
      "name": "more_inits_select_best",
      "description": "Run 6-8 CMA-ES initializations, select best 3 by RMSE before dissimilarity filter",
      "hypothesis": "More initializations increases chance of finding global optimum. Selecting best 3 improves accuracy component of score.",
      "research_source": "Scoring formula: accuracy averages over candidates. More inits with selection = better average.",
      "implementation": [
        "1. Increase n_smart_inits from 3 to 6-8",
        "2. Run all CMA-ES in parallel",
        "3. Rank by RMSE before dissimilarity filter",
        "4. Keep top 3 diverse candidates",
        "5. May need to reduce fevals/init to stay in budget"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-24T21:40:43.885713Z",
      "result": "FAILED - All runs over budget. More inits add overhead without accuracy benefit.",
      "score": 1.159,
      "time_min": 66.9,
      "worker": "W2",
      "finding": "candidate_selection family EXHAUSTED. Baseline 2-init (triangulation + hotspot) is optimal."
    },
    {
      "id": "EXP_ICA_SEEDED_INIT_001",
      "priority": 3,
      "family": "initialization_v3",
      "status": "completed",
      "name": "ica_seeded_init",
      "description": "Use FastICA for 5-10 second initialization to get good starting positions for CMA-ES",
      "hypothesis": "ICA achieved best accuracy (1.0422) but at 87 min. Using ICA only for initialization (~5-10s) then CMA-ES might combine ICA's accuracy with CMA-ES's speed.",
      "research_source": "Internal: ICA was best accuracy method. Key insight: use for init only, not full solve.",
      "implementation": [
        "1. Run FastICA on sensor data (max 10s)",
        "2. Extract source positions from ICA components",
        "3. Use as initial guess for CMA-ES",
        "4. Keep baseline sigma/fevals/polish",
        "5. Compare to random/hotspot initialization"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-24T22:54:28.890110Z",
      "completed_at": "2026-01-24T23:15:00Z",
      "worker": "W1",
      "result": "FAILED - ICA positions not better than triangulation+hotspot. -0.0087 score, +10.8 min.",
      "best_score": 1.1601,
      "best_time_min": 69.2,
      "finding": "initialization_v3 EXHAUSTED. Triangulation+hotspot already optimal.",
      "score": 1.1601,
      "time_min": 69.2
    },
    {
      "id": "EXP_SIGMA_LADDER_001",
      "priority": 3,
      "family": "sigma_scheduling_v2",
      "status": "completed",
      "name": "sigma_ladder",
      "description": "Start with high sigma (0.30), reduce to 0.15 during CMA-ES optimization",
      "hypothesis": "High sigma explores broadly, low sigma refines. Manual schedule might help when CMA-ES natural adaptation is too slow.",
      "research_source": "Research: CMA-ES adapts sigma naturally but manual schedule could front-load exploration",
      "implementation": [
        "1. Start sigma at 0.30/0.35 (1-src/2-src)",
        "2. Reduce by factor every N generations",
        "3. End at 0.15/0.20",
        "4. Compare to fixed sigma baseline",
        "5. Test with 40% temporal fidelity"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-24T22:58:53.758909Z",
      "completed_by": "W2",
      "completed_at": "2026-01-24T23:17:25.864384Z",
      "result": "FAILED - Sigma scheduling interferes with CMA-ES natural adaptation. -0.0030 score, +11.6 min.",
      "score": 1.1658,
      "time_min": 70.0,
      "finding": "sigma_scheduling_v2 EXHAUSTED. CMA-ES sigma adaptation already optimal."
    },
    {
      "id": "EXP_TWO_SOURCE_SEQUENTIAL_001",
      "priority": 4,
      "family": "two_source_strategy",
      "status": "completed",
      "name": "two_source_sequential",
      "description": "For 2-source problems, optimize one source at a time with fixed other source",
      "hypothesis": "4D optimization for 2-source is hard. Sequential 2x2D might converge better.",
      "research_source": "Coordinate descent principle applied to source-wise optimization",
      "implementation": [
        "1. For 2-source: first find best single source",
        "2. Fix that source, optimize second source (2D)",
        "3. Alternate: optimize first with second fixed",
        "4. Repeat 2-3 cycles",
        "5. Compare to simultaneous 4D optimization"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "completed_by": "W2",
      "completed_at": "2026-01-24T23:38:33.573923Z",
      "result": "ABORTED - Already tested as sequential_2src. Listed in \"Don't Retry\": high variance."
    },
    {
      "id": "EXP_WEIGHTED_CENTROID_NM_001",
      "priority": 3,
      "family": "polish_improvement",
      "status": "completed",
      "name": "weighted_centroid_nm",
      "description": "Implement weighted centroid strategy from 2023 paper specifically designed for heat source localization",
      "hypothesis": "2023 paper: Weighted mean of simplex vertices accelerates convergence. Inertia term + random node replacement improves exploration. Paper explicitly mentions heat source locator.",
      "research_source": "https://www.sciencedirect.com/science/article/abs/pii/S1568494623011961 - Weighted Centroids in Adaptive Nelder-Mead: With heat source locator applications (2023)",
      "implementation": [
        "1. Implement weighted centroid calculation instead of simple centroid",
        "2. Add inertia term based on simplex size",
        "3. Add random node replacement for exploration",
        "4. Compare to standard NM polish",
        "5. Test within baseline framework"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-24T23:16:19.418175Z",
      "completed_at": "2026-01-24T23:35:00Z",
      "worker": "W1",
      "result": "FAILED - Weighted NM hurt diversity. Score 0.9608 (-0.2080 vs baseline), only 1 candidate/sample.",
      "best_score": 0.9608,
      "best_time_min": 52.3,
      "finding": "polish_improvement FAILED. Standard NM is optimal for this problem.",
      "score": 0.9608,
      "time_min": 52.3
    },
    {
      "id": "EXP_CORRECT_TEMPORAL_SWEEP_001",
      "priority": 2,
      "family": "temporal_tuning_v2",
      "status": "completed",
      "name": "correct_temporal_sweep",
      "description": "Redo temporal sweep with CORRECT sigma 0.15/0.20 (not 0.18/0.22)",
      "hypothesis": "Previous sweep used wrong sigma. With correct sigma, 38% or 42% might beat 40%.",
      "research_source": "Internal: temporal_fidelity_sweep used wrong sigma, need to redo correctly",
      "implementation": [
        "1. Copy early_timestep_filtering optimizer",
        "2. Use sigma 0.15/0.20 (CORRECT baseline sigma)",
        "3. Test fractions: [0.38, 0.40, 0.42, 0.45]",
        "4. Keep 8 NM polish iterations",
        "5. Compare to baseline 1.1688 @ 58.4 min"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-24T21:53:37.822330Z",
      "result": "FAILED - 40% timesteps CONFIRMED optimal. All other fractions worse.",
      "score": 1.1641,
      "time_min": 71.2,
      "worker": "W1",
      "completed_at": "2026-01-24T22:53:47.348973Z",
      "finding": "temporal_tuning_v2 family EXHAUSTED. 40% is proven optimal."
    },
    {
      "id": "EXP_REDUCED_CMAES_MORE_NM_001",
      "priority": 3,
      "family": "budget_reallocation",
      "status": "completed",
      "name": "reduced_cmaes_more_nm",
      "description": "Trade CMA-ES budget for more NM polish iterations",
      "hypothesis": "CMA-ES may over-explore. Fewer fevals (15/30 vs 20/36) + more NM polish (12 vs 8) might improve.",
      "research_source": "EXP_EXTENDED_POLISH showed 12 NM gives +0.0015 but was over budget. If we reduce CMA-ES budget, might fit.",
      "implementation": [
        "1. Reduce CMA-ES fevals: 15 for 1-src, 30 for 2-src",
        "2. Increase NM polish to 10-12 iterations",
        "3. Keep 40% temporal fidelity, sigma 0.15/0.20",
        "4. Measure if accuracy improvement > time overhead"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-24T22:37:14.595672Z",
      "completed_by": "W2",
      "completed_at": "2026-01-24T22:57:29.044280Z",
      "result": "FAILED - Reducing CMA-ES fevals hurts accuracy. NM cannot compensate.",
      "score": 1.1584,
      "time_min": 71.1,
      "finding": "budget_reallocation family EXHAUSTED. Baseline 20/36 + 8 NM is optimal."
    },
    {
      "id": "EXP_BOUNDARY_HANDLING_001",
      "priority": 3,
      "family": "constraint_handling",
      "status": "completed",
      "name": "boundary_handling_methods",
      "description": "Test different CMA-ES boundary constraint handling methods",
      "hypothesis": "Default boundary handling may not be optimal. Reflection or bijective mapping might improve search near boundaries.",
      "research_source": "https://www.sciencedirect.com/science/article/abs/pii/S2210650219301622 - Survey of 22 BCHMs for CMA-ES",
      "implementation": [
        "1. Test pycma's boundary handling options: 'BoundTransform', 'BoundPenalty'",
        "2. Try bijective mapping: y = a + (b-a)*(1-cos(pi*x/10))/2",
        "3. Compare to default boundary handling",
        "4. Keep other params: 40% temporal, sigma 0.15/0.20, 8 NM polish"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-24T23:18:34.463085Z",
      "completed_by": "W2",
      "completed_at": "2026-01-24T23:36:55.679470Z",
      "result": "ABORTED - BoundPenalty not valid pycma option. Baseline uses BoundTransform (default). constraint_handling EXHAUSTED."
    },
    {
      "id": "EXP_PARALLEL_NM_POLISH_001",
      "priority": 4,
      "family": "parallelization",
      "status": "completed",
      "name": "parallel_nm_polish",
      "description": "Run NM polish on all candidates in parallel instead of sequential",
      "hypothesis": "If NM polish is currently sequential, parallelizing it could reduce wall-clock time, allowing more iterations.",
      "research_source": "Internal: Check if baseline runs NM sequentially per candidate",
      "implementation": [
        "1. Check current NM polish implementation",
        "2. If sequential, parallelize using joblib",
        "3. Measure speedup",
        "4. If faster, try more iterations within budget"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "completed_by": "W2",
      "completed_at": "2026-01-24T23:39:15.638383Z",
      "result": "ABORTED - Invalid premise. NM polish is only on BEST candidate, not all candidates. Nothing to parallelize."
    },
    {
      "id": "EXP_MAE_POLISH_001",
      "priority": 3,
      "family": "polish_loss",
      "status": "complete",
      "name": "mae_polish_loss",
      "description": "Use MAE instead of RMSE as loss function for NM polish",
      "hypothesis": "RMSE emphasizes large errors. MAE might be more robust to outliers and lead to better overall fit.",
      "research_source": "Loss function theory: MAE is more robust to outliers than RMSE",
      "implementation": [
        "1. Modify NM polish objective from RMSE to MAE",
        "2. Keep all other params: 40% temporal, sigma 0.15/0.20, 8 iterations",
        "3. Compare accuracy on 1-src and 2-src samples",
        "4. Note: Final evaluation still uses RMSE (scoring)"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-24T23:36:29.771047Z",
      "completed_at": "2026-01-24T23:41:00Z",
      "worker": "W1",
      "result": "ABORTED",
      "finding": "ABORTED based on prior evidence. EXP_LOG_RMSE_LOSS and EXP_WEIGHTED_SENSOR_LOSS both FAILED when changing loss. MAE minimizer != RMSE minimizer. polish_loss family EXHAUSTED."
    },
    {
      "id": "EXP_NM_MULTI_START_001",
      "priority": 4,
      "family": "polish_strategy",
      "status": "completed",
      "name": "nm_multi_start",
      "description": "Start NM from multiple perturbations of CMA-ES best solution",
      "hypothesis": "NM may get stuck in local minima near CMA-ES solution. Multiple starts from perturbations might find better local optima.",
      "research_source": "Multi-start optimization principle applied to local refinement",
      "implementation": [
        "1. Take top CMA-ES solution",
        "2. Create 3-4 perturbations (small random offsets)",
        "3. Run NM from each perturbation",
        "4. Keep best result",
        "5. Compare to single-start NM baseline"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-24T23:39:43.527958Z",
      "completed_by": "W2",
      "completed_at": "2026-01-24T23:40:11.180431Z",
      "result": "ABORTED - Prior evidence: 8 NM iters optimal, more polish adds time without benefit. Multi-start would need 3-4x polish budget."
    },
    {
      "id": "EXP_RANDOM_SEED_ENSEMBLE_001",
      "priority": 4,
      "family": "stochastic_ensemble",
      "status": "completed",
      "name": "random_seed_ensemble",
      "description": "Run CMA-ES with multiple random seeds and ensemble the results",
      "hypothesis": "CMA-ES is stochastic. Different seeds might find different local optima. Ensembling could improve robustness.",
      "research_source": "Ensemble methods for stochastic optimization",
      "implementation": [
        "1. Run CMA-ES with 3 different random seeds (same params)",
        "2. Keep best solution from each seed",
        "3. Apply dissimilarity filter across all candidates",
        "4. Compare to single-seed baseline"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "completed_by": "W2",
      "completed_at": "2026-01-24T23:40:31.752436Z",
      "result": "ABORTED - Same as more_inits_select_best which FAILED. Multiple seeds = more overhead without benefit."
    },
    {
      "id": "EXP_COVARIANCE_INIT_001",
      "priority": 4,
      "family": "cmaes_init",
      "status": "completed",
      "name": "covariance_init",
      "description": "Initialize CMA-ES covariance matrix with prior knowledge",
      "hypothesis": "Default identity covariance assumes equal scaling. Initializing with prior (e.g., position variance > intensity variance) might help.",
      "research_source": "CMA-ES documentation on covariance initialization",
      "implementation": [
        "1. Initialize covariance with prior: [[pos_var, 0, 0], [0, pos_var, 0], [0, 0, q_var]]",
        "2. pos_var based on domain size, q_var based on intensity range",
        "3. Compare convergence speed and final accuracy to identity init"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "completed_by": "W2",
      "completed_at": "2026-01-24T23:40:31.752444Z",
      "result": "ABORTED - CMA-ES designed to LEARN covariance from fitness landscape. Pre-init would interfere with adaptation."
    },
    {
      "id": "EXP_POLISH_TOP3_001",
      "priority": 2,
      "family": "polish_strategy_v2",
      "status": "completed",
      "name": "polish_top3_reduced",
      "description": "Polish ALL top-3 candidates with 4 NM iterations each (12 total) instead of just best with 8",
      "hypothesis": "Improving accuracy of 2nd/3rd candidates will increase average accuracy component of score",
      "research_source": "W0 analysis: Current baseline only polishes BEST candidate. Score formula averages accuracy over N candidates.",
      "implementation": [
        "After CMA-ES, identify top 3 candidates by RMSE",
        "Run NM polish (4 iterations each) on ALL 3 candidates",
        "Return polished candidates",
        "Compare total budget (12 NM iters) vs baseline (8 NM iters)"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-25T02:25:21.348733Z",
      "completed_at": "2026-01-25T02:53:54.194215Z",
      "result": {
        "score": 1.0743,
        "time_min": 114.2,
        "in_budget": false,
        "outcome": "FAILED",
        "summary": "Polishing multiple candidates spreads budget thin - 1.9x over budget, 8% worse score"
      }
    },
    {
      "id": "EXP_COORDINATE_POLISH_001",
      "priority": 3,
      "family": "polish_method_v2",
      "status": "completed",
      "name": "coordinate_descent_polish",
      "description": "Use coordinate descent (cyclic optimization) instead of Nelder-Mead for polish",
      "hypothesis": "For 2-4D problems, coordinate descent may converge faster than simplex methods",
      "research_source": "Scipy coordinate descent methods; existing coord_descent_refine folder as starting point",
      "implementation": [
        "Replace NM polish with scipy.optimize.minimize(method='Powell') or custom coordinate descent",
        "Powell uses coordinate-wise line search (similar to coordinate descent)",
        "Compare convergence speed and final accuracy"
      ],
      "success_criteria": "Score >= 1.1688 AND time < 58 min",
      "claimed_at": "2026-01-25T02:29:10.525949Z",
      "result": "FAILED - Powell coordinate descent 5-8x slower than NM due to line search overhead",
      "score": null,
      "time_min": null,
      "finding": "Line search methods require too many function evals for expensive simulations. NM x8 is optimal."
    },
    {
      "id": "EXP_ASYMMETRIC_POLISH_001",
      "priority": 3,
      "family": "polish_allocation",
      "status": "completed",
      "name": "asymmetric_polish_budget",
      "description": "Spend more polish budget on 2-source problems (harder), less on 1-source (easier)",
      "hypothesis": "1-source problems converge faster, can use fewer polish iterations; 2-source needs more",
      "research_source": "Baseline: 8 NM iterations for both. 2-src has 4D vs 1-src 2D - should need more refinement.",
      "implementation": [
        "1-source: 6 NM iterations",
        "2-source: 10 NM iterations",
        "Total budget should be similar (weighted by 40/60 split)"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-25T02:29:48.945436Z",
      "result": "FAILED - Implementation overhead: 8/8 config (same as baseline) runs 35% slower. All runs over budget.",
      "score": 1.1639,
      "time_min": 79.3,
      "finding": "Cannot test asymmetric polish hypothesis due to implementation overhead. Baseline optimizer is highly optimized."
    },
    {
      "id": "EXP_EARLY_POLISH_TRIGGER_001",
      "priority": 4,
      "family": "adaptive_pipeline",
      "status": "completed",
      "name": "early_polish_trigger",
      "description": "Start polish early if CMA-ES sigma drops below threshold (indicating convergence)",
      "hypothesis": "Some samples converge in <50% of allocated fevals; using remaining budget for polish may help",
      "research_source": "CMA-ES docs: sigma < 0.01 typically indicates convergence. Early commit can save exploration budget.",
      "implementation": [
        "Monitor CMA-ES sigma during optimization",
        "If sigma < threshold (0.02?) before feval budget exhausted, terminate CMA-ES early",
        "Use saved budget for additional polish iterations",
        "Track how often early trigger activates"
      ],
      "success_criteria": "Score >= 1.1688 AND time <= 58 min",
      "result": "ABORTED - Same as prior early termination experiments (adaptive_budget, early_exit). CMA-ES needs full budget.",
      "score": null,
      "finding": "Prior evidence: Early termination hurts CMA-ES covariance adaptation. adaptive_pipeline family EXHAUSTED."
    },
    {
      "id": "EXP_SPLIT_POLISH_FINE_001",
      "priority": 4,
      "family": "fidelity_polish",
      "status": "completed",
      "name": "split_polish_fidelity",
      "description": "Run first 4 NM iterations at 40% fidelity, last 4 at 100% fidelity",
      "hypothesis": "Initial polish moves are large - 40% fidelity sufficient. Final refinement needs full fidelity.",
      "research_source": "Baseline uses 100% fidelity for all 8 polish iterations. Coarse polish might speed up without hurting accuracy.",
      "implementation": [
        "NM polish iterations 1-4: use 40% timesteps (fast)",
        "NM polish iterations 5-8: use 100% timesteps (accurate)",
        "Compare final RMSE and time"
      ],
      "success_criteria": "Score >= 1.1688 AND time < 55 min",
      "note": "WARNING: Prior progressive_polish_fidelity FAILED because truncated polish 'overfits to proxy noise'. This is different - we use FULL fidelity for final 4 iterations.",
      "claimed_at": "2026-01-25T03:00:59.989317Z",
      "result": "ABORTED - Same as progressive_polish_fidelity. Truncated polish overfits to proxy noise.",
      "score": null,
      "time_min": null,
      "finding": "Prior evidence proves full timesteps REQUIRED for polish. Truncation causes -0.0346 score."
    },
    {
      "id": "EXP_TV_REGULARIZATION_001",
      "priority": 3,
      "family": "regularization",
      "status": "completed",
      "name": "tv_regularization_sparse",
      "description": "Add Total Variation regularization to RMSE objective for sparse source recovery",
      "hypothesis": "TV regularization is optimal for 'atomic measures' (sparse point sources like ours) per 2025 arxiv paper",
      "research_source": "arxiv 2507.02677 (July 2025): 'For recovering atomic measures, total variation (TV) is more suitable than Tikhonov'",
      "implementation": [
        "Add TV penalty term: objective = RMSE + lambda * TV(sources)",
        "TV norm for point sources = sum of intensities (L1 on q)",
        "Test lambda values: 0.01, 0.05, 0.1",
        "Compare stability and accuracy"
      ],
      "success_criteria": "Score >= 1.1688 AND time <= 60 min",
      "note": "WARNING: Loss function changes have failed before. This is REGULARIZATION (stability) not loss change.",
      "claimed_at": "2026-01-25T02:54:54.421058Z",
      "completed_at": "2026-01-25T03:15:42.934801Z",
      "result": {
        "score": 1.1614,
        "time_min": 83.7,
        "in_budget": false,
        "outcome": "FAILED",
        "summary": "TV regularization biases away from RMSE optimum - worse score, over budget"
      }
    },
    {
      "id": "EXP_ADAPTIVE_NM_COEFFICIENTS_001",
      "priority": 4,
      "family": "polish_tuning",
      "status": "completed",
      "name": "adaptive_nm_coefficients",
      "description": "Tune NM simplex coefficients (reflection, expansion, contraction) for our 2-4D problems",
      "hypothesis": "Default NM coefficients (r=1.0, e=2.0, c=0.5) may not be optimal for low-dimensional heat source problems",
      "research_source": "Analytical Letters study on expansion/contraction coefficients; SIAM J. Optimization convergence analysis",
      "implementation": [
        "Test aggressive expansion (e=2.5) for faster convergence",
        "Test conservative contraction (c=0.3) for stability",
        "Test larger reflection (r=1.2) for faster progress",
        "Compare convergence speed and final RMSE"
      ],
      "success_criteria": "Score >= 1.1688 AND time < 55 min",
      "claimed_at": "2026-01-25T03:03:19.203944Z",
      "result": "FAILED - adaptive=True makes NM 45% slower, -1.7% score. Default coefficients optimal.",
      "score": 1.1493,
      "time_min": 85.0,
      "finding": "Adaptive NM coefficients designed for high-D. For 2-4D, aggressive defaults are optimal."
    },
    {
      "id": "EXP_MOMENT_INVERSION_001",
      "priority": 5,
      "family": "direct_inversion_v2",
      "status": "completed",
      "name": "moment_based_inversion",
      "description": "Use moment analysis of heat flow for source identification (arxiv 2025 novel approach)",
      "hypothesis": "Moment formulation transforms ill-posed inverse problem to more stable form without optimization iteration",
      "research_source": "arxiv 2507.02677 (July 2025): 'Moments, Time-Inversion and Source Identification for the Heat Equation'",
      "implementation": [
        "Compute spatial moments of temperature field: M_ij = integral(x^i * y^j * T dA)",
        "Low-order moments encode source location information",
        "Derive source position from moment ratios",
        "Use as initialization for CMA-ES or direct solution"
      ],
      "success_criteria": "Score >= 1.1688 AND time < 60 min",
      "note": "Novel approach - may require significant implementation effort. Priority 5 = experimental.",
      "claimed_at": "2026-01-25T03:27:30.145639Z",
      "result": "ABORTED - Sparse sensors (2-6) insufficient for moment computation. Prior direct inversion methods all failed.",
      "score": null,
      "time_min": null,
      "finding": "Moment-based inversion theoretically unsound. Baseline triangulation is already the best direct approach."
    },
    {
      "id": "EXP_NM_DIM_ADAPTIVE_001",
      "priority": 3,
      "family": "polish_tuning_v2",
      "status": "claimed_by_W3",
      "name": "nm_dimension_adaptive",
      "description": "Use dimension-dependent NM parameters (2024 research: expansion/contraction depend on dim)",
      "hypothesis": "Standard NM params designed for general case; dimension-specific params may converge faster for 2D/4D",
      "research_source": "SIAM J. Optimization: 'Adaptive parameters depending on dimension outperform standard NM for high-dim'",
      "implementation": [
        "For 1-src (2D): expansion=1.5, contraction=0.6 (more conservative)",
        "For 2-src (4D): expansion=1.3, contraction=0.7 (even more conservative)",
        "Based on research showing NM becomes less stable in higher dims"
      ],
      "success_criteria": "Score >= 1.1688 AND time <= 58 min",
      "claimed_at": "2026-01-25T03:16:34.043628Z"
    },
    {
      "id": "EXP_FINAL_POP_SELECTION_001",
      "priority": 4,
      "family": "candidate_selection_v2",
      "status": "completed",
      "name": "final_pop_diversity_select",
      "description": "Select candidates from final CMA-ES population using diversity-aware selection (not just top-3 by RMSE)",
      "hypothesis": "Final CMA-ES population contains diverse candidates; selecting top-3 by RMSE may miss better diversity",
      "research_source": "CMA-ES multi-objective research on diversity preservation in final population",
      "implementation": [
        "After CMA-ES, get final population (6-8 candidates)",
        "Rank by RMSE, then apply dissimilarity filter (tau=0.2)",
        "If filtered < 3, add next-best RMSE candidates",
        "Compare N_valid and score vs baseline top-3"
      ],
      "success_criteria": "Score >= 1.1688 AND N_valid >= 2.75",
      "note": "Baseline already achieves 2.75/3 N_valid - improvement may be marginal",
      "claimed_at": "2026-01-25T03:26:50.226478Z",
      "result": "ABORTED - Baseline ALREADY implements this. filter_dissimilar() with tau=0.2 is identical to proposed approach.",
      "score": null,
      "finding": "Baseline already does: sort by RMSE, apply dissimilarity filter (tau=0.2), keep up to 3 diverse candidates."
    },
    {
      "id": "EXP_HYBRID_RESTART_001",
      "priority": 4,
      "family": "restart_v3",
      "status": "completed",
      "name": "micro_restart_polish",
      "description": "If NM polish stalls, restart NM from perturbed position (micro-restart)",
      "hypothesis": "Some NM runs may get stuck; small perturbation + restart may find better local optima",
      "research_source": "Basin hopping uses perturbation + local search; apply to NM polish",
      "implementation": [
        "Run NM for 4 iterations, check improvement",
        "If improvement < 0.001 RMSE, perturb position by 0.05*sigma",
        "Restart NM for 4 more iterations from perturbed position",
        "Keep best result from original or perturbed"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "claimed_at": "2026-01-25T03:24:50.822330Z",
      "result": "ABORTED - All polish modifications FAIL. NM x8 defaults optimal.",
      "score": null,
      "time_min": null,
      "finding": "Prior evidence: extended polish, Powell, adaptive coefficients ALL failed. restart_v3 family EXHAUSTED."
    },
    {
      "id": "EXP_SIMPLEX_PERTURBATION_001",
      "priority": 5,
      "family": "polish_robustness",
      "status": "completed",
      "name": "simplex_perturbation_init",
      "description": "Initialize NM simplex with slightly perturbed vertices for exploration diversity",
      "hypothesis": "Default NM simplex may be too small; slightly larger initial simplex could explore more and find better local optima",
      "research_source": "NM simplex research: initial simplex size affects convergence and exploration",
      "implementation": [
        "Standard: simplex edge = 0.05 * parameter range",
        "Test: simplex edge = 0.10 * parameter range (2x larger)",
        "Compare convergence and final RMSE"
      ],
      "success_criteria": "Score > 1.1688 AND time <= 60 min",
      "note": "Low priority exploratory experiment",
      "result": "ABORTED - All polish modifications FAIL. NM x8 defaults is OPTIMAL.",
      "score": null,
      "finding": "polish_robustness family is subset of failed polish experiments."
    }
  ],
  "algorithm_families": {
    "evolutionary_cmaes": "EXHAUSTED - 34 experiments, no more tuning",
    "evolutionary_other": "PSO failed, may try genetic algorithms",
    "gradient_based": "EXHAUSTED - Adjoint wrong (1e-6 error), JAX blocked (ADI stability), finite diff too slow (99 min). NM is optimal.",
    "gradient_free_local": "Nelder-Mead OK for refinement, COBYLA too slow",
    "surrogate": "Custom NN failed. lq-CMA-ES FAILED (API mismatch). POD re-enabled.",
    "surrogate_lq": "FAILED - fmin_lq_surr returns ONE solution, we need MULTIPLE candidates",
    "surrogate_pod": "FAILED - Sample-specific physics (kappa varies) prevents universal POD basis. Online POD defeats purpose.",
    "decomposition": "ABANDONED - Fast ICA failed due to refinement overhead",
    "ensemble": "ABANDONED - 5.8x over budget, worse score. Fundamentally unworkable.",
    "hybrid": "FAILED - Sequential handoff doesn't fit in budget.",
    "bayesian_opt": "FAILED - GP surrogate doesn't model thermal RMSE landscape.",
    "multi_fidelity_spatial": "FAILED - Coarse grid (SPATIAL) RMSE landscape differs from fine grid.",
    "temporal_fidelity": "SUCCESS! 40% timesteps achieves 1.1362 @ 39 min. NEW BASELINE.",
    "temporal_fidelity_extended": "EXHAUSTED - sigma tuning, IPOP, adaptive timesteps all failed",
    "budget_allocation": "FAILED - Early termination hurts CMA-ES accuracy.",
    "meta_learning": "EXHAUSTED - Both cluster_transfer and WS-CMA-ES FAILED.",
    "preprocessing": "ABORTED - n_sources already in sample data.",
    "diversity": "EXHAUSTED - Niching hurts accuracy more than it helps diversity",
    "initialization": "EXHAUSTED - Gradient-based init WORSE than simple hottest-sensor. Smart init is optimal.",
    "initialization_v2": "EXHAUSTED - 24% samples have boundary sources. Biasing away hurts. Smart init is optimal.",
    "---NEW FAMILIES TO RESEARCH---": "========================================",
    "adjoint_gradient": "FAILED - Manual adjoint derivation is error-prone for ADI time-stepping. Gradient 1e-6 of correct value. Do NOT retry manual adjoint.",
    "differentiable_simulation": "FAILED - Explicit Euler stability requires 4x more timesteps than implicit ADI. JAX autodiff incompatible.",
    "nonlinear_least_squares": "FAILED - LM is local optimizer that gets stuck in local minima. Finite diff Jacobian too expensive. TRF likely same issue.",
    "hybrid_gradient": "FAILED - L-BFGS-B polish NOT better than NM. Finite diff overhead outweighs gradient advantage. NM x8 is optimal.",
    "variable_projection": "FAILED - Baseline already uses VP (analytical q). Gauss-Newton on positions gets stuck in local minima. CMA-ES + analytical q is optimal.",
    "frequency_domain": "NOT TRIED - Heat equation simplifies in Fourier space. May be faster/more accurate.",
    "neural_operator": "EXHAUSTED - Surrogates fail (sample-specific RMSE). PINN fails (needs gradients). No NN approach viable.",
    "alternative_es": "FAILED - OpenAI ES's diagonal covariance loses correlations. CMA-ES optimal for low-dim expensive problems. Do not pursue Natural ES/PEPG.",
    "differential_evolution": "FAILED - DE's mutation/crossover less efficient than CMA-ES covariance adaptation. Best 1.1325 @ 35.2 min (-0.0037 score). CMA-ES is optimal.",
    "multi_objective": "EXHAUSTED - Scoring formula already IS multi-objective (accuracy + diversity). Pareto optimization cannot improve on direct optimization of combined score.",
    "problem_reformulation": "FAILED - Polar parameterization is WORSE than Cartesian (-0.0216 score, +14.3 min). Rectangular domain incompatible with polar coords. Don't change coordinate systems.",
    "simulated_annealing": "EXHAUSTED - SA is 2-5x slower than CMA-ES with local search, 23% worse without. Do NOT retry.",
    "surrogate_assisted": "ABORTED - RMSE landscapes are sample-specific. Surrogates from some samples don't generalize to others.",
    "cmaes_improvement": "FAILED - Two-phase restart (large sigma then small sigma) doesn't improve. Phase 2 adds overhead without accuracy gain, reduces diversity.",
    "loss_function": "FAILED - Optimizing weighted RMSE finds different optimum than unweighted (scored). Score 1.0131 vs 1.1688. Don't change the loss function.",
    "multi_start": "FAILED - Multi-start elite selection adds overhead without improving accuracy. Early generation fitness NOT predictive of final quality. Single-start with smart init is optimal.",
    "refinement": "EXHAUSTED - Adaptive NM iterations does NOT improve. Fixed 8 NM is optimal. Adaptive batching adds overhead. Source-count based (6/10) worse.",
    "efficiency": "EXHAUSTED - Early rejection adds overhead (191 sims vs 100 baseline). 8.6% rejection rate not enough to break even. 40% temporal is optimal efficiency.",
    "loss_reformulation": "FAILED - Both weighted RMSE and log-RMSE failed. RMSE is optimal objective.",
    "cmaes_enhancement": "EXHAUSTED - Adaptive popsize (2.1x over budget). Combined with IPOP and larger popsize failures: any popsize manipulation hurts. Default CMA-ES popsize is optimal.",
    "temporal_sampling": "FAILED - Random timesteps requires full simulation (3-4x slower). Early timesteps are most informative.",
    "cmaes_variants": "EXHAUSTED - sep-CMA-ES (5.3x over budget, -0.02 score) and dd-CMA-ES (no benefit) both FAILED. Position correlations require full covariance. Standard CMA-ES is optimal.",
    "sigma_tuning": "EXHAUSTED - Lower sigma (0.15/0.18) and higher sigma (0.18/0.22) both WORSE than baseline (0.15/0.20). Baseline sigma is locally optimal.",
    "cmaes_restart_v2": "EXHAUSTED - Both IPOP and BIPOP tested, both FAILED. Problem lacks local optima where restarts help.",
    "meshless_direct": "EXHAUSTED - RBF meshless requires n_rbf_points simulations for observation matrix. Even coarsest 5x3=15 pts exceeds budget. Same issue as D-PBCS.",
    "surrogate_hybrid": "EXHAUSTED - CMA-ES covariance adaptation is already implicit surrogate modeling. ~10% rejection rate, small population (6-8) mean minimal benefit.",
    "gradient_numerical": "EXHAUSTED - SLSQP/L-BFGS-B/Powell all use finite diff gradients. O(n) overhead per iteration makes gradient methods slower than NM for 400ms simulations.",
    "deterministic_global": "EXHAUSTED - DIRECT uses 441-443 evals vs CMA-ES 20-36 (12-22x more). Space partitioning doesn't scale. No covariance adaptation.",
    "alternative_optimizer": "EXHAUSTED - GA proven inferior to CMA-ES (slower, less accurate). Not worth exploring other general-purpose optimizers.",
    "alternative_metaheuristic": "EXHAUSTED - ALL tested metaheuristics (GA, DE, OpenAI ES, SA, GO) FAILED vs CMA-ES. Covariance adaptation is essential for expensive continuous optimization. Do NOT test more metaheuristics.",
    "early_exit": "EXHAUSTED - ALL early termination approaches FAILED. CMA-ES needs full budget for covariance adaptation. Cannot be stopped early."
  },
  "completed_experiments": [
    "EXP_WEIGHTED_LOSS_001",
    "EXP_CMAES_RESTART_BEST_001",
    "CMAES_TUNING_BATCH",
    "EXP_PSO_001",
    "EXP_BASINHOPPING_001",
    "EXP_DIFFEVO_001",
    "EXP_NM_MULTISTART_001",
    "EXP_HYBRID_TWOSTAGE_001",
    "EXP_ULTRACOARSE_001",
    "EXP_ADAPTIVE_SIGMA_001",
    "EXP_OUTLIER_FOCUS_001",
    "EXP_TARGETED_2SRC_001",
    "EXP_SURROGATE_NN_001",
    "EXP_COBYLA_REFINE_001",
    "EXP_FAST_ICA_001",
    "EXP_ENSEMBLE_001",
    "EXP_TRANSFER_LEARN_001",
    "EXP_LQ_CMAES_001",
    "EXP_BAYESIAN_OPT_001",
    "EXP_MULTIFID_OPT_001",
    "EXP_SEQUENTIAL_HANDOFF_001",
    "EXP_FAST_SOURCE_DETECT_001",
    "EXP_WS_CMAES_001",
    "EXP_ADAPTIVE_BUDGET_001",
    "EXP_TEMPORAL_FIDELITY_001",
    "EXP_TEMPORAL_HIGHER_SIGMA_001",
    "EXP_NICHING_CMAES_001",
    "EXP_EXTENDED_POLISH_001",
    "EXP_IPOP_TEMPORAL_001",
    "EXP_ADAPTIVE_TIMESTEP_001",
    "EXP_PHYSICS_INIT_001",
    "EXP_POD_SURROGATE_001",
    "EXP_2SOURCE_FOCUS_001",
    "EXP_ACTIVE_CMAES_001",
    "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
    "EXP_LARGER_POPSIZE_001",
    "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
    "EXP_PARAMETER_SCALING_001",
    "EXP_BOUNDARY_AWARE_INIT_001",
    "EXP_JAX_AUTODIFF_001",
    "EXP_LEVENBERG_MARQUARDT_001",
    "EXP_HYBRID_CMAES_LBFGSB_001",
    "EXP_CONJUGATE_GRADIENT_001",
    "EXP_OPENAI_ES_001",
    "EXP_DIFFERENTIAL_EVOLUTION_001",
    "EXP_SEPARABLE_VP_001",
    "EXP_ADAPTIVE_SA_001",
    "EXP_PRETRAINED_SURROGATE_001",
    "EXP_SURROGATE_CMAES_001",
    "EXP_PINN_DIRECT_001",
    "EXP_MULTISTART_ELITE_001",
    "EXP_ADAPTIVE_NM_POLISH_001",
    "EXP_EARLY_REJECTION_001",
    "EXP_POLAR_PARAM_001",
    "EXP_SOLUTION_INJECTION_001",
    "EXP_LOG_RMSE_LOSS_001",
    "EXP_ADAPTIVE_POPSIZE_001",
    "EXP_RANDOM_TIMESTEPS_001",
    "EXP_DD_CMAES_001",
    "EXP_POWELL_POLISH_001",
    "EXP_LRA_CMAES_001",
    "EXP_COORD_SIGMA_001",
    "EXP_BFGS_POLISH_001",
    "EXP_LOWER_SIGMA_001",
    {
      "id": "EXP_GENETIC_ALGORITHM_001",
      "name": "genetic_algorithm_optimizer",
      "worker": "W1",
      "result": "FAILED",
      "best_score": 1.1615,
      "best_time": 64.8,
      "in_budget": false,
      "reason": "GA fundamentally inferior to CMA-ES - slower and less accurate",
      "completed_at": "2026-01-24T16:06:48.037753Z"
    },
    {
      "id": "EXP_MULTI_OBJECTIVE_001",
      "name": "multi_objective_pareto",
      "worker": "W1",
      "result": "ABORTED",
      "reason": "Theoretically unsound - scoring formula is already multi-objective",
      "completed_at": "2026-01-24T16:10:29.556159Z"
    },
    {
      "id": "EXP_GROWTH_OPTIMIZER_001",
      "name": "growth_optimizer",
      "worker": "W1",
      "result": "ABORTED",
      "reason": "Pattern of metaheuristic failures proves CMA-ES covariance adaptation is essential",
      "completed_at": "2026-01-24T16:12:37.223071Z"
    },
    {
      "id": "EXP_EARLY_EXIT_THRESHOLD_001",
      "name": "confidence_based_early_exit",
      "worker": "W1",
      "result": "ABORTED",
      "reason": "Same as prior failed experiments (ADAPTIVE_BUDGET, EARLY_STOPPING)",
      "completed_at": "2026-01-24T16:13:54.972973Z"
    }
  ]
}