{
  "version": "5.0",
  "last_updated": "2026-01-19T05:00:00Z",
  "notes": "QUEUE REFILLED. W2 SUCCESS with temporal fidelity (1.1362 @ 39 min). Added 6 new experiments building on this breakthrough. New baseline: 1.1362.",

  "experiment_queue": [
    {
      "id": "EXP_LQ_CMAES_001",
      "priority": 99,
      "family": "surrogate_lq",
      "status": "completed",
      "result": "FAILED - API mismatch. fmin_lq_surr returns ONE solution, we need MULTIPLE candidates. 29-71% worse RMSE, 7-17% slower.",
      "score": 0.253,
      "time_min": 64.1,
      "worker": "W1",
      "name": "lq_cma_es_builtin",
      "description": "Use pycma's built-in lq-CMA-ES (linear-quadratic surrogate model)",
      "finding": "fmin_lq_surr is designed for single-objective returning ONE best solution. Our scoring needs MULTIPLE diverse candidates. Fundamental mismatch."
    },
    {
      "id": "EXP_SEQUENTIAL_HANDOFF_001",
      "priority": 99,
      "family": "hybrid",
      "status": "completed",
      "result": "FAILED - Best in-budget 1.1132 is WORSE than baseline 1.1247. Best overall 1.1439 @ 126.5 min (2x over budget).",
      "score": 1.1132,
      "time_min": 56.6,
      "worker": "W2",
      "name": "cmaes_to_nm_sequential",
      "description": "CMA-ES exploration then hand off best to Nelder-Mead run",
      "finding": "Sequential handoff fundamentally doesn't work. NM improves score (+0.0192) when given time, but time overhead is prohibitive."
    },
    {
      "id": "EXP_BAYESIAN_OPT_001",
      "priority": 99,
      "family": "bayesian_opt",
      "status": "completed",
      "result": "FAILED - GP surrogate doesn't model thermal RMSE landscape. 31-91% worse accuracy, or 45% over budget if tuned.",
      "score": 0.31,
      "time_min": 57.6,
      "worker": "W1",
      "name": "bayesian_optimization_gp",
      "description": "Use Bayesian Optimization with Gaussian Process surrogate as alternative to CMA-ES",
      "finding": "Bayesian Optimization is NOT suitable for thermal inverse problem. GP surrogate fails to capture the complex RMSE landscape."
    },
    {
      "id": "EXP_MULTIFID_OPT_001",
      "priority": 99,
      "family": "multi_fidelity",
      "status": "completed",
      "result": "FAILED - Coarse grid RMSE landscape differs from fine grid. Best in-budget: RMSE 0.259 @ 45 min (45% worse than baseline).",
      "score": 0.259,
      "time_min": 44.9,
      "worker": "W1",
      "name": "multi_fidelity_proper_ratio",
      "actual_folder": "multi_fidelity_pyramid",
      "description": "Multi-fidelity optimization with literature-recommended grid ratios",
      "finding": "Multi-fidelity via grid coarsening doesn't work for inverse problems. RMSE landscape is fundamentally different at different resolutions."
    },
    {
      "id": "EXP_FAST_SOURCE_DETECT_001",
      "priority": 99,
      "family": "preprocessing",
      "status": "completed",
      "result": "ABORTED - Invalid premise. n_sources already in sample data.",
      "worker": "W2",
      "name": "fast_source_count_detection",
      "description": "Fast detection of 1-source vs 2-source BEFORE optimization",
      "finding": "Baseline already uses sample['n_sources']. Detection not needed."
    },
    {
      "id": "EXP_WS_CMAES_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - WS-CMA-ES causes divergence. Best: RMSE 0.276 @ 66 min (62% worse than baseline).",
      "score": 0.276,
      "time_min": 65.8,
      "worker": "W1",
      "name": "warm_start_cmaes",
      "description": "Use CyberAgentAILab's WS-CMA-ES to transfer solutions between similar samples",
      "finding": "WS-CMA-ES doesn't work for thermal inverse problems. Each sample is unique with no shared optimization landscape structure."
    },
    {
      "id": "EXP_TRANSFER_LEARN_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - Sensor feature clustering doesn't predict solution similarity",
      "score": 1.0804,
      "time_min": 59.7,
      "worker": "W2",
      "name": "sample_clustering_transfer",
      "actual_folder": "cluster_transfer",
      "description": "Cluster similar samples, transfer solutions between them",
      "finding": "Clustering by sensor features doesn't predict solution similarity. Transfer learning doesn't work for this problem."
    },
    {
      "id": "EXP_ADAPTIVE_BUDGET_001",
      "priority": 99,
      "family": "budget_allocation",
      "status": "completed",
      "result": "FAILED - Early termination hurts accuracy. Best in-budget: 1.1143 @ 56 min (1% worse than baseline).",
      "score": 1.1143,
      "time_min": 56.2,
      "worker": "W1",
      "name": "adaptive_sample_budget",
      "description": "Dynamically allocate fevals per sample based on convergence speed",
      "finding": "Early termination based on sigma/stagnation hurts accuracy. CMA-ES needs full budget."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity",
      "status": "completed",
      "result": "SUCCESS - NEW BEST! Score 1.1362 @ 39 min (+0.0115 vs baseline, 32% faster).",
      "score": 1.1362,
      "time_min": 39.0,
      "worker": "W2",
      "name": "early_timestep_filtering",
      "description": "Use reduced timesteps (40%) for candidate filtering, same spatial grid",
      "finding": "40% timesteps is optimal. Maintains 100x50 spatial grid, RMSE correlation 0.95+. Counterintuitive: more fevals HURT."
    },
    {
      "id": "EXP_TEMPORAL_HIGHER_SIGMA_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Higher sigma increases time disproportionately. Cannot beat W2 baseline within budget.",
      "score": 1.1745,
      "time_min": 63.0,
      "best_in_budget_score": 1.1584,
      "best_in_budget_time": 52.1,
      "worker": "W1",
      "name": "temporal_40pct_higher_sigma",
      "description": "Combine 40% temporal fidelity with higher sigma (0.25/0.30) for better accuracy",
      "finding": "Higher sigma (0.25/0.30) improves score by +0.0057 but adds 5+ min. Reducing polish to stay in budget loses the accuracy gain. W2's sigma 0.18/0.22 is optimal."
    },
    {
      "id": "EXP_NICHING_CMAES_001",
      "priority": 99,
      "family": "diversity",
      "status": "completed",
      "result": "FAILED - Scoring formula AVERAGES accuracy over candidates. Adding worse diverse candidates hurts score. Baseline already at 2.75/3 N_valid.",
      "score": 1.0622,
      "time_min": 46.9,
      "worker": "W2",
      "name": "niching_cmaes_diversity",
      "description": "Use Niching CMA-ES to maintain population diversity for multiple candidate solutions",
      "finding": "Diversity is NOT the bottleneck. Baseline already achieves 80% 3-candidate samples. Taboo-based niching hurts accuracy more than diversity helps."
    },
    {
      "id": "EXP_IPOP_TEMPORAL_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - IPOP adds time overhead (75.7 min) without improving accuracy. Splitting fevals across restarts reduces convergence.",
      "score": 1.1687,
      "time_min": 75.7,
      "worker": "W2",
      "name": "ipop_cmaes_temporal",
      "description": "IPOP-CMA-ES (increasing population) with 40% temporal fidelity",
      "finding": "IPOP divides feval budget across restarts, each restart gets insufficient fevals. Problem doesn't have local optima - restarts don't help."
    },
    {
      "id": "EXP_PHYSICS_INIT_001",
      "priority": 4,
      "family": "initialization",
      "status": "claimed_by_W1",
      "claimed_at": "2026-01-19T09:40:00Z",
      "name": "physics_informed_init",
      "description": "Use sensor temperature gradients to initialize source location estimates",
      "hypothesis": "High temperature sensors likely near heat sources. Gradient analysis can provide better initial guesses than random initialization.",
      "research_source": "Standard practice in inverse problems. Temperature gradient points toward heat source.",
      "why_different": "Current approach uses random smart_init. Physics-based init may start closer to optimum, reducing iterations needed.",
      "implementation": [
        "1. Analyze sensor temperatures at final timestep",
        "2. Find hottest sensor region(s)",
        "3. Estimate source x,y from temperature gradient direction",
        "4. Use estimated positions as CMA-ES initial mean",
        "5. Compare convergence speed and final accuracy vs random init",
        "6. Combine with 40% temporal fidelity"
      ],
      "success_criteria": "Faster convergence (20%+ fewer fevals) OR better accuracy",
      "abort_criteria": "Physics init doesn't beat random init consistently"
    },
    {
      "id": "EXP_ADAPTIVE_TIMESTEP_001",
      "priority": 5,
      "family": "temporal_fidelity_extended",
      "status": "claimed_by_W2",
      "claimed_at": "2026-01-19T10:00:00Z",
      "name": "adaptive_timestep_fraction",
      "description": "Start with 25% timesteps, increase to 40% as CMA-ES converges",
      "hypothesis": "Early CMA-ES iterations need rough landscape estimate (25% timesteps fine). Later iterations benefit from more accurate signal (40% timesteps).",
      "research_source": "Multi-fidelity optimization literature: start cheap, increase fidelity near convergence.",
      "why_different": "Fixed 40% is suboptimal - early iterations overpay, late iterations may need more accuracy.",
      "implementation": [
        "1. First 50% of fevals: use 25% timesteps",
        "2. Last 50% of fevals: use 40% timesteps",
        "3. Alternative: continuous increase 25% -> 40% based on generation",
        "4. Track if this improves speed or accuracy",
        "5. Test thresholds: (25%/50%), (30%/40%), (25%/40%)"
      ],
      "success_criteria": "Score >= 1.13 AND time <= 35 min (faster than fixed 40%)",
      "abort_criteria": "Variable timesteps hurt accuracy vs fixed 40%"
    },
    {
      "id": "EXP_POD_SURROGATE_001",
      "priority": 6,
      "family": "surrogate_pod",
      "status": "available",
      "name": "pod_reduced_order_model",
      "description": "Use POD (Proper Orthogonal Decomposition) as fast surrogate for simulation",
      "hypothesis": "POD-based reduced order model can be 70-100x faster than full simulation",
      "research_source": "ResearchGate: 'POD-based inverse algorithms more than 83 times faster than CFD'",
      "why_different": "POD learns optimal basis functions from simulation snapshots, preserving accuracy while reducing DOFs",
      "implementation": [
        "1. Collect snapshots from existing CMA-ES runs (T(x,y) fields for various source configs)",
        "2. Compute SVD/POD to extract dominant modes (5-10 modes should capture 99% energy)",
        "3. Build POD surrogate: project any new source config to get approximate T field",
        "4. Use POD for CMA-ES candidate filtering (like NN surrogate but physics-based)",
        "5. Only full-simulate candidates with low POD RMSE",
        "6. Compare speed and accuracy"
      ],
      "success_criteria": "Score >= 1.13 AND time <= 35 min",
      "abort_criteria": "POD fails to capture 2-source thermal fields accurately",
      "complexity": "HIGH - requires collecting and processing snapshot data",
      "note": "Re-enabled from paused status. Temporal fidelity success shows cheap surrogates can work."
    },
    {
      "id": "EXP_EXTENDED_POLISH_001",
      "priority": 99,
      "family": "refinement",
      "status": "completed",
      "result": "FAILED - 12 iterations takes 82.3 min (37% over budget) for only +0.0015 score. 8 iterations is optimal.",
      "score": 1.1703,
      "time_min": 82.3,
      "worker": "W2",
      "name": "extended_nm_polish",
      "description": "Increase NM polish iterations from 8 to 12-16 for better accuracy",
      "finding": "More polish iterations exceeds time budget. Each additional iteration adds ~6 min on 80 samples. 8 is already optimal."
    },
    {
      "id": "EXP_LARGER_POPSIZE_001",
      "priority": 7,
      "family": "cmaes_accuracy",
      "status": "available",
      "name": "larger_cmaes_population",
      "description": "Use larger CMA-ES population size for better accuracy in inverse problem",
      "hypothesis": "Research shows larger populations improve convergence quality in inverse problems. With temporal fidelity speedup, we can afford larger populations.",
      "research_source": "Oxford Academic: 'convergence can be further improved by using larger population sizes' for inverse problems",
      "why_different": "Default population=4+floor(3*ln(n)) may be too small. Larger population = better covariance estimation.",
      "implementation": [
        "1. Start with temporal fidelity optimizer (40% timesteps)",
        "2. Increase popsize from default (~8) to 12, 16, 20",
        "3. May need to reduce fevals to stay in budget",
        "4. Monitor accuracy vs time tradeoff",
        "5. Combine with NM polish (full timesteps)"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 59 min",
      "abort_criteria": "Larger population adds too much time without accuracy benefit"
    },
    {
      "id": "EXP_2SOURCE_FOCUS_001",
      "priority": 8,
      "family": "source_specific",
      "status": "available",
      "name": "two_source_specialized",
      "description": "Specialized optimization for 2-source samples (60% of data, higher RMSE)",
      "hypothesis": "2-source samples have higher RMSE and dominate the score. Using different hyperparameters (more fevals, different sigma) for 2-source may improve overall accuracy.",
      "research_source": "Extended polish showed 2-source samples are 2x slower per iteration. They need more optimization effort but also cost more time.",
      "why_different": "Current approach treats 1-src and 2-src the same. 2-src is harder (4D vs 3D search space) and may benefit from specialized treatment.",
      "implementation": [
        "1. Identify 2-source samples from n_sources metadata",
        "2. For 2-source: increase fevals by 20-30%, use higher initial sigma",
        "3. For 1-source: reduce fevals slightly to stay in budget",
        "4. Budget allocation: 40% time to 1-src (40% of samples), 60% to 2-src (60% of samples)",
        "5. Measure improvement on 2-source RMSE specifically"
      ],
      "success_criteria": "2-source RMSE drops from 0.14 to <0.12, overall score >= 1.17",
      "abort_criteria": "Specialized treatment doesn't improve 2-source RMSE"
    },
    {
      "id": "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
      "priority": 9,
      "family": "sigma_scheduling",
      "status": "available",
      "name": "adaptive_sigma_schedule",
      "description": "Start with higher sigma (0.25/0.30), decay to lower sigma (0.18/0.22) during optimization",
      "hypothesis": "Higher sigma enables better exploration early. Lower sigma allows faster convergence late. Schedule combines benefits without time penalty.",
      "research_source": "EXP_TEMPORAL_HIGHER_SIGMA_001 showed sigma 0.25/0.30 finds better solutions but takes too long. Adaptive scheduling may get best of both.",
      "why_different": "Fixed sigma is suboptimal - high sigma is slow but accurate, low sigma is fast but may miss optima. Schedule adapts throughout.",
      "implementation": [
        "1. Start CMA-ES with sigma=0.25/0.30 for first 30% of fevals",
        "2. Linear decay to sigma=0.18/0.22 for remaining fevals",
        "3. Alternative: step-wise decay (3 stages)",
        "4. Track convergence vs fixed baseline",
        "5. Combine with 40% temporal fidelity and 8 NM polish"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 59 min",
      "abort_criteria": "Adaptive schedule doesn't improve over fixed sigma 0.18/0.22"
    },
    {
      "id": "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
      "priority": 10,
      "family": "temporal_fidelity_extended",
      "status": "available",
      "name": "progressive_polish_fidelity",
      "description": "Use progressive timestep fidelity during NM polish: 60% early, 100% final",
      "hypothesis": "Early polish iterations don't need full accuracy since they'll be refined. Saving time on early polish could allow more fevals or iterations.",
      "research_source": "Current approach: CMA-ES (40%) + NM polish (100%). Intermediate fidelity during polish may reduce overhead.",
      "why_different": "Current baseline uses 100% timesteps for all 8 polish iterations. Progressive approach starts cheaper.",
      "implementation": [
        "1. CMA-ES: 40% timesteps (unchanged)",
        "2. NM polish iterations 1-4: 60% timesteps (cheaper)",
        "3. NM polish iterations 5-8: 100% timesteps (accuracy)",
        "4. Alternative: 50%/75%/100% three-stage progression",
        "5. Measure accuracy vs time tradeoff"
      ],
      "success_criteria": "Score >= 1.168 AND time <= 55 min (faster with same accuracy)",
      "abort_criteria": "Progressive fidelity hurts final accuracy"
    },
    {
      "id": "EXP_COARSE_SURROGATE_001",
      "priority": 99,
      "family": "surrogate",
      "status": "deprioritized",
      "name": "coarse_grid_as_surrogate",
      "description": "Use 40x20 coarse grid as cheap surrogate for candidate filtering",
      "note": "DEPRIORITIZED - Multi-fidelity via spatial coarsening PROVEN to fail (EXP_MULTIFID_OPT_001). Different resolution = different landscape."
    },
    {
      "id": "EXP_ENSEMBLE_001",
      "priority": 99,
      "family": "ensemble",
      "status": "completed",
      "result": "FAILED - 347.9 min (5.8x over budget), score 1.0972 (WORSE than baseline 1.1247)",
      "score": 1.0972,
      "time_min": 347.9,
      "worker": "W1",
      "name": "optimizer_ensemble_voting",
      "description": "Run multiple optimizers in parallel, take best result per sample",
      "finding": "Nelder-Mead won 74% but running both optimizers doubles simulation count. Key: REDUCE simulations, not add optimizers."
    },
    {
      "id": "EXP_SURROGATE_NN_001",
      "priority": 99,
      "family": "surrogate",
      "status": "completed",
      "result": "FAILED - Online learning doesn't work with parallel processing",
      "score": 1.1203,
      "time_min": 75.6,
      "worker": "W1",
      "name": "neural_network_surrogate_prefilter"
    },
    {
      "id": "EXP_COBYLA_REFINE_001",
      "priority": 99,
      "family": "gradient_free_local",
      "status": "completed",
      "result": "FAILED - 88.8 min over budget, COBYLA uses 20+ extra evals vs NM's 3-6",
      "score": 1.1235,
      "time_min": 88.8,
      "worker": "W2",
      "name": "cobyla_trust_region_refinement"
    },
    {
      "id": "EXP_FAST_ICA_001",
      "priority": 99,
      "family": "decomposition",
      "status": "completed",
      "result": "FAILED - 151.1 min projected (91 min over budget), coarse-to-fine adds overhead",
      "score": 1.1046,
      "time_min": 151.1,
      "worker": "W2",
      "name": "accelerated_ica_decomposition",
      "finding": "Coarse-to-fine adds massive overhead. ICA itself is fast but extra refinement stage doubles simulation count."
    }
  ],

  "algorithm_families": {
    "evolutionary_cmaes": "EXHAUSTED - 34 experiments, no more tuning",
    "evolutionary_other": "PSO failed, may try genetic algorithms",
    "gradient_based": "ABANDONED - finite differences too slow (adjoint method could change this)",
    "gradient_free_local": "Nelder-Mead OK for refinement, COBYLA too slow",
    "surrogate": "Custom NN failed. lq-CMA-ES FAILED (API mismatch). POD re-enabled.",
    "surrogate_lq": "FAILED - fmin_lq_surr returns ONE solution, we need MULTIPLE candidates",
    "surrogate_pod": "AVAILABLE - Re-enabled, complexity high but temporal fidelity shows cheap surrogates work",
    "decomposition": "ABANDONED - Fast ICA failed due to refinement overhead",
    "ensemble": "ABANDONED - 5.8x over budget, worse score. Fundamentally unworkable.",
    "hybrid": "FAILED - Sequential handoff doesn't fit in budget.",
    "bayesian_opt": "FAILED - GP surrogate doesn't model thermal RMSE landscape.",
    "multi_fidelity": "FAILED - Coarse grid (SPATIAL) RMSE landscape differs from fine grid.",
    "temporal_fidelity": "SUCCESS! 40% timesteps achieves 1.1362 @ 39 min. NEW BASELINE.",
    "temporal_fidelity_extended": "NEW - Building on temporal fidelity success with sigma tuning, IPOP, adaptive timesteps",
    "budget_allocation": "FAILED - Early termination hurts CMA-ES accuracy.",
    "meta_learning": "EXHAUSTED - Both cluster_transfer and WS-CMA-ES FAILED.",
    "preprocessing": "ABORTED - n_sources already in sample data.",
    "diversity": "NEW - Niching CMA-ES for multiple candidate solutions",
    "initialization": "NEW - Physics-informed initialization from sensor gradients"
  },

  "completed_experiments": [
    "CMAES_TUNING_BATCH",
    "EXP_PSO_001",
    "EXP_BASINHOPPING_001",
    "EXP_DIFFEVO_001",
    "EXP_NM_MULTISTART_001",
    "EXP_HYBRID_TWOSTAGE_001",
    "EXP_ULTRACOARSE_001",
    "EXP_ADAPTIVE_SIGMA_001",
    "EXP_OUTLIER_FOCUS_001",
    "EXP_TARGETED_2SRC_001",
    "EXP_SURROGATE_NN_001",
    "EXP_COBYLA_REFINE_001",
    "EXP_FAST_ICA_001",
    "EXP_ENSEMBLE_001",
    "EXP_TRANSFER_LEARN_001",
    "EXP_LQ_CMAES_001",
    "EXP_BAYESIAN_OPT_001",
    "EXP_MULTIFID_OPT_001",
    "EXP_SEQUENTIAL_HANDOFF_001",
    "EXP_FAST_SOURCE_DETECT_001",
    "EXP_WS_CMAES_001",
    "EXP_ADAPTIVE_BUDGET_001",
    "EXP_TEMPORAL_FIDELITY_001",
    "EXP_TEMPORAL_HIGHER_SIGMA_001",
    "EXP_NICHING_CMAES_001",
    "EXP_EXTENDED_POLISH_001"
  ]
}
