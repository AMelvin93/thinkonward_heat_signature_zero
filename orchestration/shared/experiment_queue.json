{
  "last_updated": "2026-01-18T08:00:00Z",
  "updated_by": "ALIGNMENT_FIX",
  "notes": "CRITICAL FIX: Previous experiments used wrong optimizer classes. All aligned to use RobustFallbackOptimizer from experiments/robust_fallback/. Run final submission with: uv run python scripts/run_final_submission.py --workers 7 --shuffle",

  "baseline": {
    "claimed_score": 1.1247,
    "claimed_time_min": 57.2,
    "verified_score": 1.1142,
    "verified_time_min": 62.4,
    "variance_note": "HIGH VARIANCE detected - ~0.01 score variance between runs with same config",
    "optimizer": "RobustFallbackOptimizer",
    "config": {
      "sigma0_1src": 0.15,
      "sigma0_2src": 0.20,
      "threshold_1src": 0.35,
      "threshold_2src": 0.45,
      "max_fevals_1src": 20,
      "max_fevals_2src": 36
    }
  },

  "ready_experiments": [
    {
      "id": "ALIGNED_001",
      "name": "baseline_multi_run",
      "priority": 1,
      "status": "available",
      "description": "Run baseline 3x to measure variance properly",
      "run_command": "uv run python scripts/run_final_submission.py --workers 7 --shuffle --seed 42,43,44",
      "hypothesis": "Measure true variance of baseline config across multiple seeds"
    },
    {
      "id": "ALIGNED_002",
      "name": "sigma_0.20_0.25_solo",
      "priority": 2,
      "status": "available",
      "description": "Test best-scoring sigma config with correct optimizer, solo run",
      "config": {
        "sigma0_1src": 0.20,
        "sigma0_2src": 0.25,
        "threshold_1src": 0.35,
        "threshold_2src": 0.45,
        "max_fevals_1src": 20,
        "max_fevals_2src": 36
      },
      "modify_required": "Add sigma0_1src/sigma0_2src params to RobustFallbackOptimizer",
      "hypothesis": "Verify if 1.1362 score is real and check timing on solo run"
    }
  ],

  "completed_experiments": [
    {"id": "EXP001-037", "note": "Experiments run with WRONG optimizer classes - results may not be comparable to baseline"}
  ],

  "CRITICAL_ISSUE_DISCOVERED": {
    "problem": "32 experiments used wrong optimizer classes",
    "details": {
      "baseline_optimizer": "RobustFallbackOptimizer (CMA-ES + fallback)",
      "worker_experiments_used": [
        "SigmaBaselineOptimizer",
        "Sigma020_025ReducedFevalsOptimizer",
        "MultiFidelityOptimizer",
        "etc."
      ],
      "old_submission_used": "HybridOptimizer (L-BFGS-B - completely different algorithm!)"
    },
    "fix_applied": {
      "updated_file": "scripts/run_final_submission.py",
      "now_uses": "RobustFallbackOptimizer with baseline config",
      "config_updated": "configs/final_submission.yaml"
    },
    "impact": "All 32 previous experiment results may be INVALID for comparison to baseline"
  },

  "scoring_clarification": {
    "formula": "score = (1/(1+RMSE)) + 0.3 * (n_candidates/3)",
    "old_HybridOptimizer_bug": "Generated only 1 candidate, scoring 0.5-0.8",
    "RobustFallbackOptimizer": "Generates 2-3 candidates, scoring 1.1+",
    "difference": "Diversity bonus adds ~0.2 points per sample"
  }
}
