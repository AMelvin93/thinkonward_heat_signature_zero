{
  "version": "3.3",
  "last_updated": "2026-01-18T21:15:00Z",
  "notes": "W0 CYCLE 2: Added 2 more experiments from research - Fast Source Detection and Warm-Start CMA-ES. Total 8 available experiments.",

  "experiment_queue": [
    {
      "id": "EXP_LQ_CMAES_001",
      "priority": 1,
      "family": "surrogate_lq",
      "status": "available",
      "name": "lq_cma_es_builtin",
      "description": "Use pycma's built-in lq-CMA-ES (linear-quadratic surrogate model)",
      "hypothesis": "Global quadratic surrogate can reduce function evaluations by 2-6x while maintaining accuracy",
      "research_source": "GECCO 2019 paper + pycma documentation. Shows 2-6x speedup on benchmark functions.",
      "why_different": "Uses pycma's BUILT-IN surrogate support (cma.fmin_lq_surr), not custom implementation",
      "implementation": [
        "1. Replace cma.CMAEvolutionStrategy with cma.fmin_lq_surr or cma.fmin_lq_surr2",
        "2. Configure: popsize=6-10, maxfevals=20-36 (matching our current budget)",
        "3. The quadratic model is built from Moore-Penrose pseudoinverse",
        "4. Surrogate is only used when rank correlation is high (built-in adaptive)",
        "5. Compare time and accuracy to baseline CMA-ES"
      ],
      "success_criteria": "Score >= 1.12 AND time <= 45 min (25% speedup)",
      "abort_criteria": "Score < 1.08 OR lq-CMA-ES doesn't converge"
    },
    {
      "id": "EXP_ENSEMBLE_001",
      "priority": 1,
      "family": "ensemble",
      "status": "running",
      "claimed_by": "W1",
      "started": "2026-01-18T17:00:00Z",
      "name": "optimizer_ensemble_voting",
      "description": "Run multiple optimizers in parallel, take best result per sample",
      "hypothesis": "Different optimizers excel on different samples; ensemble captures best of each",
      "implementation": [
        "1. For each sample, run: CMA-ES (20 fevals), Nelder-Mead from multiple inits (10 fevals)",
        "2. Take candidate with lowest RMSE from all runs",
        "3. If time permits, do light refinement on winner",
        "4. Track which optimizer won for which sample type"
      ],
      "success_criteria": "Score >= 1.13 AND time <= 60 min",
      "abort_criteria": "Time > 70 min consistently"
    },
    {
      "id": "EXP_BAYESIAN_OPT_001",
      "priority": 2,
      "family": "bayesian_opt",
      "status": "available",
      "name": "bayesian_optimization_gp",
      "description": "Use Bayesian Optimization with Gaussian Process surrogate as alternative to CMA-ES",
      "hypothesis": "BO with GP+EI acquisition may be more sample-efficient for 4-6 parameter optimization",
      "research_source": "Frontiers 2022: 'Bayesian optimization is capable of finding promising solutions for expensive-to-evaluate black box functions'",
      "why_different": "BO uses explicit uncertainty modeling and acquisition functions (EI), unlike CMA-ES population-based search",
      "implementation": [
        "1. Use scikit-optimize (skopt) or GPyOpt for implementation",
        "2. For 1-source: 4 parameters (x, y, q, n_sources)",
        "3. For 2-source: 6 parameters (x1, y1, q1, x2, y2, q2)",
        "4. Acquisition function: Expected Improvement (EI)",
        "5. Initial samples: 5-10 random, then BO for 15-25 iterations",
        "6. Compare to CMA-ES on same budget"
      ],
      "success_criteria": "Score >= 1.12 AND time <= 55 min",
      "abort_criteria": "Score < 1.05 OR BO fails to converge"
    },
    {
      "id": "EXP_MULTIFID_OPT_001",
      "priority": 2,
      "family": "multi_fidelity",
      "status": "available",
      "name": "multi_fidelity_proper_ratio",
      "description": "Multi-fidelity optimization with literature-recommended grid ratios",
      "hypothesis": "4:1 cell ratio between fidelity levels gives 5-10x speedup per level",
      "research_source": "Literature: 'coarse mesh 10^3 cells vs fine mesh 10^4 nodes gives model time ratio of 5-10x'",
      "why_different": "Our current multi-fidelity uses 30x15 which may be too coarse. Use 25x12 -> 50x25 -> 100x50 pyramid.",
      "implementation": [
        "1. Level 1 (coarse): 25x12 grid = 300 cells (17x faster than fine)",
        "2. Level 2 (medium): 50x25 grid = 1250 cells (4x faster than fine)",
        "3. Level 3 (fine): 100x50 grid = 5000 cells (baseline)",
        "4. CMA-ES on coarse for 5-10 generations",
        "5. Transfer top 50% to medium for 3-5 generations",
        "6. Final refinement on fine grid",
        "7. Test with different transfer ratios"
      ],
      "success_criteria": "Score >= 1.11 AND time <= 45 min",
      "abort_criteria": "Coarse grid accuracy too poor for useful transfer"
    },
    {
      "id": "EXP_TRANSFER_LEARN_001",
      "priority": 3,
      "family": "meta_learning",
      "status": "running",
      "claimed_by": "W2",
      "started": "2026-01-18T18:30:00Z",
      "name": "sample_clustering_transfer",
      "description": "Cluster similar samples, transfer solutions between them",
      "hypothesis": "Similar sensor patterns have similar solutions; transfer saves optimization time",
      "implementation": [
        "1. Extract features from sensor readings (peak locations, onset times)",
        "2. Cluster samples using K-means (K=5-10)",
        "3. Solve one sample per cluster fully",
        "4. Use cluster centroid solution to initialize others in cluster",
        "5. Compare to independent optimization"
      ],
      "success_criteria": "Same score with 20%+ time reduction",
      "abort_criteria": "Clustering groups dissimilar samples"
    },
    {
      "id": "EXP_POD_SURROGATE_001",
      "priority": 3,
      "family": "surrogate_pod",
      "status": "available",
      "name": "pod_reduced_order_model",
      "description": "Use POD (Proper Orthogonal Decomposition) as fast surrogate for simulation",
      "hypothesis": "POD-based reduced order model can be 70-100x faster than full simulation",
      "research_source": "ResearchGate: 'POD-based inverse algorithms more than 83 times faster than CFD'",
      "why_different": "POD learns optimal basis functions from simulation snapshots, preserving accuracy while reducing DOFs",
      "implementation": [
        "1. Collect snapshots from existing CMA-ES runs (T(x,y) fields for various source configs)",
        "2. Compute SVD/POD to extract dominant modes (5-10 modes should capture 99% energy)",
        "3. Build POD surrogate: project any new source config to get approximate T field",
        "4. Use POD for CMA-ES candidate filtering (like NN surrogate but physics-based)",
        "5. Only full-simulate candidates with low POD RMSE",
        "6. Compare speed and accuracy"
      ],
      "success_criteria": "Score >= 1.12 AND time <= 40 min (40% speedup)",
      "abort_criteria": "POD fails to capture 2-source thermal fields accurately",
      "complexity": "HIGH - requires collecting and processing snapshot data"
    },
    {
      "id": "EXP_FAST_SOURCE_DETECT_001",
      "priority": 2,
      "family": "preprocessing",
      "status": "available",
      "name": "fast_source_count_detection",
      "description": "Fast detection of 1-source vs 2-source BEFORE optimization",
      "hypothesis": "Simple peak detection on thermal images can determine n_sources, saving time by not running 2-src optimizer on 1-src samples",
      "research_source": "W0 Research Cycle 2: Peak detection in thermal images, ThermoMesh 99% accuracy",
      "why_different": "Currently we may waste time trying 2-source optimization on 1-source samples. Pre-detect and route.",
      "implementation": [
        "1. Use scipy.ndimage.maximum_filter on observed temperature field",
        "2. Find local maxima above threshold (e.g., 0.1 above baseline)",
        "3. Count distinct peaks - if 1 peak, likely 1-source; if 2+, likely 2-source",
        "4. Route sample to appropriate optimizer (1-src: 4D, 2-src: 6D)",
        "5. Measure detection accuracy and time savings"
      ],
      "success_criteria": ">=95% detection accuracy AND 10%+ time reduction",
      "abort_criteria": "Detection accuracy <90%"
    },
    {
      "id": "EXP_WS_CMAES_001",
      "priority": 2,
      "family": "meta_learning",
      "status": "available",
      "name": "warm_start_cmaes",
      "description": "Use CyberAgentAILab's WS-CMA-ES to transfer solutions between similar samples",
      "hypothesis": "Warm starting from similar samples reduces CMA-ES adaptation phase, speeding up convergence",
      "research_source": "AAAI 2021 'Warm Starting CMA-ES for Hyperparameter Optimization' by Nomura et al.",
      "why_different": "Uses formal warm-start method with get_warm_start_mgd() instead of ad-hoc initialization",
      "implementation": [
        "1. Install cmaes library (pip install cmaes) - different from pycma",
        "2. For first N samples: run standard CMA-ES, collect (params, fitness) history",
        "3. For subsequent samples: use get_warm_start_mgd() to estimate initial distribution",
        "4. Pass mean, sigma, covariance to CMA optimizer",
        "5. Track convergence speed improvement"
      ],
      "success_criteria": "Same score with 20%+ time reduction OR better score in same time",
      "abort_criteria": "Warm start causes divergence or negative transfer"
    },
    {
      "id": "EXP_COARSE_SURROGATE_001",
      "priority": 4,
      "family": "surrogate",
      "status": "available",
      "name": "coarse_grid_as_surrogate",
      "description": "Use 40x20 coarse grid as cheap surrogate for candidate filtering (not 30x15 which failed)",
      "hypothesis": "40x20 grid is 6x faster than 100x50; use to filter before fine evaluation",
      "implementation": [
        "1. For each CMA-ES generation: evaluate all candidates on 40x20 grid",
        "2. Filter to top 50% by coarse RMSE",
        "3. Evaluate filtered candidates on 50x25 grid",
        "4. Use 50x25 results for CMA-ES update",
        "5. Final evaluation on 100x50"
      ],
      "success_criteria": "Score >= 1.11 AND time <= 50 min",
      "abort_criteria": "Coarse filtering removes good candidates consistently"
    },
    {
      "id": "EXP_SURROGATE_NN_001",
      "priority": 99,
      "family": "surrogate",
      "status": "completed",
      "result": "FAILED - Online learning doesn't work with parallel processing",
      "score": 1.1203,
      "time_min": 75.6,
      "worker": "W1",
      "name": "neural_network_surrogate_prefilter",
      "description": "Train small NN on (params -> RMSE) to pre-filter bad CMA-ES candidates"
    },
    {
      "id": "EXP_COBYLA_REFINE_001",
      "priority": 99,
      "family": "gradient_free_local",
      "status": "completed",
      "result": "FAILED - 88.8 min over budget, COBYLA uses 20+ extra evals vs NM's 3-6",
      "score": 1.1235,
      "time_min": 88.8,
      "worker": "W2",
      "name": "cobyla_trust_region_refinement"
    },
    {
      "id": "EXP_FAST_ICA_001",
      "priority": 99,
      "family": "decomposition",
      "status": "completed",
      "result": "FAILED - 151.1 min projected (91 min over budget), coarse-to-fine adds overhead",
      "score": 1.1046,
      "time_min": 151.1,
      "worker": "W2",
      "name": "accelerated_ica_decomposition",
      "finding": "Coarse-to-fine (50x25 search + Nelder-Mead refinement) adds massive overhead. ICA itself is fast but the extra refinement stage doubles simulation count."
    }
  ],

  "algorithm_families": {
    "evolutionary_cmaes": "EXHAUSTED - 34 experiments, no more tuning",
    "evolutionary_other": "PSO failed, may try genetic algorithms",
    "gradient_based": "ABANDONED - finite differences too slow (adjoint method could change this)",
    "gradient_free_local": "Nelder-Mead OK for refinement, COBYLA too slow",
    "surrogate": "Custom NN failed. Try pycma's built-in lq-CMA-ES or POD",
    "surrogate_lq": "NEW - pycma's built-in linear-quadratic surrogate. LOW-HANGING FRUIT",
    "surrogate_pod": "NEW - POD-based reduced order model. HIGH potential but complex",
    "decomposition": "ABANDONED - Fast ICA failed due to refinement overhead",
    "ensemble": "In progress (W1)",
    "bayesian_opt": "NEW - Alternative to CMA-ES using GP surrogate",
    "multi_fidelity": "NEW - Proper grid ratios from literature",
    "meta_learning": "In progress - transfer learning between similar samples"
  },

  "completed_experiments": [
    "CMAES_TUNING_BATCH",
    "EXP_PSO_001",
    "EXP_BASINHOPPING_001",
    "EXP_DIFFEVO_001",
    "EXP_NM_MULTISTART_001",
    "EXP_HYBRID_TWOSTAGE_001",
    "EXP_ULTRACOARSE_001",
    "EXP_ADAPTIVE_SIGMA_001",
    "EXP_OUTLIER_FOCUS_001",
    "EXP_TARGETED_2SRC_001",
    "EXP_SURROGATE_NN_001",
    "EXP_COBYLA_REFINE_001",
    "EXP_FAST_ICA_001"
  ]
}
