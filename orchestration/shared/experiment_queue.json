{
  "version": "5.5",
  "last_updated": "2026-01-22T10:00:00Z",
  "notes": "V5.7: Added 3 focused CMA-ES tuning experiments (2026-01-22T13:59:17.573458). coord_sigma, early_stopping, noise_injection.",
  "queue_status": "HEALTHY",
  "experiment_queue": [
    {
      "id": "EXP_LQ_CMAES_001",
      "priority": 99,
      "family": "surrogate_lq",
      "status": "completed",
      "result": "FAILED - API mismatch. fmin_lq_surr returns ONE solution, we need MULTIPLE candidates. 29-71% worse RMSE, 7-17% slower.",
      "score": 0.253,
      "time_min": 64.1,
      "worker": "W1",
      "name": "lq_cma_es_builtin",
      "description": "Use pycma's built-in lq-CMA-ES (linear-quadratic surrogate model)",
      "finding": "fmin_lq_surr is designed for single-objective returning ONE best solution. Our scoring needs MULTIPLE diverse candidates. Fundamental mismatch."
    },
    {
      "id": "EXP_SEQUENTIAL_HANDOFF_001",
      "priority": 99,
      "family": "hybrid",
      "status": "completed",
      "result": "FAILED - Best in-budget 1.1132 is WORSE than baseline 1.1247. Best overall 1.1439 @ 126.5 min (2x over budget).",
      "score": 1.1132,
      "time_min": 56.6,
      "worker": "W2",
      "name": "cmaes_to_nm_sequential",
      "description": "CMA-ES exploration then hand off best to Nelder-Mead run",
      "finding": "Sequential handoff fundamentally doesn't work. NM improves score (+0.0192) when given time, but time overhead is prohibitive."
    },
    {
      "id": "EXP_BAYESIAN_OPT_001",
      "priority": 99,
      "family": "bayesian_opt",
      "status": "completed",
      "result": "FAILED - GP surrogate doesn't model thermal RMSE landscape. 31-91% worse accuracy, or 45% over budget if tuned.",
      "score": 0.31,
      "time_min": 57.6,
      "worker": "W1",
      "name": "bayesian_optimization_gp",
      "description": "Use Bayesian Optimization with Gaussian Process surrogate as alternative to CMA-ES",
      "finding": "Bayesian Optimization is NOT suitable for thermal inverse problem. GP surrogate fails to capture the complex RMSE landscape."
    },
    {
      "id": "EXP_MULTIFID_OPT_001",
      "priority": 99,
      "family": "multi_fidelity",
      "status": "completed",
      "result": "FAILED - Coarse grid RMSE landscape differs from fine grid. Best in-budget: RMSE 0.259 @ 45 min (45% worse than baseline).",
      "score": 0.259,
      "time_min": 44.9,
      "worker": "W1",
      "name": "multi_fidelity_proper_ratio",
      "actual_folder": "multi_fidelity_pyramid",
      "description": "Multi-fidelity optimization with literature-recommended grid ratios",
      "finding": "Multi-fidelity via grid coarsening doesn't work for inverse problems. RMSE landscape is fundamentally different at different resolutions."
    },
    {
      "id": "EXP_FAST_SOURCE_DETECT_001",
      "priority": 99,
      "family": "preprocessing",
      "status": "completed",
      "result": "ABORTED - Invalid premise. n_sources already in sample data.",
      "worker": "W2",
      "name": "fast_source_count_detection",
      "description": "Fast detection of 1-source vs 2-source BEFORE optimization",
      "finding": "Baseline already uses sample['n_sources']. Detection not needed."
    },
    {
      "id": "EXP_WS_CMAES_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - WS-CMA-ES causes divergence. Best: RMSE 0.276 @ 66 min (62% worse than baseline).",
      "score": 0.276,
      "time_min": 65.8,
      "worker": "W1",
      "name": "warm_start_cmaes",
      "description": "Use CyberAgentAILab's WS-CMA-ES to transfer solutions between similar samples",
      "finding": "WS-CMA-ES doesn't work for thermal inverse problems. Each sample is unique with no shared optimization landscape structure."
    },
    {
      "id": "EXP_TRANSFER_LEARN_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - Sensor feature clustering doesn't predict solution similarity",
      "score": 1.0804,
      "time_min": 59.7,
      "worker": "W2",
      "name": "sample_clustering_transfer",
      "actual_folder": "cluster_transfer",
      "description": "Cluster similar samples, transfer solutions between them",
      "finding": "Clustering by sensor features doesn't predict solution similarity. Transfer learning doesn't work for this problem."
    },
    {
      "id": "EXP_ADAPTIVE_BUDGET_001",
      "priority": 99,
      "family": "budget_allocation",
      "status": "completed",
      "result": "FAILED - Early termination hurts accuracy. Best in-budget: 1.1143 @ 56 min (1% worse than baseline).",
      "score": 1.1143,
      "time_min": 56.2,
      "worker": "W1",
      "name": "adaptive_sample_budget",
      "description": "Dynamically allocate fevals per sample based on convergence speed",
      "finding": "Early termination based on sigma/stagnation hurts accuracy. CMA-ES needs full budget."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity",
      "status": "completed",
      "result": "SUCCESS - NEW BEST! Score 1.1362 @ 39 min (+0.0115 vs baseline, 32% faster).",
      "score": 1.1362,
      "time_min": 39.0,
      "worker": "W2",
      "name": "early_timestep_filtering",
      "description": "Use reduced timesteps (40%) for candidate filtering, same spatial grid",
      "finding": "40% timesteps is optimal. Maintains 100x50 spatial grid, RMSE correlation 0.95+. Counterintuitive: more fevals HURT."
    },
    {
      "id": "EXP_TEMPORAL_HIGHER_SIGMA_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Higher sigma increases time disproportionately. Cannot beat W2 baseline within budget.",
      "score": 1.1745,
      "time_min": 63.0,
      "best_in_budget_score": 1.1584,
      "best_in_budget_time": 52.1,
      "worker": "W1",
      "name": "temporal_40pct_higher_sigma",
      "description": "Combine 40% temporal fidelity with higher sigma (0.25/0.30) for better accuracy",
      "finding": "Higher sigma (0.25/0.30) improves score by +0.0057 but adds 5+ min. Reducing polish to stay in budget loses the accuracy gain. W2's sigma 0.18/0.22 is optimal."
    },
    {
      "id": "EXP_NICHING_CMAES_001",
      "priority": 99,
      "family": "diversity",
      "status": "completed",
      "result": "FAILED - Scoring formula AVERAGES accuracy over candidates. Adding worse diverse candidates hurts score. Baseline already at 2.75/3 N_valid.",
      "score": 1.0622,
      "time_min": 46.9,
      "worker": "W2",
      "name": "niching_cmaes_diversity",
      "description": "Use Niching CMA-ES to maintain population diversity for multiple candidate solutions",
      "finding": "Diversity is NOT the bottleneck. Baseline already achieves 80% 3-candidate samples. Taboo-based niching hurts accuracy more than diversity helps."
    },
    {
      "id": "EXP_IPOP_TEMPORAL_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - IPOP adds time overhead (75.7 min) without improving accuracy. Splitting fevals across restarts reduces convergence.",
      "score": 1.1687,
      "time_min": 75.7,
      "worker": "W2",
      "name": "ipop_cmaes_temporal",
      "description": "IPOP-CMA-ES (increasing population) with 40% temporal fidelity",
      "finding": "IPOP divides feval budget across restarts, each restart gets insufficient fevals. Problem doesn't have local optima - restarts don't help."
    },
    {
      "id": "EXP_PHYSICS_INIT_001",
      "priority": 99,
      "family": "initialization",
      "status": "completed",
      "result": "FAILED - Gradient init is WORSE than smart init: -0.0046 score, +2.3 min",
      "score": 1.1639,
      "time_min": 69.6,
      "worker": "W1",
      "name": "physics_informed_init",
      "description": "Use sensor temperature gradients to initialize source location estimates",
      "finding": "Temperature gradients at sensors don't accurately point to sources. Heat diffusion corrupts gradient signal. Simple hottest-sensor init is already optimal."
    },
    {
      "id": "EXP_ADAPTIVE_TIMESTEP_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Adaptive timestep switching during CMA-ES is counterproductive. Best: 1.1635 @ 69.9 min (both worse than baseline).",
      "score": 1.1635,
      "time_min": 69.9,
      "worker": "W2",
      "name": "adaptive_timestep_fraction",
      "description": "Start with 25% timesteps, increase to 40% as CMA-ES converges",
      "finding": "CMA-ES covariance adaptation requires consistent fitness landscape. Switching fidelity mid-run disrupts learning and leads to worse performance."
    },
    {
      "id": "EXP_POD_SURROGATE_001",
      "priority": 99,
      "family": "surrogate_pod",
      "status": "completed",
      "result": "ABORTED - POD not viable for this problem due to sample-specific physics (kappa varies). Can't pre-build universal basis.",
      "worker": "W2",
      "name": "pod_reduced_order_model",
      "description": "Use POD (Proper Orthogonal Decomposition) as fast surrogate for simulation",
      "finding": "Each sample has unique physics (kappa, bc, T0). POD requires pre-computed snapshots from SAME system. Online POD would need simulations, defeating purpose. Temporal fidelity already provides similar speedup."
    },
    {
      "id": "EXP_EXTENDED_POLISH_001",
      "priority": 99,
      "family": "refinement",
      "status": "completed",
      "result": "FAILED - 12 iterations takes 82.3 min (37% over budget) for only +0.0015 score. 8 iterations is optimal.",
      "score": 1.1703,
      "time_min": 82.3,
      "worker": "W2",
      "name": "extended_nm_polish",
      "description": "Increase NM polish iterations from 8 to 12-16 for better accuracy",
      "finding": "More polish iterations exceeds time budget. Each additional iteration adds ~6 min on 80 samples. 8 is already optimal."
    },
    {
      "id": "EXP_LARGER_POPSIZE_001",
      "priority": 99,
      "family": "cmaes_accuracy",
      "status": "completed",
      "result": "FAILED - Popsize=12 adds 14 min without improving accuracy. Default popsize is optimal.",
      "score": 1.1666,
      "time_min": 73.0,
      "worker": "W1",
      "name": "larger_cmaes_population",
      "description": "Use larger CMA-ES population size for better accuracy in inverse problem",
      "finding": "Larger popsize reduces generations with fixed feval budget. CMA-ES needs multiple generations to adapt covariance. Default popsize (4+3*ln(n)) is already optimal."
    },
    {
      "id": "EXP_2SOURCE_FOCUS_001",
      "priority": 99,
      "family": "source_specific",
      "status": "completed",
      "result": "FAILED - Specialized feval allocation hurts performance. Reducing 1-src hurts accuracy, increasing 2-src doesn't help.",
      "score": 1.162,
      "time_min": 69.3,
      "worker": "W2",
      "name": "two_source_specialized",
      "description": "Specialized optimization for 2-source samples (60% of data, higher RMSE)",
      "finding": "Baseline 20/36 feval split is already optimal. 2-source is harder due to 4D search space, not under-optimization. More fevals don't help."
    },
    {
      "id": "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
      "priority": 99,
      "family": "sigma_scheduling",
      "status": "completed",
      "result": "ABORTED - Prior evidence shows this approach will fail. CMA-ES already adapts sigma naturally.",
      "worker": "W1",
      "name": "adaptive_sigma_schedule",
      "description": "Start with higher sigma (0.25/0.30), decay to lower sigma (0.18/0.22) during optimization",
      "finding": "EXP_TEMPORAL_HIGHER_SIGMA_001 showed high sigma adds time without in-budget accuracy gain. EXP_ADAPTIVE_TIMESTEP_001 showed changing conditions mid-run disrupts CMA-ES. No point testing a known-to-fail approach."
    },
    {
      "id": "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "ABORTED - Prior experiment showed truncated polish HURTS accuracy. 40% polish gave -0.0346 score. 60% would similarly hurt.",
      "worker": "W2",
      "name": "progressive_polish_fidelity",
      "description": "Use progressive timestep fidelity during NM polish: 60% early, 100% final",
      "finding": "NM polish requires full timesteps for accurate refinement. Truncated polish 'overfits to proxy noise'. Full timestep polish is optimal."
    },
    {
      "id": "EXP_ACTIVE_CMAES_001",
      "priority": 99,
      "family": "cmaes_variants",
      "status": "completed",
      "result": "ABORTED - Wrong premise. CMA_active already defaults to True in pycma. Baseline already uses active CMA-ES.",
      "worker": "W2",
      "name": "active_cmaes_covariance",
      "description": "Use Active CMA-ES variant with enhanced negative covariance update",
      "finding": "pycma's CMA_active option defaults to True. The baseline already uses active covariance update. No experiment needed."
    },
    {
      "id": "EXP_PARAMETER_SCALING_001",
      "priority": 99,
      "family": "problem_specific",
      "status": "completed",
      "result": "ABORTED - Wrong premise. CMA-ES only optimizes positions (x,y). Intensity (q) is computed analytically via least squares. No scaling needed.",
      "worker": "W1",
      "name": "weighted_parameter_scaling",
      "description": "Different scaling/weights for position (x,y) vs intensity (q) parameters",
      "finding": "Optimizer already separates position (CMA-ES) from intensity (analytical). CMA-ES only sees (x,y) or (x1,y1,x2,y2), not q."
    },
    {
      "id": "EXP_BOUNDARY_AWARE_INIT_001",
      "priority": 99,
      "family": "initialization_v2",
      "status": "completed",
      "result": "ABORTED - Data analysis shows 24% of samples have boundary hotspots. Biasing away would hurt these cases.",
      "worker": "W1",
      "name": "boundary_aware_initialization",
      "description": "Initialize heat sources away from domain boundaries where physics differs",
      "finding": "24% of samples have hottest sensor near boundary (<10% margin). Abort criteria met: biasing away hurts boundary source cases. initialization_v2 family should be marked EXHAUSTED."
    },
    {
      "id": "EXP_COARSE_SURROGATE_001",
      "priority": 99,
      "family": "surrogate",
      "status": "deprioritized",
      "name": "coarse_grid_as_surrogate",
      "description": "Use 40x20 coarse grid as cheap surrogate for candidate filtering",
      "note": "DEPRIORITIZED - Multi-fidelity via spatial coarsening PROVEN to fail (EXP_MULTIFID_OPT_001). Different resolution = different landscape."
    },
    {
      "id": "EXP_ENSEMBLE_001",
      "priority": 99,
      "family": "ensemble",
      "status": "completed",
      "result": "FAILED - 347.9 min (5.8x over budget), score 1.0972 (WORSE than baseline 1.1247)",
      "score": 1.0972,
      "time_min": 347.9,
      "worker": "W1",
      "name": "optimizer_ensemble_voting",
      "description": "Run multiple optimizers in parallel, take best result per sample",
      "finding": "Nelder-Mead won 74% but running both optimizers doubles simulation count. Key: REDUCE simulations, not add optimizers."
    },
    {
      "id": "EXP_SURROGATE_NN_001",
      "priority": 99,
      "family": "surrogate",
      "status": "completed",
      "result": "FAILED - Online learning doesn't work with parallel processing",
      "score": 1.1203,
      "time_min": 75.6,
      "worker": "W1",
      "name": "neural_network_surrogate_prefilter"
    },
    {
      "id": "EXP_COBYLA_REFINE_001",
      "priority": 99,
      "family": "gradient_free_local",
      "status": "completed",
      "result": "FAILED - 88.8 min over budget, COBYLA uses 20+ extra evals vs NM's 3-6",
      "score": 1.1235,
      "time_min": 88.8,
      "worker": "W2",
      "name": "cobyla_trust_region_refinement"
    },
    {
      "id": "EXP_FAST_ICA_001",
      "priority": 99,
      "family": "decomposition",
      "status": "completed",
      "result": "FAILED - 151.1 min projected (91 min over budget), coarse-to-fine adds overhead",
      "score": 1.1046,
      "time_min": 151.1,
      "worker": "W2",
      "name": "accelerated_ica_decomposition",
      "finding": "Coarse-to-fine adds massive overhead. ICA itself is fast but extra refinement stage doubles simulation count."
    },
    {
      "id": "EXP_CONJUGATE_GRADIENT_001",
      "priority": 99,
      "family": "adjoint_gradient",
      "status": "completed",
      "result": "FAILED - Adjoint gradient is 5-6 orders of magnitude too small. Manual derivation errors cause L-BFGS-B to see near-zero gradient and not iterate.",
      "score": 0.9006,
      "time_min": 56.5,
      "worker": "W2",
      "name": "conjugate_gradient_adjoint",
      "description": "Conjugate Gradient Method with adjoint equation for O(1) gradient computation",
      "finding": "Gradient verification: adjoint=[-0.000017, 0.000004], finite_diff=[-9.079, 1.980]. Ratio ~0.000002. Manual adjoint derivation for ADI time-stepping is error-prone. Use JAX autodiff instead."
    },
    {
      "id": "EXP_JAX_AUTODIFF_001",
      "priority": 99,
      "family": "differentiable_simulation",
      "status": "completed",
      "result": "ABORTED - Fundamental incompatibility. Explicit Euler stability constraint requires 4x more timesteps than implicit ADI.",
      "worker": "W1",
      "name": "jax_differentiable_solver",
      "description": "Rewrite thermal simulator in JAX for automatic differentiation",
      "finding": "JAX autodiff requires explicit time-stepping. Stability constraint dt < 0.002061 vs original dt=0.004 means 4x more timesteps needed. Running fewer timesteps gives wrong physics (5.7x temperature error). Implicit ADI is essential for efficiency."
    },
    {
      "id": "EXP_HYBRID_CMAES_LBFGSB_001",
      "priority": 99,
      "family": "hybrid_gradient",
      "status": "completed",
      "result": "FAILED - L-BFGS-B polish NOT better than NM polish. Finite diff overhead (O(n) extra sims/iter) outweighs gradient advantage. Best in-budget L-BFGS-B: 1.1174 @ 42 min (WORSE than NM x8: 1.1415 @ 38 min).",
      "score": 1.1174,
      "time_min": 42.4,
      "worker": "W1",
      "name": "cmaes_then_gradient_refinement",
      "description": "Use CMA-ES for global search, then L-BFGS-B with finite differences for final polish",
      "finding": "L-BFGS-B finite diff gradient requires O(n) extra sims per iteration. For 4D (2-src): ~250-350 sims vs NM's ~180. NM x8 is optimal polish method. hybrid_gradient family FAILED."
    },
    {
      "id": "EXP_OPENAI_ES_001",
      "priority": 99,
      "family": "alternative_es",
      "status": "completed",
      "result": "FAILED - OpenAI ES cannot beat CMA-ES. Diagonal covariance loses correlation information. Best: 1.1204 @ 43.3 min (-0.0158 score, +4.3 min vs baseline).",
      "score": 1.1204,
      "time_min": 43.3,
      "worker": "W1",
      "name": "openai_evolution_strategy",
      "description": "Try OpenAI's Evolution Strategy as alternative to CMA-ES",
      "finding": "OpenAI ES's diagonal covariance approximation loses critical parameter correlations that CMA-ES captures. For low-dim expensive black-box optimization, CMA-ES is optimal. alternative_es family FAILED."
    },
    {
      "id": "EXP_PRETRAINED_SURROGATE_001",
      "priority": 99,
      "family": "neural_operator",
      "status": "completed",
      "name": "pretrained_nn_surrogate",
      "description": "Train neural network on many simulations OFFLINE, then use as fast surrogate",
      "hypothesis": "Previous NN surrogate failed due to online learning + parallel issues. Pre-training OFFLINE on 10K+ simulations creates a fixed surrogate that works with parallel processing.",
      "research_source": "FNO inverse problems paper (AISTATS 2025). Neural operators can learn PDE solution operators with high accuracy.",
      "why_different": "Previous approach: online learning during optimization (failed). New approach: offline pre-training, fixed surrogate during optimization.",
      "implementation": [
        "1. Generate training data: 10K random source configs -> RMSE values",
        "2. Train small MLP: (x, y, [x2, y2]) -> predicted_RMSE",
        "3. Use trained surrogate for candidate filtering in CMA-ES",
        "4. Only full-simulate candidates with low surrogate RMSE",
        "5. Compare total sims needed vs baseline"
      ],
      "success_criteria": "Reduce simulations by 50%+ while maintaining score >= 1.16",
      "abort_criteria": "Surrogate accuracy too poor or training data generation too expensive",
      "result": "ABORTED - RMSE landscape is sample-specific (avg correlation -0.167). Universal surrogate cannot predict RMSE without sample information.",
      "worker": "W2",
      "finding": "Tested RMSE landscape correlation across 5 samples. Average Spearman r = -0.167 (anti-correlated). Each sample has unique physics (kappa, bc, Y_obs). Surrogate needs sample info, defeating purpose."
    },
    {
      "id": "EXP_PINN_DIRECT_001",
      "priority": 99,
      "family": "neural_operator",
      "status": "completed",
      "name": "pinn_inverse_heat_source",
      "description": "Use Physics-Informed Neural Network to directly solve the inverse heat source problem",
      "hypothesis": "PINNs can directly optimize for heat source parameters by encoding physics constraints in the loss. Recent 2025 paper specifically addresses heat source field inversion.",
      "research_source": "Heat source field inversion with PINN (ScienceDirect 2025). ASME Journal review on PINNs for heat transfer.",
      "why_different": "Direct approach: NN takes sensor readings, outputs source parameters. No iterative optimization loop.",
      "implementation": [
        "1. Define PINN architecture: input=sensor_temps, output=source_params",
        "2. Loss = data_mismatch + physics_residual (heat equation)",
        "3. Train on sensor observations to find source parameters",
        "4. Use PyTorch or TensorFlow with autodiff",
        "5. Compare accuracy and inference speed"
      ],
      "success_criteria": "Score >= 1.15 AND inference time < 1 sec per sample",
      "abort_criteria": "PINN requires too much training time per sample",
      "result": "ABORTED - PINN requires efficient gradients. Adjoint failed (1e-6 error), JAX failed (stability), finite diff too slow (99 min vs 60 budget).",
      "worker": "W2",
      "finding": "L-BFGS-B achieves 15% better RMSE than NM but takes 2.3x longer due to finite diff. ADI solver blocks autodiff. Gradient-free methods (CMA-ES + NM) remain optimal."
    },
    {
      "id": "EXP_LEVENBERG_MARQUARDT_001",
      "priority": 99,
      "family": "nonlinear_least_squares",
      "status": "completed",
      "result": "FAILED - LM is local optimizer that gets stuck in local minima. Finite diff Jacobian needs 3-5 sims/iter. Best in-budget: 1.0350 @ 56 min (-9% vs baseline).",
      "score": 1.035,
      "time_min": 56.0,
      "worker": "W1",
      "name": "levenberg_marquardt_inverse",
      "description": "Use Levenberg-Marquardt algorithm via scipy.optimize.least_squares for inverse heat problem",
      "finding": "LM fundamentally unsuitable: (1) local optimizer vs multi-modal landscape, (2) expensive finite diff Jacobian (3-5 sims/iter), (3) CMA-ES is 3x more sample-efficient. nonlinear_least_squares family should be marked FAILED."
    },
    {
      "id": "EXP_TRUST_REGION_REFLECTIVE_001",
      "priority": 99,
      "family": "nonlinear_least_squares",
      "status": "deprioritized",
      "name": "trust_region_reflective_bounded",
      "description": "Use Trust-Region Reflective method for bounded inverse heat problem",
      "deprioritize_reason": "DEPRIORITIZED based on EXP_LEVENBERG_MARQUARDT_001 results. TRF is also a local optimizer like LM - will have same local minima issues on multi-modal RMSE landscape. Same expensive finite-diff Jacobian problem. Not worth testing.",
      "original_hypothesis": "TRF handles bounded constraints naturally (source positions have box constraints). Our x,y are bounded to domain [0,Lx]x[0,Ly]. TRF respects bounds without penalty methods, potentially more accurate."
    },
    {
      "id": "EXP_SEPARABLE_VP_001",
      "priority": 99,
      "family": "variable_projection",
      "status": "completed",
      "result": "FAILED - VP's Gauss-Newton is local optimizer that gets stuck. Baseline already uses VP implicitly (analytical q). Best: 1.0025 @ 54.3 min (-0.1337 score).",
      "score": 1.0025,
      "time_min": 54.3,
      "worker": "W1",
      "name": "variable_projection_separable",
      "description": "Exploit separable structure via Variable Projection: optimize positions, solve intensity analytically",
      "finding": "Baseline already uses VP (analytical q for each CMA-ES candidate). Gauss-Newton on positions gets stuck in local minima. CMA-ES + analytical q is optimal for exploiting separability."
    },
    {
      "id": "EXP_ADAPTIVE_SA_001",
      "priority": 99,
      "family": "simulated_annealing",
      "status": "completed",
      "name": "adaptive_simulated_annealing",
      "description": "Use Adaptive Simulated Annealing (ASA) for heat source inverse problem",
      "hypothesis": "2024 Nature paper shows ASA successfully reconstructs internal heat sources in biological tissue. ASA adaptively adjusts temperature schedule based on search state, improving global optimization for complex inverse problems.",
      "research_source": "Noninvasive reconstruction of internal heat source in biological tissue using ASA (Nature Scientific Reports, July 2024). ASA avoids complex regularization by treating inverse problem as optimization.",
      "why_different": "SA is a global optimizer like CMA-ES but with different exploration mechanism (probabilistic uphill moves). ASA adapts cooling schedule dynamically. May find different optima than CMA-ES. Very different from local methods (LM) that failed.",
      "implementation": [
        "1. Use scipy.optimize.dual_annealing (generalized SA with local search)",
        "2. Set bounds for source positions",
        "3. Use same objective function as baseline (RMSE after q optimization)",
        "4. Compare global search efficiency vs CMA-ES",
        "5. If promising, implement full ASA with adaptive cooling"
      ],
      "success_criteria": "Score >= 1.16 AND time <= 55 min",
      "abort_criteria": "SA converges slower than CMA-ES OR gets stuck in same local minima",
      "result": "FAILED - dual_annealing is fundamentally unsuitable. With local search: 2-5x over budget. Without: 23% worse accuracy.",
      "score": 0.8666,
      "time_min": 28.2,
      "worker": "W2",
      "finding": "SA local search uses L-BFGS-B finite differences = slow. Pure SA = fast but inaccurate. CMA-ES covariance adaptation is more sample-efficient."
    },
    {
      "id": "EXP_SURROGATE_CMAES_001",
      "priority": 99,
      "family": "surrogate_assisted",
      "status": "completed",
      "name": "surrogate_assisted_cmaes",
      "description": "Use kriging/GP surrogate to pre-filter CMA-ES candidates, reducing simulation count",
      "hypothesis": "SCR algorithm (2025) shows surrogate-assisted CMA-ES works well for expensive black-box functions. Build kriging surrogate from initial samples, use it to filter unpromising candidates before full simulation.",
      "research_source": "SCR: Surrogate-CMAES-RQLIF for expensive black-box optimization (Optimization and Engineering, March 2025). Automotive crashworthiness optimization with surrogate-based hyperparameter tuning.",
      "why_different": "Previous surrogate (NN online) failed due to parallel processing. This approach: (1) builds surrogate from existing samples, (2) uses kriging not NN, (3) only filters obviously bad candidates. More conservative than full surrogate replacement.",
      "implementation": [
        "1. Run baseline CMA-ES on first 10 samples, collect (params, RMSE) pairs",
        "2. Fit kriging/GP model to collected data",
        "3. For remaining samples: use surrogate to filter CMA-ES population",
        "4. Only simulate candidates with low predicted RMSE (top 50%)",
        "5. Update surrogate periodically with new data"
      ],
      "success_criteria": "Reduce simulations by 30%+ while maintaining score >= 1.15",
      "abort_criteria": "Surrogate filtering removes good candidates OR overhead exceeds savings",
      "result": "ABORTED - Prior finding: RMSE landscapes are sample-specific (avg correlation -0.167). Kriging surrogate from samples 1-10 will NOT generalize to samples 11-80.",
      "worker": "W2",
      "finding": "Based on EXP_PRETRAINED_SURROGATE_001 results: RMSE = f(x, y, Y_obs, kappa, bc) is sample-specific. Surrogate needs sample info to work, defeating the purpose."
    },
    {
      "id": "EXP_DIFFERENTIAL_EVOLUTION_001",
      "priority": 99,
      "family": "differential_evolution",
      "status": "completed",
      "result": "FAILED - DE cannot match CMA-ES accuracy. Best: 1.1325 @ 35.2 min (-0.0037 score, -3.8 min vs baseline). CMA-ES covariance adaptation is essential.",
      "score": 1.1325,
      "time_min": 35.2,
      "worker": "W1",
      "name": "differential_evolution_inverse",
      "description": "Use scipy.optimize.differential_evolution for inverse heat problem",
      "finding": "DE's mutation/crossover is less efficient than CMA-ES's covariance adaptation. DE is 10% faster but sacrifices accuracy. differential_evolution family FAILED.",
      "success_criteria": "Score >= 1.16 AND time <= 55 min",
      "abort_criteria": "DE converges slower than CMA-ES OR same local minima issue"
    },
    {
      "id": "EXP_WEIGHTED_LOSS_001",
      "priority": 99,
      "family": "loss_function",
      "status": "completed",
      "result": "FAILED - Optimizing weighted RMSE finds different optimum than unweighted (scored). Score 1.0131 (-0.1557). Diversity destroyed.",
      "score": 1.0131,
      "time_min": 90.7,
      "worker": "W1",
      "name": "weighted_sensor_loss",
      "description": "Weight sensor contributions by their informativeness or reliability",
      "finding": "CRITICAL: Do NOT change loss function - optimize exactly what you are scored on. loss_function family FAILED."
    },
    {
      "id": "EXP_POLAR_PARAM_001",
      "priority": 99,
      "family": "problem_reformulation",
      "status": "completed",
      "result": "FAILED - Rectangular domain incompatible with polar coords. Score 1.1472 @ 72.7 min vs baseline.",
      "score": 1.1472,
      "time_min": 72.7,
      "worker": "W2",
      "name": "polar_parameterization",
      "description": "Reparameterize source positions using polar coordinates centered on domain centroid",
      "finding": "Cartesian is optimal for rectangular domains. problem_reformulation family FAILED."
    },
    {
      "id": "EXP_EARLY_REJECTION_001",
      "priority": 99,
      "family": "efficiency",
      "status": "completed",
      "result": "FAILED - Two-stage evaluation ADDS overhead (191 sims vs 100 baseline). Only 8.6% rejection rate - need >25% to break even. Efficiency family EXHAUSTED.",
      "score": 1.1598,
      "time_min": 146.7,
      "worker": "W1",
      "name": "early_rejection_partial_sim",
      "description": "Reject clearly bad candidates early using partial simulation (first 10% timesteps)",
      "finding": "Early rejection fundamentally flawed: filter cost not offset by savings. CMA-ES candidates cluster near optima, most pass filter. Baseline 40% temporal is optimal."
    },
    {
      "id": "EXP_ADAPTIVE_NM_POLISH_001",
      "priority": 4,
      "family": "refinement",
      "status": "completed",
      "name": "adaptive_nm_iterations",
      "description": "Dynamically adjust NM polish iterations based on convergence rate",
      "hypothesis": "Some samples converge quickly (4-5 NM iters), others need more (10+). Fixed 8 iters is wasteful for easy samples, insufficient for hard ones.",
      "research_source": "Adaptive algorithms adjust parameters based on progress. Early stopping when converged saves compute.",
      "why_different": "Current baseline uses fixed 8 NM iterations. Adaptive could be more efficient.",
      "implementation": [
        "1. Start NM polish with initial iterations (e.g., 4)",
        "2. Check convergence: delta_rmse < threshold?",
        "3. If not converged and iterations < max (e.g., 12), continue",
        "4. If converged early, stop and save time",
        "5. Track time savings and accuracy"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (save time on easy samples)",
      "abort_criteria": "Adaptive logic overhead exceeds savings OR accuracy drops",
      "worker": "W2",
      "result": "FAILED - Adaptive NM iterations does NOT improve. Best: 1.1607 @ 78.3 min (-0.0081 score, +19.9 min vs baseline).",
      "score": 1.1607,
      "time_min": 78.3,
      "finding": "Fixed 8 NM iterations is already optimal. Adaptive batching adds overhead. Source-count based (6/10) is worse. refinement family EXHAUSTED."
    },
    {
      "id": "EXP_CMAES_RESTART_BEST_001",
      "priority": 99,
      "family": "cmaes_improvement",
      "status": "completed",
      "result": "FAILED - Two-phase restart doubles sims (175.7 min), causes diversity loss (1.7 vs 3 candidates), no accuracy improvement. Score 1.0986 vs baseline 1.1688.",
      "score": 1.0986,
      "time_min": 175.7,
      "worker": "W1",
      "name": "cmaes_restart_from_best",
      "description": "Run short CMA-ES, restart from best solution found with tighter sigma",
      "finding": "CMA-ES converges well in single phase. Restart wastes budget re-converging to same solutions. Small sigma in phase 2 destroys diversity."
    },
    {
      "id": "EXP_GRADIENT_INIT_001",
      "priority": 99,
      "family": "initialization",
      "status": "invalid",
      "name": "gradient_informed_init",
      "description": "Use temperature gradient direction to inform initial source position guess",
      "hypothesis": "Heat sources create temperature gradients pointing away from them. Tracing gradient backward could give better init than just hottest sensor.",
      "research_source": "Physics-based initialization. Heat equation has predictable gradient structure.",
      "why_different": "Current smart init uses hottest sensor position. Gradient-informed could be more accurate.",
      "implementation": [
        "1. Compute temperature gradient at each sensor location",
        "2. Trace gradient backward (uphill in temp) from hot sensors",
        "3. Estimate source position as intersection point or max temp region",
        "4. Use as initial guess for CMA-ES",
        "5. Compare with hottest-sensor init"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Gradient-based init is not consistently better than hottest-sensor",
      "invalidation_reason": "DUPLICATE of EXP_PHYSICS_INIT_001 which FAILED. That experiment tested 'sensor temperature gradients to initialize source location estimates' and found gradient init WORSE than hottest-sensor (-0.0046 score, +2.3 min). This experiment proposes essentially the same thing. initialization family already marked EXHAUSTED."
    },
    {
      "id": "EXP_MULTISTART_ELITE_001",
      "priority": 99,
      "family": "multi_start",
      "status": "completed",
      "name": "multistart_elite_selection",
      "description": "Run multiple short CMA-ES from different inits, continue only the best 1-2",
      "hypothesis": "Instead of running N full optimizations, run N short ones and continue only the most promising. More efficient budget allocation.",
      "research_source": "Elite selection in multi-start optimization. Racing algorithms.",
      "why_different": "Current approach runs all inits to completion. Elite selection may be more efficient.",
      "implementation": [
        "1. Start 4-5 CMA-ES runs from different smart inits",
        "2. After 5 generations, evaluate all",
        "3. Continue only top 1-2 performers",
        "4. Run to full convergence",
        "5. Compare with baseline multi-start"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (better budget allocation)",
      "abort_criteria": "Elite selection kills good runs OR overhead exceeds savings",
      "worker": "W2",
      "result": "FAILED - Multi-start overhead dominates. Early fitness not predictive. Score 1.1529 @ 67 min vs baseline 1.1688 @ 58.4 min.",
      "score": 1.1529,
      "time_min": 67.0,
      "finding": "Single-start with good initialization is already optimal. multi_start family FAILED."
    },
    {
      "id": "EXP_LOG_RMSE_LOSS_001",
      "priority": 99,
      "family": "loss_reformulation",
      "status": "completed",
      "result": "FAILED - Monotonic transformation adds overhead without benefit. Score 1.1677 @ 70.5 min vs baseline.",
      "score": 1.1677,
      "time_min": 70.5,
      "worker": "W2",
      "name": "log_rmse_loss",
      "description": "Use log(1+RMSE) instead of RMSE as optimization objective",
      "finding": "loss_reformulation family FAILED. RMSE is optimal objective."
    },
    {
      "id": "EXP_SOLUTION_INJECTION_001",
      "priority": 99,
      "family": "population_enhancement",
      "status": "completed",
      "result": "FAILED - Sequential execution kills parallelism. Score 0.9929 @ 122.5 min vs baseline 1.1688 @ 58.4 min.",
      "score": 0.9929,
      "time_min": 122.5,
      "worker": "W1",
      "name": "solution_injection_cmaes",
      "description": "Inject best solutions from early inits into later CMA-ES populations (GloMPO-inspired)",
      "finding": "Information sharing between inits not worth losing parallelism. population_enhancement family FAILED."
    },
    {
      "id": "EXP_TIKHONOV_REG_001",
      "priority": 4,
      "family": "regularization",
      "status": "available",
      "name": "tikhonov_regularized_loss",
      "description": "Add Tikhonov regularization penalty to RMSE for smoother solutions",
      "hypothesis": "Regularization is standard for ill-posed inverse problems. Penalizing extreme source positions/intensities may improve robustness. WARNING: Adding any penalty term changes the optimum. Must verify regularized optimum still has good unweighted RMSE.",
      "research_source": "Modern regularization methods for inverse problems (Acta Numerica). SpanReg (Nature 2022). Tikhonov widely used.",
      "why_different": "Current baseline uses pure RMSE. Adding regularization may stabilize optimization.",
      "implementation": [
        "1. Modified objective: RMSE + lambda * ||params - prior||^2",
        "2. Prior = domain center for positions, 1.0 for intensity",
        "3. Tune lambda on small subset first",
        "4. Compare solution quality and robustness"
      ],
      "success_criteria": "Score >= 1.17 AND more robust (lower variance across samples)",
      "abort_criteria": "Regularization biases solutions away from true optima (same issue as weighted_sensor_loss)"
    },
    {
      "id": "EXP_ADAPTIVE_POPSIZE_001",
      "priority": 4,
      "family": "cmaes_enhancement",
      "status": "claimed_by_W1",
      "worker": "W1",
      "name": "adaptive_population_size",
      "description": "Start with larger population for exploration, reduce for exploitation",
      "hypothesis": "Different from IPOP (which increases). Start large (popsize=12) for first 60% of fevals, then reduce to default (6-8) for fine-tuning.",
      "research_source": "CMA-ES with adaptive population (Hansen). Inverse of IPOP logic may work for expensive functions.",
      "why_different": "IPOP failed (more restarts hurt). This is single-run with decreasing population.",
      "implementation": [
        "1. Phase 1 (60% fevals): popsize=12, broader exploration",
        "2. Phase 2 (40% fevals): popsize=default, focused convergence",
        "3. No restart - single continuous run",
        "4. Compare with fixed popsize baseline"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Large popsize wastes budget without improving exploration"
    },
    {
      "id": "EXP_COORD_SIGMA_001",
      "priority": 3,
      "family": "cmaes_tuning",
      "status": "available",
      "name": "coordinate_wise_sigma",
      "description": "Use different initial sigma for positions (x,y) vs intensity (q)",
      "hypothesis": "Positions are bounded [0,2]x[0,1] but intensity is [0.5,2.0]. Different scales suggest different exploration rates. CMA-ES sigma0 applies uniformly - coordinate-specific may help.",
      "research_source": "CMA-ES supports csigma parameter for coordinate-wise sigma. Low-dimensional tuning.",
      "why_different": "Baseline uses uniform sigma. Position errors are larger impact than intensity errors for RMSE.",
      "implementation": [
        "1. Use sigma_x=sigma_y=0.15 for positions (tight)",
        "2. Use sigma_q=0.30 for intensity (looser)",
        "3. Implement via initial covariance matrix scaling",
        "4. Compare with uniform sigma baseline"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Coordinate-wise sigma doesn't improve or complicates convergence"
    },
    {
      "id": "EXP_EARLY_STOP_CMA_001",
      "priority": 3,
      "family": "efficiency",
      "status": "available",
      "name": "cmaes_early_stopping",
      "description": "Stop CMA-ES early if best solution hasn't improved for K generations",
      "hypothesis": "Many samples converge quickly (5-8 gens) but baseline runs full budget. Early stopping could save time on easy samples for more polish.",
      "research_source": "CMA-ES convergence criteria. Stagnation detection.",
      "why_different": "Baseline uses fixed fevals. Adaptive stopping based on convergence could reallocate budget.",
      "implementation": [
        "1. Track best fitness over last 3 generations",
        "2. If improvement < 1% for 3 consecutive gens, stop early",
        "3. Reallocate saved budget to NM polish (up to 12 iters)",
        "4. Compare time savings vs accuracy impact"
      ],
      "success_criteria": "Score >= 1.16 AND time <= 50 min (significant time savings)",
      "abort_criteria": "Early stopping hurts accuracy more than it saves time"
    },
    {
      "id": "EXP_NOISE_INJECTION_001",
      "priority": 4,
      "family": "exploration",
      "status": "available",
      "name": "noise_injection_escape",
      "description": "Inject small noise into CMA-ES solutions when convergence stagnates",
      "hypothesis": "If CMA-ES converges prematurely to local optimum, small noise injection may help escape. Similar to simulated annealing restart.",
      "research_source": "Restarting strategies for evolutionary algorithms. Noise injection for escaping local optima.",
      "why_different": "Baseline CMA-ES has no explicit escape mechanism. Noise injection adds controlled exploration.",
      "implementation": [
        "1. Monitor CMA-ES sigma - if it drops below 0.01, inject noise",
        "2. Noise: Gaussian with std=0.05 added to best solution",
        "3. Continue CMA-ES from noisy point",
        "4. Limit noise injections to 2 per run"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 58 min",
      "abort_criteria": "Noise injection disrupts convergence or adds overhead without benefit"
    },
    {
      "id": "EXP_POWELL_POLISH_001",
      "priority": 3,
      "family": "local_search",
      "status": "available",
      "name": "powell_polish_instead_nm",
      "description": "Use Powell's method (coordinate-wise search) instead of Nelder-Mead for final polish",
      "hypothesis": "Powell's method performs coordinate-wise line searches which may be more efficient for the 2-4D search space than NM's simplex operations. Could converge faster or to better optima.",
      "research_source": "scipy.optimize.minimize(method='powell'). Coordinate descent methods popular for low-dimensional problems.",
      "why_different": "Baseline uses NM which moves simplex vertices. Powell moves along coordinate axes independently.",
      "implementation": [
        "1. Replace NM polish with scipy.optimize.minimize(method='powell')",
        "2. Same maxiter setting (8) for fair comparison",
        "3. Compare convergence rate and final RMSE",
        "4. Check if coordinate independence helps or hurts"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Powell converges slower than NM or gets stuck in saddle points"
    },
    {
      "id": "EXP_RANDOM_TIMESTEPS_001",
      "priority": 4,
      "family": "temporal_sampling",
      "status": "claimed_by_W2",
      "worker": "W2",
      "name": "random_timestep_selection",
      "description": "Use random 40% of timesteps instead of first 40% for CMA-ES evaluation",
      "hypothesis": "First 40% timesteps may miss important late-time dynamics. Random sampling captures different parts of temporal evolution, potentially providing more informative RMSE signal.",
      "research_source": "Stochastic sampling in reduced-order models. Random subsets can capture global behavior better than contiguous subsets.",
      "why_different": "Baseline uses first 40% timesteps (early dynamics only). Random samples entire time window.",
      "implementation": [
        "1. Pre-select random 40% of timestep indices (once per sample)",
        "2. Use same indices for all CMA-ES candidates in that sample",
        "3. Compare RMSE correlation with full-timestep RMSE",
        "4. Compare final optimization quality"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Random timesteps give worse RMSE correlation than contiguous (current method)"
    }
  ],
  "algorithm_families": {
    "evolutionary_cmaes": "EXHAUSTED - 34 experiments, no more tuning",
    "evolutionary_other": "PSO failed, may try genetic algorithms",
    "gradient_based": "EXHAUSTED - Adjoint wrong (1e-6 error), JAX blocked (ADI stability), finite diff too slow (99 min). NM is optimal.",
    "gradient_free_local": "Nelder-Mead OK for refinement, COBYLA too slow",
    "surrogate": "Custom NN failed. lq-CMA-ES FAILED (API mismatch). POD re-enabled.",
    "surrogate_lq": "FAILED - fmin_lq_surr returns ONE solution, we need MULTIPLE candidates",
    "surrogate_pod": "FAILED - Sample-specific physics (kappa varies) prevents universal POD basis. Online POD defeats purpose.",
    "decomposition": "ABANDONED - Fast ICA failed due to refinement overhead",
    "ensemble": "ABANDONED - 5.8x over budget, worse score. Fundamentally unworkable.",
    "hybrid": "FAILED - Sequential handoff doesn't fit in budget.",
    "bayesian_opt": "FAILED - GP surrogate doesn't model thermal RMSE landscape.",
    "multi_fidelity_spatial": "FAILED - Coarse grid (SPATIAL) RMSE landscape differs from fine grid.",
    "temporal_fidelity": "SUCCESS! 40% timesteps achieves 1.1362 @ 39 min. NEW BASELINE.",
    "temporal_fidelity_extended": "EXHAUSTED - sigma tuning, IPOP, adaptive timesteps all failed",
    "budget_allocation": "FAILED - Early termination hurts CMA-ES accuracy.",
    "meta_learning": "EXHAUSTED - Both cluster_transfer and WS-CMA-ES FAILED.",
    "preprocessing": "ABORTED - n_sources already in sample data.",
    "diversity": "EXHAUSTED - Niching hurts accuracy more than it helps diversity",
    "initialization": "EXHAUSTED - Gradient-based init WORSE than simple hottest-sensor. Smart init is optimal.",
    "initialization_v2": "EXHAUSTED - 24% samples have boundary sources. Biasing away hurts. Smart init is optimal.",
    "---NEW FAMILIES TO RESEARCH---": "========================================",
    "adjoint_gradient": "FAILED - Manual adjoint derivation is error-prone for ADI time-stepping. Gradient 1e-6 of correct value. Do NOT retry manual adjoint.",
    "differentiable_simulation": "FAILED - Explicit Euler stability requires 4x more timesteps than implicit ADI. JAX autodiff incompatible.",
    "nonlinear_least_squares": "FAILED - LM is local optimizer that gets stuck in local minima. Finite diff Jacobian too expensive. TRF likely same issue.",
    "hybrid_gradient": "FAILED - L-BFGS-B polish NOT better than NM. Finite diff overhead outweighs gradient advantage. NM x8 is optimal.",
    "variable_projection": "FAILED - Baseline already uses VP (analytical q). Gauss-Newton on positions gets stuck in local minima. CMA-ES + analytical q is optimal.",
    "frequency_domain": "NOT TRIED - Heat equation simplifies in Fourier space. May be faster/more accurate.",
    "neural_operator": "EXHAUSTED - Surrogates fail (sample-specific RMSE). PINN fails (needs gradients). No NN approach viable.",
    "alternative_es": "FAILED - OpenAI ES's diagonal covariance loses correlations. CMA-ES optimal for low-dim expensive problems. Do not pursue Natural ES/PEPG.",
    "differential_evolution": "FAILED - DE's mutation/crossover less efficient than CMA-ES covariance adaptation. Best 1.1325 @ 35.2 min (-0.0037 score). CMA-ES is optimal.",
    "multi_objective": "NOT TRIED - Pareto optimization of accuracy/speed jointly.",
    "problem_reformulation": "FAILED - Polar parameterization is WORSE than Cartesian (-0.0216 score, +14.3 min). Rectangular domain incompatible with polar coords. Don't change coordinate systems.",
    "simulated_annealing": "EXHAUSTED - SA is 2-5x slower than CMA-ES with local search, 23% worse without. Do NOT retry.",
    "surrogate_assisted": "ABORTED - RMSE landscapes are sample-specific. Surrogates from some samples don't generalize to others.",
    "cmaes_improvement": "FAILED - Two-phase restart (large sigma then small sigma) doesn't improve. Phase 2 adds overhead without accuracy gain, reduces diversity.",
    "loss_function": "FAILED - Optimizing weighted RMSE finds different optimum than unweighted (scored). Score 1.0131 vs 1.1688. Don't change the loss function.",
    "multi_start": "FAILED - Multi-start elite selection adds overhead without improving accuracy. Early generation fitness NOT predictive of final quality. Single-start with smart init is optimal.",
    "refinement": "EXHAUSTED - Adaptive NM iterations does NOT improve. Fixed 8 NM is optimal. Adaptive batching adds overhead. Source-count based (6/10) worse.",
    "efficiency": "EXHAUSTED - Early rejection adds overhead (191 sims vs 100 baseline). 8.6% rejection rate not enough to break even. 40% temporal is optimal efficiency.",
    "loss_reformulation": "FAILED - Both weighted RMSE and log-RMSE failed. RMSE is optimal objective."
  },
  "completed_experiments": [
    "EXP_WEIGHTED_LOSS_001",
    "EXP_CMAES_RESTART_BEST_001",
    "CMAES_TUNING_BATCH",
    "EXP_PSO_001",
    "EXP_BASINHOPPING_001",
    "EXP_DIFFEVO_001",
    "EXP_NM_MULTISTART_001",
    "EXP_HYBRID_TWOSTAGE_001",
    "EXP_ULTRACOARSE_001",
    "EXP_ADAPTIVE_SIGMA_001",
    "EXP_OUTLIER_FOCUS_001",
    "EXP_TARGETED_2SRC_001",
    "EXP_SURROGATE_NN_001",
    "EXP_COBYLA_REFINE_001",
    "EXP_FAST_ICA_001",
    "EXP_ENSEMBLE_001",
    "EXP_TRANSFER_LEARN_001",
    "EXP_LQ_CMAES_001",
    "EXP_BAYESIAN_OPT_001",
    "EXP_MULTIFID_OPT_001",
    "EXP_SEQUENTIAL_HANDOFF_001",
    "EXP_FAST_SOURCE_DETECT_001",
    "EXP_WS_CMAES_001",
    "EXP_ADAPTIVE_BUDGET_001",
    "EXP_TEMPORAL_FIDELITY_001",
    "EXP_TEMPORAL_HIGHER_SIGMA_001",
    "EXP_NICHING_CMAES_001",
    "EXP_EXTENDED_POLISH_001",
    "EXP_IPOP_TEMPORAL_001",
    "EXP_ADAPTIVE_TIMESTEP_001",
    "EXP_PHYSICS_INIT_001",
    "EXP_POD_SURROGATE_001",
    "EXP_2SOURCE_FOCUS_001",
    "EXP_ACTIVE_CMAES_001",
    "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
    "EXP_LARGER_POPSIZE_001",
    "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
    "EXP_PARAMETER_SCALING_001",
    "EXP_BOUNDARY_AWARE_INIT_001",
    "EXP_JAX_AUTODIFF_001",
    "EXP_LEVENBERG_MARQUARDT_001",
    "EXP_HYBRID_CMAES_LBFGSB_001",
    "EXP_CONJUGATE_GRADIENT_001",
    "EXP_OPENAI_ES_001",
    "EXP_DIFFERENTIAL_EVOLUTION_001",
    "EXP_SEPARABLE_VP_001",
    "EXP_ADAPTIVE_SA_001",
    "EXP_PRETRAINED_SURROGATE_001",
    "EXP_SURROGATE_CMAES_001",
    "EXP_PINN_DIRECT_001",
    "EXP_MULTISTART_ELITE_001",
    "EXP_ADAPTIVE_NM_POLISH_001",
    "EXP_EARLY_REJECTION_001",
    "EXP_POLAR_PARAM_001",
    "EXP_SOLUTION_INJECTION_001",
    "EXP_LOG_RMSE_LOSS_001"
  ]
}