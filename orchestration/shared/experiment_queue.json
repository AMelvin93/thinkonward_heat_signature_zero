{
  "last_updated": "2026-01-18T12:00:00Z",
  "updated_by": "W0_RESEARCH_ORCHESTRATOR",
  "notes": "Comprehensive multi-algorithm exploration. CMA-ES exhausted after 32+ experiments. Time to try fundamentally different approaches.",

  "current_best": {
    "score": 1.1247,
    "time_min": 57.2,
    "algorithm": "cmaes_robust_fallback",
    "config": "threshold_1src=0.35, threshold_2src=0.45, sigma=0.15/0.20, fevals=20/36",
    "run_date": "2026-01-17"
  },

  "best_ever": {
    "score": 1.1362,
    "time_min": 69.2,
    "algorithm": "cmaes_high_sigma",
    "config": "sigma=0.20/0.25, threshold=0.35/0.45",
    "note": "OVER BUDGET by 9.2 min - proves headroom exists",
    "run_date": "2026-01-17"
  },

  "target": {
    "score": 1.25,
    "time_max_min": 60,
    "gap_from_current": 0.1253,
    "gap_percentage": "10.1%"
  },

  "leaderboard_context": {
    "our_position": 8,
    "top_5_threshold": 1.1533,
    "gap_to_top_5": 0.0286,
    "top_2_threshold": 1.2265,
    "gap_to_top_2": 0.1018
  },

  "approach_status": {
    "cmaes_multifidelity": {
      "experiments_run": 32,
      "best_score": 1.1362,
      "best_time_min": 69.2,
      "in_budget_best": 1.1247,
      "status": "EXHAUSTED",
      "key_findings": [
        "Higher sigma (0.20/0.25) improves score by +0.0115 but adds ~10 min",
        "Threshold tuning has diminishing returns",
        "~68 min runtime is from PRIMARY optimization, NOT fallbacks",
        "Cannot beat baseline within budget - fundamental tradeoff"
      ],
      "recommendation": "MOVE ON - try fundamentally different algorithms"
    },
    "lbfgs_multistart": {
      "experiments_run": 5,
      "best_score": 0.77,
      "status": "ABANDONED",
      "key_findings": [
        "Gets stuck in local minima for 2-source problems",
        "Fast when it works, but unreliable",
        "Uses finite differences for gradients: 2n evals per gradient",
        "W2 hybrid test showed 352 min (5.9x over budget) due to gradient overhead",
        "L-BFGS-B is fundamentally incompatible with expensive simulators without analytic gradients"
      ],
      "recommendation": "SKIP - incompatible with expensive simulators"
    },
    "nelder_mead": {
      "experiments_run": 4,
      "best_score": 1.03,
      "best_time_min": 240.1,
      "status": "TESTED_FAILED",
      "key_findings": [
        "Used within multi-fidelity as refinement step works well",
        "As primary optimizer with multistart: too slow and lower accuracy",
        "With 5/8 starts x 50 iter: 1.03 score @ 240 min (4x over budget)",
        "With 2/3 starts x 20 iter: 1.01 score @ 93 min (still over budget)",
        "Simplex method inherently requires more function evaluations than CMA-ES"
      ],
      "recommendation": "KEEP as refinement step only, NOT as primary optimizer"
    },
    "bayesian_optimization": {
      "experiments_run": 1,
      "status": "TESTED_FAILED",
      "key_findings": [
        "GP overhead too high for time budget"
      ],
      "recommendation": "Skip - overhead doesn't pay off"
    },
    "differential_evolution": {
      "experiments_run": 1,
      "status": "TESTED_FAILED",
      "best_score": 0.9502,
      "best_time_min": 60.2,
      "potential": "NONE - poor convergence without covariance adaptation",
      "key_findings": [
        "With aggressive settings (popsize=2, maxiter=6/10): score=0.95 @ 60 min",
        "With original settings (popsize=10, maxiter=50): score=1.03 @ 122 min (over budget)",
        "DE lacks CMA-ES's covariance adaptation - converges slower",
        "Cannot match CMA-ES score within time budget"
      ],
      "recommendation": "SKIP - CMA-ES is superior for this problem"
    },
    "basin_hopping": {
      "experiments_run": 1,
      "status": "TESTED_FAILED",
      "best_score": 1.0573,
      "best_time_min": 1105.2,
      "potential": "NONE - fundamentally too slow",
      "key_findings": [
        "L-BFGS-B does ~50-80 function evaluations per call",
        "Basin hopping does multiple L-BFGS-B runs per restart",
        "Total: 500-800 simulator calls per sample (vs CMA-ES 20-36)",
        "Even with reduced params (niter=5, restarts=2): 1105 min projected"
      ],
      "recommendation": "SKIP - algorithm incompatible with expensive simulators"
    },
    "hybrid_twostage": {
      "experiments_run": 1,
      "status": "TESTED_FAILED",
      "best_score": 1.1459,
      "best_time_min": 352.0,
      "potential": "NONE - L-BFGS-B too slow due to gradient estimation",
      "key_findings": [
        "L-BFGS-B uses finite differences for gradients: 2n function calls per gradient",
        "For 4D (2-source): ~8 evals per gradient * 15 iters * 3 candidates = 360+ evals/sample",
        "CMA-ES with 8-12 fevals is MUCH cheaper than L-BFGS-B refinement",
        "352 min projected (5.9x over budget) despite slight score improvement (+0.02)",
        "Same issue as basin_hopping: L-BFGS-B incompatible with expensive simulators"
      ],
      "recommendation": "SKIP - L-BFGS-B refinement adds massive overhead without analytic gradients"
    },
    "reduced_feval_conservative": {
      "experiments_run": 3,
      "best_score": 1.1201,
      "best_time_min": 63.4,
      "status": "TESTED_PARTIAL",
      "key_findings": [
        "18/32 fevals reduces time but loses ~0.01 score",
        "18/34 fevals: 1.1201 @ 63.4 min - still over budget and lower score",
        "Reduced fevals does NOT save time sufficiently",
        "Sample 57 major outlier (RMSE 0.7686) hurts overall score"
      ],
      "recommendation": "Stick with 20/36 baseline fevals"
    }
  },

  "ready_experiments": [
    {
      "id": "EXP_ULTRACOARSE_001",
      "algorithm": "cmaes_ultracoarse",
      "name": "cmaes_30x15_coarse_grid",
      "priority": 1,
      "status": "available",
      "assigned_to": null,
      "implementation": {
        "base": "experiments/multi_fidelity/",
        "modify": "Use 30x15 coarse grid instead of 50x25"
      },
      "config": {
        "coarse_grid": [30, 15],
        "fine_grid": [100, 50],
        "fevals_1src": 20,
        "fevals_2src": 36,
        "sigma0_1src": 0.20,
        "sigma0_2src": 0.25,
        "threshold_1src": 0.35,
        "threshold_2src": 0.45
      },
      "hypothesis": "Ultra-coarse grid (30x15) is 2.8x faster than current coarse (50x25). If CMA-ES can still converge with noisier objective, total time reduces by ~10+ min, allowing higher sigma within budget.",
      "success_criteria": "Score >= 1.13 AND time <= 60 min",
      "abandon_criteria": "Score < 1.08 OR accuracy degradation on fine grid"
    },
    {
      "id": "EXP_OUTLIER_FOCUS_001",
      "algorithm": "cmaes_outlier_handling",
      "name": "sample_57_specific_handling",
      "priority": 1,
      "status": "available",
      "assigned_to": null,
      "implementation": {
        "base": "experiments/robust_fallback/",
        "modify": "Add sample difficulty detection and adaptive fevals"
      },
      "config": {
        "base_fevals_1src": 20,
        "base_fevals_2src": 36,
        "hard_sample_extra_fevals": 10,
        "hard_sample_detection": "initial_rmse > 0.4",
        "sigma0_1src": 0.15,
        "sigma0_2src": 0.20,
        "threshold_1src": 0.35,
        "threshold_2src": 0.45
      },
      "hypothesis": "Sample 57 and similar outliers consistently fail. Detect hard samples early (high initial RMSE) and allocate extra fevals. May improve outliers without slowing easy samples.",
      "success_criteria": "Score >= 1.13 AND time <= 60 min",
      "abandon_criteria": "Time > 65 min OR no improvement on outliers"
    },
    {
      "id": "EXP_ADAPTIVE_SIGMA_001",
      "algorithm": "cmaes_adaptive_sigma",
      "name": "sigma_schedule_exploration_to_refinement",
      "priority": 2,
      "status": "available",
      "assigned_to": null,
      "implementation": {
        "base": "experiments/robust_fallback/",
        "modify": "Start with high sigma for exploration, reduce for refinement"
      },
      "config": {
        "initial_sigma_1src": 0.30,
        "initial_sigma_2src": 0.35,
        "final_sigma_1src": 0.10,
        "final_sigma_2src": 0.15,
        "sigma_decay": "linear_over_fevals",
        "fevals_1src": 20,
        "fevals_2src": 36
      },
      "hypothesis": "High sigma early explores broadly, low sigma late refines. May get benefits of high sigma exploration without full time cost.",
      "success_criteria": "Score >= 1.12 AND time <= 58 min",
      "abandon_criteria": "Score < 1.10 OR time > 65 min"
    }
  ],

  "completed_experiments": [
    {
      "id": "CMAES_BATCH",
      "note": "32+ CMA-ES experiments completed (Sessions 15-19). See ITERATION_LOG.md for details.",
      "summary": {
        "best_score": 1.1362,
        "best_time": 69.2,
        "in_budget_best": 1.1247,
        "conclusion": "CMA-ES approach exhausted - higher sigma helps accuracy but exceeds time budget"
      }
    },
    {
      "id": "EXP_BASINHOPPING_001",
      "algorithm": "basin_hopping",
      "name": "basin_hopping_lbfgs_refinement",
      "run_date": "2026-01-18",
      "assigned_to": "W1",
      "result": "FAILED",
      "score": 1.0573,
      "time_min": 1105.2,
      "analysis": "Basin hopping fundamentally too slow - L-BFGS-B does 50-80 fevals per call, totaling 500-800 simulator calls per sample vs CMA-ES's 20-36. Even reduced params (niter=5, restarts=2) projected 1105 min (18x over budget). Algorithm designed for cheap function evaluations, not expensive simulators.",
      "key_insight": "Scipy basin_hopping incompatible with expensive simulators - stick with population-based methods like CMA-ES"
    },
    {
      "id": "EXP_DIFFEVO_001",
      "algorithm": "differential_evolution",
      "name": "differential_evolution_multistart",
      "run_date": "2026-01-18",
      "assigned_to": "W1",
      "result": "FAILED",
      "score": 0.9502,
      "time_min": 60.2,
      "analysis": "DE lacks CMA-ES's covariance adaptation. Tested multiple configs: aggressive (popsize=2, maxiter=6/10) gave 0.95 score @ 60 min (abandon criteria met); original settings gave 1.03 @ 122 min (over budget). Cannot achieve CMA-ES-level accuracy within time budget.",
      "key_insight": "CMA-ES's covariance adaptation is crucial for convergence on this problem - simpler evolutionary algorithms don't substitute"
    },
    {
      "id": "EXP_NM_MULTISTART_001",
      "algorithm": "nelder_mead",
      "name": "nelder_mead_pure_multistart",
      "run_date": "2026-01-18",
      "assigned_to": "W1",
      "result": "FAILED",
      "score": 1.0315,
      "time_min": 240.1,
      "analysis": "Pure Nelder-Mead multistart is too slow. With 5/8 starts x 50 iter: 1.03 @ 240 min (4x over budget). With reduced 2/3 starts x 20 iter: 1.01 @ 93 min (still over budget, worse accuracy). Simplex method requires more function evaluations than CMA-ES per iteration.",
      "key_insight": "Nelder-Mead works well as refinement step (3 iters on top-2 candidates) but NOT as primary optimizer"
    },
    {
      "id": "EXP_REDUCED_FEVAL_001",
      "algorithm": "cmaes_reduced_feval",
      "name": "cmaes_18_34_baseline_sigma",
      "run_date": "2026-01-18",
      "assigned_to": "W1",
      "result": "PARTIAL",
      "score": 1.1201,
      "time_min": 63.4,
      "analysis": "Reduced fevals (18/34) did not improve over baseline. Score dropped to 1.1201 (-0.0046 vs baseline 1.1247) AND time is 63.4 min (still over budget). Sample 57 is major outlier (RMSE 0.7686). Reduced fevals doesn't save enough time and hurts accuracy.",
      "key_insight": "Cannot reduce fevals below 20/36 without losing both accuracy and time savings"
    },
    {
      "id": "EXP_HYBRID_TWOSTAGE_001",
      "algorithm": "hybrid_twostage",
      "name": "fast_cmaes_lbfgs_refine",
      "run_date": "2026-01-18",
      "assigned_to": "W2",
      "result": "FAILED",
      "score": 1.1459,
      "time_min": 352.0,
      "analysis": "Hybrid two-stage is massively over budget (5.9x). L-BFGS-B uses finite differences for gradients requiring 2n function calls per gradient. For 4D (2-source): ~8 evals per gradient * 15 iters * 3 candidates = 360+ evals per sample. CMA-ES with 8-12 fevals is actually MUCH cheaper. Score improved slightly (+0.02) but irrelevant given 352 min projected time.",
      "key_insight": "L-BFGS-B without analytic gradients is incompatible with expensive simulators - same issue as basin_hopping"
    }
  ],

  "research_queue": [
    {
      "topic": "Neural network surrogate models",
      "search_queries": [
        "neural network surrogate PDE optimization 2025",
        "fast surrogate model heat equation inverse problem"
      ],
      "status": "researched",
      "priority": "medium",
      "findings": [
        "PINNs (Physics-Informed Neural Networks) can provide millisecond predictions",
        "Transfer learning PINNs reduce forward simulations needed",
        "Multi-fidelity PINNs combine cheap/expensive evaluations",
        "BUT: Requires offline training phase - complex implementation",
        "Would need to pre-train on similar problems, fine-tune at inference"
      ],
      "references": [
        "https://www.sciencedirect.com/science/article/abs/pii/S0735193323000519",
        "https://www.nature.com/articles/s41598-021-99037-x"
      ],
      "recommendation": "HIGH EFFORT - consider only if time permits"
    },
    {
      "topic": "Coarser grid exploration",
      "status": "proposed",
      "priority": "high",
      "note": "30x15 or 25x12 coarse grid might speed up exploration phase without losing accuracy at final evaluation",
      "hypothesis": "CMA-ES on ultra-coarse grid + fine-grid final evaluation"
    },
    {
      "topic": "Outlier-specific handling",
      "status": "proposed",
      "priority": "high",
      "note": "Sample 57 consistently appears as major outlier across all experiments. May need sample-specific initialization or more fevals."
    }
  ],

  "key_insights": {
    "timing_bottleneck": "Session 19 proved ~68 min runtime is from PRIMARY optimization, not fallbacks. Higher sigma adds ~10 min regardless of threshold settings.",
    "score_accuracy_tradeoff": "Higher sigma (0.20/0.25) achieves 1.1362 score but exceeds budget. Baseline sigma (0.15/0.20) stays in budget but caps at 1.1247.",
    "l_bfgs_b_incompatible": "L-BFGS-B without analytic gradients uses finite differences (2n evals per gradient) - incompatible with expensive simulators. Basin-hopping, hybrid two-stage all failed due to this.",
    "cmaes_essential": "CMA-ES covariance adaptation is ESSENTIAL. Simpler evolutionary algorithms (DE) and simplex methods (Nelder-Mead) cannot match CMA-ES accuracy.",
    "tested_failed": [
      "Basin-hopping: 1105 min (18x over budget) - L-BFGS-B gradient overhead",
      "Hybrid two-stage: 352 min (5.9x over budget) - same L-BFGS-B issue",
      "Differential evolution: 0.95 score - lacks covariance adaptation",
      "Nelder-Mead multistart: 240 min (4x over budget) - too many fevals",
      "Reduced fevals (18/34): 1.12 @ 63 min - worse score, still over budget"
    ],
    "remaining_opportunities": [
      "Ultra-coarse grid (30x15) - faster simulations may allow higher sigma within budget",
      "Outlier-specific handling - sample 57 consistently fails, needs special treatment",
      "Adaptive sigma schedule - high sigma early, low sigma late"
    ],
    "conclusion": "CMA-ES is the ONLY viable algorithm. Further improvement requires: (1) faster coarse grid, (2) outlier handling, or (3) sigma scheduling."
  },

  "execution_order": [
    "EXP_ULTRACOARSE_001 and EXP_OUTLIER_FOCUS_001 in parallel (priority 1)",
    "EXP_ADAPTIVE_SIGMA_001 if above don't work (priority 2)"
  ]
}
