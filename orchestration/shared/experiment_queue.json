{
  "version": "6.0",
  "last_updated": "2026-01-24T12:15:00Z",
  "notes": "V6.0: W0 emergency cycle - queue was CRITICAL (3). Added 4 experiments: sigma_termination, seed_ensemble, sample_timeout, intensity_bounds. Queue restored to 7. LRA-CMA, coord_sigma, BFGS all failed/aborted.",
  "queue_status": "HEALTHY",
  "experiment_queue": [
    {
      "id": "EXP_LQ_CMAES_001",
      "priority": 99,
      "family": "surrogate_lq",
      "status": "completed",
      "result": "FAILED - API mismatch. fmin_lq_surr returns ONE solution, we need MULTIPLE candidates. 29-71% worse RMSE, 7-17% slower.",
      "score": 0.253,
      "time_min": 64.1,
      "worker": "W1",
      "name": "lq_cma_es_builtin",
      "description": "Use pycma's built-in lq-CMA-ES (linear-quadratic surrogate model)",
      "finding": "fmin_lq_surr is designed for single-objective returning ONE best solution. Our scoring needs MULTIPLE diverse candidates. Fundamental mismatch."
    },
    {
      "id": "EXP_SEQUENTIAL_HANDOFF_001",
      "priority": 99,
      "family": "hybrid",
      "status": "completed",
      "result": "FAILED - Best in-budget 1.1132 is WORSE than baseline 1.1247. Best overall 1.1439 @ 126.5 min (2x over budget).",
      "score": 1.1132,
      "time_min": 56.6,
      "worker": "W2",
      "name": "cmaes_to_nm_sequential",
      "description": "CMA-ES exploration then hand off best to Nelder-Mead run",
      "finding": "Sequential handoff fundamentally doesn't work. NM improves score (+0.0192) when given time, but time overhead is prohibitive."
    },
    {
      "id": "EXP_BAYESIAN_OPT_001",
      "priority": 99,
      "family": "bayesian_opt",
      "status": "completed",
      "result": "FAILED - GP surrogate doesn't model thermal RMSE landscape. 31-91% worse accuracy, or 45% over budget if tuned.",
      "score": 0.31,
      "time_min": 57.6,
      "worker": "W1",
      "name": "bayesian_optimization_gp",
      "description": "Use Bayesian Optimization with Gaussian Process surrogate as alternative to CMA-ES",
      "finding": "Bayesian Optimization is NOT suitable for thermal inverse problem. GP surrogate fails to capture the complex RMSE landscape."
    },
    {
      "id": "EXP_MULTIFID_OPT_001",
      "priority": 99,
      "family": "multi_fidelity",
      "status": "completed",
      "result": "FAILED - Coarse grid RMSE landscape differs from fine grid. Best in-budget: RMSE 0.259 @ 45 min (45% worse than baseline).",
      "score": 0.259,
      "time_min": 44.9,
      "worker": "W1",
      "name": "multi_fidelity_proper_ratio",
      "actual_folder": "multi_fidelity_pyramid",
      "description": "Multi-fidelity optimization with literature-recommended grid ratios",
      "finding": "Multi-fidelity via grid coarsening doesn't work for inverse problems. RMSE landscape is fundamentally different at different resolutions."
    },
    {
      "id": "EXP_FAST_SOURCE_DETECT_001",
      "priority": 99,
      "family": "preprocessing",
      "status": "completed",
      "result": "ABORTED - Invalid premise. n_sources already in sample data.",
      "worker": "W2",
      "name": "fast_source_count_detection",
      "description": "Fast detection of 1-source vs 2-source BEFORE optimization",
      "finding": "Baseline already uses sample['n_sources']. Detection not needed."
    },
    {
      "id": "EXP_WS_CMAES_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - WS-CMA-ES causes divergence. Best: RMSE 0.276 @ 66 min (62% worse than baseline).",
      "score": 0.276,
      "time_min": 65.8,
      "worker": "W1",
      "name": "warm_start_cmaes",
      "description": "Use CyberAgentAILab's WS-CMA-ES to transfer solutions between similar samples",
      "finding": "WS-CMA-ES doesn't work for thermal inverse problems. Each sample is unique with no shared optimization landscape structure."
    },
    {
      "id": "EXP_TRANSFER_LEARN_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - Sensor feature clustering doesn't predict solution similarity",
      "score": 1.0804,
      "time_min": 59.7,
      "worker": "W2",
      "name": "sample_clustering_transfer",
      "actual_folder": "cluster_transfer",
      "description": "Cluster similar samples, transfer solutions between them",
      "finding": "Clustering by sensor features doesn't predict solution similarity. Transfer learning doesn't work for this problem."
    },
    {
      "id": "EXP_ADAPTIVE_BUDGET_001",
      "priority": 99,
      "family": "budget_allocation",
      "status": "completed",
      "result": "FAILED - Early termination hurts accuracy. Best in-budget: 1.1143 @ 56 min (1% worse than baseline).",
      "score": 1.1143,
      "time_min": 56.2,
      "worker": "W1",
      "name": "adaptive_sample_budget",
      "description": "Dynamically allocate fevals per sample based on convergence speed",
      "finding": "Early termination based on sigma/stagnation hurts accuracy. CMA-ES needs full budget."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity",
      "status": "completed",
      "result": "SUCCESS - NEW BEST! Score 1.1362 @ 39 min (+0.0115 vs baseline, 32% faster).",
      "score": 1.1362,
      "time_min": 39.0,
      "worker": "W2",
      "name": "early_timestep_filtering",
      "description": "Use reduced timesteps (40%) for candidate filtering, same spatial grid",
      "finding": "40% timesteps is optimal. Maintains 100x50 spatial grid, RMSE correlation 0.95+. Counterintuitive: more fevals HURT."
    },
    {
      "id": "EXP_TEMPORAL_HIGHER_SIGMA_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Higher sigma increases time disproportionately. Cannot beat W2 baseline within budget.",
      "score": 1.1745,
      "time_min": 63.0,
      "best_in_budget_score": 1.1584,
      "best_in_budget_time": 52.1,
      "worker": "W1",
      "name": "temporal_40pct_higher_sigma",
      "description": "Combine 40% temporal fidelity with higher sigma (0.25/0.30) for better accuracy",
      "finding": "Higher sigma (0.25/0.30) improves score by +0.0057 but adds 5+ min. Reducing polish to stay in budget loses the accuracy gain. W2's sigma 0.18/0.22 is optimal."
    },
    {
      "id": "EXP_NICHING_CMAES_001",
      "priority": 99,
      "family": "diversity",
      "status": "completed",
      "result": "FAILED - Scoring formula AVERAGES accuracy over candidates. Adding worse diverse candidates hurts score. Baseline already at 2.75/3 N_valid.",
      "score": 1.0622,
      "time_min": 46.9,
      "worker": "W2",
      "name": "niching_cmaes_diversity",
      "description": "Use Niching CMA-ES to maintain population diversity for multiple candidate solutions",
      "finding": "Diversity is NOT the bottleneck. Baseline already achieves 80% 3-candidate samples. Taboo-based niching hurts accuracy more than diversity helps."
    },
    {
      "id": "EXP_IPOP_TEMPORAL_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - IPOP adds time overhead (75.7 min) without improving accuracy. Splitting fevals across restarts reduces convergence.",
      "score": 1.1687,
      "time_min": 75.7,
      "worker": "W2",
      "name": "ipop_cmaes_temporal",
      "description": "IPOP-CMA-ES (increasing population) with 40% temporal fidelity",
      "finding": "IPOP divides feval budget across restarts, each restart gets insufficient fevals. Problem doesn't have local optima - restarts don't help."
    },
    {
      "id": "EXP_PHYSICS_INIT_001",
      "priority": 99,
      "family": "initialization",
      "status": "completed",
      "result": "FAILED - Gradient init is WORSE than smart init: -0.0046 score, +2.3 min",
      "score": 1.1639,
      "time_min": 69.6,
      "worker": "W1",
      "name": "physics_informed_init",
      "description": "Use sensor temperature gradients to initialize source location estimates",
      "finding": "Temperature gradients at sensors don't accurately point to sources. Heat diffusion corrupts gradient signal. Simple hottest-sensor init is already optimal."
    },
    {
      "id": "EXP_ADAPTIVE_TIMESTEP_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Adaptive timestep switching during CMA-ES is counterproductive. Best: 1.1635 @ 69.9 min (both worse than baseline).",
      "score": 1.1635,
      "time_min": 69.9,
      "worker": "W2",
      "name": "adaptive_timestep_fraction",
      "description": "Start with 25% timesteps, increase to 40% as CMA-ES converges",
      "finding": "CMA-ES covariance adaptation requires consistent fitness landscape. Switching fidelity mid-run disrupts learning and leads to worse performance."
    },
    {
      "id": "EXP_POD_SURROGATE_001",
      "priority": 99,
      "family": "surrogate_pod",
      "status": "completed",
      "result": "ABORTED - POD not viable for this problem due to sample-specific physics (kappa varies). Can't pre-build universal basis.",
      "worker": "W2",
      "name": "pod_reduced_order_model",
      "description": "Use POD (Proper Orthogonal Decomposition) as fast surrogate for simulation",
      "finding": "Each sample has unique physics (kappa, bc, T0). POD requires pre-computed snapshots from SAME system. Online POD would need simulations, defeating purpose. Temporal fidelity already provides similar speedup."
    },
    {
      "id": "EXP_EXTENDED_POLISH_001",
      "priority": 99,
      "family": "refinement",
      "status": "completed",
      "result": "FAILED - 12 iterations takes 82.3 min (37% over budget) for only +0.0015 score. 8 iterations is optimal.",
      "score": 1.1703,
      "time_min": 82.3,
      "worker": "W2",
      "name": "extended_nm_polish",
      "description": "Increase NM polish iterations from 8 to 12-16 for better accuracy",
      "finding": "More polish iterations exceeds time budget. Each additional iteration adds ~6 min on 80 samples. 8 is already optimal."
    },
    {
      "id": "EXP_LARGER_POPSIZE_001",
      "priority": 99,
      "family": "cmaes_accuracy",
      "status": "completed",
      "result": "FAILED - Popsize=12 adds 14 min without improving accuracy. Default popsize is optimal.",
      "score": 1.1666,
      "time_min": 73.0,
      "worker": "W1",
      "name": "larger_cmaes_population",
      "description": "Use larger CMA-ES population size for better accuracy in inverse problem",
      "finding": "Larger popsize reduces generations with fixed feval budget. CMA-ES needs multiple generations to adapt covariance. Default popsize (4+3*ln(n)) is already optimal."
    },
    {
      "id": "EXP_2SOURCE_FOCUS_001",
      "priority": 99,
      "family": "source_specific",
      "status": "completed",
      "result": "FAILED - Specialized feval allocation hurts performance. Reducing 1-src hurts accuracy, increasing 2-src doesn't help.",
      "score": 1.162,
      "time_min": 69.3,
      "worker": "W2",
      "name": "two_source_specialized",
      "description": "Specialized optimization for 2-source samples (60% of data, higher RMSE)",
      "finding": "Baseline 20/36 feval split is already optimal. 2-source is harder due to 4D search space, not under-optimization. More fevals don't help."
    },
    {
      "id": "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
      "priority": 99,
      "family": "sigma_scheduling",
      "status": "completed",
      "result": "ABORTED - Prior evidence shows this approach will fail. CMA-ES already adapts sigma naturally.",
      "worker": "W1",
      "name": "adaptive_sigma_schedule",
      "description": "Start with higher sigma (0.25/0.30), decay to lower sigma (0.18/0.22) during optimization",
      "finding": "EXP_TEMPORAL_HIGHER_SIGMA_001 showed high sigma adds time without in-budget accuracy gain. EXP_ADAPTIVE_TIMESTEP_001 showed changing conditions mid-run disrupts CMA-ES. No point testing a known-to-fail approach."
    },
    {
      "id": "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "ABORTED - Prior experiment showed truncated polish HURTS accuracy. 40% polish gave -0.0346 score. 60% would similarly hurt.",
      "worker": "W2",
      "name": "progressive_polish_fidelity",
      "description": "Use progressive timestep fidelity during NM polish: 60% early, 100% final",
      "finding": "NM polish requires full timesteps for accurate refinement. Truncated polish 'overfits to proxy noise'. Full timestep polish is optimal."
    },
    {
      "id": "EXP_ACTIVE_CMAES_001",
      "priority": 99,
      "family": "cmaes_variants",
      "status": "completed",
      "result": "ABORTED - Wrong premise. CMA_active already defaults to True in pycma. Baseline already uses active CMA-ES.",
      "worker": "W2",
      "name": "active_cmaes_covariance",
      "description": "Use Active CMA-ES variant with enhanced negative covariance update",
      "finding": "pycma's CMA_active option defaults to True. The baseline already uses active covariance update. No experiment needed."
    },
    {
      "id": "EXP_PARAMETER_SCALING_001",
      "priority": 99,
      "family": "problem_specific",
      "status": "completed",
      "result": "ABORTED - Wrong premise. CMA-ES only optimizes positions (x,y). Intensity (q) is computed analytically via least squares. No scaling needed.",
      "worker": "W1",
      "name": "weighted_parameter_scaling",
      "description": "Different scaling/weights for position (x,y) vs intensity (q) parameters",
      "finding": "Optimizer already separates position (CMA-ES) from intensity (analytical). CMA-ES only sees (x,y) or (x1,y1,x2,y2), not q."
    },
    {
      "id": "EXP_BOUNDARY_AWARE_INIT_001",
      "priority": 99,
      "family": "initialization_v2",
      "status": "completed",
      "result": "ABORTED - Data analysis shows 24% of samples have boundary hotspots. Biasing away would hurt these cases.",
      "worker": "W1",
      "name": "boundary_aware_initialization",
      "description": "Initialize heat sources away from domain boundaries where physics differs",
      "finding": "24% of samples have hottest sensor near boundary (<10% margin). Abort criteria met: biasing away hurts boundary source cases. initialization_v2 family should be marked EXHAUSTED."
    },
    {
      "id": "EXP_COARSE_SURROGATE_001",
      "priority": 99,
      "family": "surrogate",
      "status": "deprioritized",
      "name": "coarse_grid_as_surrogate",
      "description": "Use 40x20 coarse grid as cheap surrogate for candidate filtering",
      "note": "DEPRIORITIZED - Multi-fidelity via spatial coarsening PROVEN to fail (EXP_MULTIFID_OPT_001). Different resolution = different landscape."
    },
    {
      "id": "EXP_ENSEMBLE_001",
      "priority": 99,
      "family": "ensemble",
      "status": "completed",
      "result": "FAILED - 347.9 min (5.8x over budget), score 1.0972 (WORSE than baseline 1.1247)",
      "score": 1.0972,
      "time_min": 347.9,
      "worker": "W1",
      "name": "optimizer_ensemble_voting",
      "description": "Run multiple optimizers in parallel, take best result per sample",
      "finding": "Nelder-Mead won 74% but running both optimizers doubles simulation count. Key: REDUCE simulations, not add optimizers."
    },
    {
      "id": "EXP_SURROGATE_NN_001",
      "priority": 99,
      "family": "surrogate",
      "status": "completed",
      "result": "FAILED - Online learning doesn't work with parallel processing",
      "score": 1.1203,
      "time_min": 75.6,
      "worker": "W1",
      "name": "neural_network_surrogate_prefilter"
    },
    {
      "id": "EXP_COBYLA_REFINE_001",
      "priority": 99,
      "family": "gradient_free_local",
      "status": "completed",
      "result": "FAILED - 88.8 min over budget, COBYLA uses 20+ extra evals vs NM's 3-6",
      "score": 1.1235,
      "time_min": 88.8,
      "worker": "W2",
      "name": "cobyla_trust_region_refinement"
    },
    {
      "id": "EXP_FAST_ICA_001",
      "priority": 99,
      "family": "decomposition",
      "status": "completed",
      "result": "FAILED - 151.1 min projected (91 min over budget), coarse-to-fine adds overhead",
      "score": 1.1046,
      "time_min": 151.1,
      "worker": "W2",
      "name": "accelerated_ica_decomposition",
      "finding": "Coarse-to-fine adds massive overhead. ICA itself is fast but extra refinement stage doubles simulation count."
    },
    {
      "id": "EXP_CONJUGATE_GRADIENT_001",
      "priority": 99,
      "family": "adjoint_gradient",
      "status": "completed",
      "result": "FAILED - Adjoint gradient is 5-6 orders of magnitude too small. Manual derivation errors cause L-BFGS-B to see near-zero gradient and not iterate.",
      "score": 0.9006,
      "time_min": 56.5,
      "worker": "W2",
      "name": "conjugate_gradient_adjoint",
      "description": "Conjugate Gradient Method with adjoint equation for O(1) gradient computation",
      "finding": "Gradient verification: adjoint=[-0.000017, 0.000004], finite_diff=[-9.079, 1.980]. Ratio ~0.000002. Manual adjoint derivation for ADI time-stepping is error-prone. Use JAX autodiff instead."
    },
    {
      "id": "EXP_JAX_AUTODIFF_001",
      "priority": 99,
      "family": "differentiable_simulation",
      "status": "completed",
      "result": "ABORTED - Fundamental incompatibility. Explicit Euler stability constraint requires 4x more timesteps than implicit ADI.",
      "worker": "W1",
      "name": "jax_differentiable_solver",
      "description": "Rewrite thermal simulator in JAX for automatic differentiation",
      "finding": "JAX autodiff requires explicit time-stepping. Stability constraint dt < 0.002061 vs original dt=0.004 means 4x more timesteps needed. Running fewer timesteps gives wrong physics (5.7x temperature error). Implicit ADI is essential for efficiency."
    },
    {
      "id": "EXP_HYBRID_CMAES_LBFGSB_001",
      "priority": 99,
      "family": "hybrid_gradient",
      "status": "completed",
      "result": "FAILED - L-BFGS-B polish NOT better than NM polish. Finite diff overhead (O(n) extra sims/iter) outweighs gradient advantage. Best in-budget L-BFGS-B: 1.1174 @ 42 min (WORSE than NM x8: 1.1415 @ 38 min).",
      "score": 1.1174,
      "time_min": 42.4,
      "worker": "W1",
      "name": "cmaes_then_gradient_refinement",
      "description": "Use CMA-ES for global search, then L-BFGS-B with finite differences for final polish",
      "finding": "L-BFGS-B finite diff gradient requires O(n) extra sims per iteration. For 4D (2-src): ~250-350 sims vs NM's ~180. NM x8 is optimal polish method. hybrid_gradient family FAILED."
    },
    {
      "id": "EXP_OPENAI_ES_001",
      "priority": 99,
      "family": "alternative_es",
      "status": "completed",
      "result": "FAILED - OpenAI ES cannot beat CMA-ES. Diagonal covariance loses correlation information. Best: 1.1204 @ 43.3 min (-0.0158 score, +4.3 min vs baseline).",
      "score": 1.1204,
      "time_min": 43.3,
      "worker": "W1",
      "name": "openai_evolution_strategy",
      "description": "Try OpenAI's Evolution Strategy as alternative to CMA-ES",
      "finding": "OpenAI ES's diagonal covariance approximation loses critical parameter correlations that CMA-ES captures. For low-dim expensive black-box optimization, CMA-ES is optimal. alternative_es family FAILED."
    },
    {
      "id": "EXP_PRETRAINED_SURROGATE_001",
      "priority": 99,
      "family": "neural_operator",
      "status": "completed",
      "name": "pretrained_nn_surrogate",
      "description": "Train neural network on many simulations OFFLINE, then use as fast surrogate",
      "hypothesis": "Previous NN surrogate failed due to online learning + parallel issues. Pre-training OFFLINE on 10K+ simulations creates a fixed surrogate that works with parallel processing.",
      "research_source": "FNO inverse problems paper (AISTATS 2025). Neural operators can learn PDE solution operators with high accuracy.",
      "why_different": "Previous approach: online learning during optimization (failed). New approach: offline pre-training, fixed surrogate during optimization.",
      "implementation": [
        "1. Generate training data: 10K random source configs -> RMSE values",
        "2. Train small MLP: (x, y, [x2, y2]) -> predicted_RMSE",
        "3. Use trained surrogate for candidate filtering in CMA-ES",
        "4. Only full-simulate candidates with low surrogate RMSE",
        "5. Compare total sims needed vs baseline"
      ],
      "success_criteria": "Reduce simulations by 50%+ while maintaining score >= 1.16",
      "abort_criteria": "Surrogate accuracy too poor or training data generation too expensive",
      "result": "ABORTED - RMSE landscape is sample-specific (avg correlation -0.167). Universal surrogate cannot predict RMSE without sample information.",
      "worker": "W2",
      "finding": "Tested RMSE landscape correlation across 5 samples. Average Spearman r = -0.167 (anti-correlated). Each sample has unique physics (kappa, bc, Y_obs). Surrogate needs sample info, defeating purpose."
    },
    {
      "id": "EXP_PINN_DIRECT_001",
      "priority": 99,
      "family": "neural_operator",
      "status": "completed",
      "name": "pinn_inverse_heat_source",
      "description": "Use Physics-Informed Neural Network to directly solve the inverse heat source problem",
      "hypothesis": "PINNs can directly optimize for heat source parameters by encoding physics constraints in the loss. Recent 2025 paper specifically addresses heat source field inversion.",
      "research_source": "Heat source field inversion with PINN (ScienceDirect 2025). ASME Journal review on PINNs for heat transfer.",
      "why_different": "Direct approach: NN takes sensor readings, outputs source parameters. No iterative optimization loop.",
      "implementation": [
        "1. Define PINN architecture: input=sensor_temps, output=source_params",
        "2. Loss = data_mismatch + physics_residual (heat equation)",
        "3. Train on sensor observations to find source parameters",
        "4. Use PyTorch or TensorFlow with autodiff",
        "5. Compare accuracy and inference speed"
      ],
      "success_criteria": "Score >= 1.15 AND inference time < 1 sec per sample",
      "abort_criteria": "PINN requires too much training time per sample",
      "result": "ABORTED - PINN requires efficient gradients. Adjoint failed (1e-6 error), JAX failed (stability), finite diff too slow (99 min vs 60 budget).",
      "worker": "W2",
      "finding": "L-BFGS-B achieves 15% better RMSE than NM but takes 2.3x longer due to finite diff. ADI solver blocks autodiff. Gradient-free methods (CMA-ES + NM) remain optimal."
    },
    {
      "id": "EXP_LEVENBERG_MARQUARDT_001",
      "priority": 99,
      "family": "nonlinear_least_squares",
      "status": "completed",
      "result": "FAILED - LM is local optimizer that gets stuck in local minima. Finite diff Jacobian needs 3-5 sims/iter. Best in-budget: 1.0350 @ 56 min (-9% vs baseline).",
      "score": 1.035,
      "time_min": 56.0,
      "worker": "W1",
      "name": "levenberg_marquardt_inverse",
      "description": "Use Levenberg-Marquardt algorithm via scipy.optimize.least_squares for inverse heat problem",
      "finding": "LM fundamentally unsuitable: (1) local optimizer vs multi-modal landscape, (2) expensive finite diff Jacobian (3-5 sims/iter), (3) CMA-ES is 3x more sample-efficient. nonlinear_least_squares family should be marked FAILED."
    },
    {
      "id": "EXP_TRUST_REGION_REFLECTIVE_001",
      "priority": 99,
      "family": "nonlinear_least_squares",
      "status": "deprioritized",
      "name": "trust_region_reflective_bounded",
      "description": "Use Trust-Region Reflective method for bounded inverse heat problem",
      "deprioritize_reason": "DEPRIORITIZED based on EXP_LEVENBERG_MARQUARDT_001 results. TRF is also a local optimizer like LM - will have same local minima issues on multi-modal RMSE landscape. Same expensive finite-diff Jacobian problem. Not worth testing.",
      "original_hypothesis": "TRF handles bounded constraints naturally (source positions have box constraints). Our x,y are bounded to domain [0,Lx]x[0,Ly]. TRF respects bounds without penalty methods, potentially more accurate."
    },
    {
      "id": "EXP_SEPARABLE_VP_001",
      "priority": 99,
      "family": "variable_projection",
      "status": "completed",
      "result": "FAILED - VP's Gauss-Newton is local optimizer that gets stuck. Baseline already uses VP implicitly (analytical q). Best: 1.0025 @ 54.3 min (-0.1337 score).",
      "score": 1.0025,
      "time_min": 54.3,
      "worker": "W1",
      "name": "variable_projection_separable",
      "description": "Exploit separable structure via Variable Projection: optimize positions, solve intensity analytically",
      "finding": "Baseline already uses VP (analytical q for each CMA-ES candidate). Gauss-Newton on positions gets stuck in local minima. CMA-ES + analytical q is optimal for exploiting separability."
    },
    {
      "id": "EXP_ADAPTIVE_SA_001",
      "priority": 99,
      "family": "simulated_annealing",
      "status": "completed",
      "name": "adaptive_simulated_annealing",
      "description": "Use Adaptive Simulated Annealing (ASA) for heat source inverse problem",
      "hypothesis": "2024 Nature paper shows ASA successfully reconstructs internal heat sources in biological tissue. ASA adaptively adjusts temperature schedule based on search state, improving global optimization for complex inverse problems.",
      "research_source": "Noninvasive reconstruction of internal heat source in biological tissue using ASA (Nature Scientific Reports, July 2024). ASA avoids complex regularization by treating inverse problem as optimization.",
      "why_different": "SA is a global optimizer like CMA-ES but with different exploration mechanism (probabilistic uphill moves). ASA adapts cooling schedule dynamically. May find different optima than CMA-ES. Very different from local methods (LM) that failed.",
      "implementation": [
        "1. Use scipy.optimize.dual_annealing (generalized SA with local search)",
        "2. Set bounds for source positions",
        "3. Use same objective function as baseline (RMSE after q optimization)",
        "4. Compare global search efficiency vs CMA-ES",
        "5. If promising, implement full ASA with adaptive cooling"
      ],
      "success_criteria": "Score >= 1.16 AND time <= 55 min",
      "abort_criteria": "SA converges slower than CMA-ES OR gets stuck in same local minima",
      "result": "FAILED - dual_annealing is fundamentally unsuitable. With local search: 2-5x over budget. Without: 23% worse accuracy.",
      "score": 0.8666,
      "time_min": 28.2,
      "worker": "W2",
      "finding": "SA local search uses L-BFGS-B finite differences = slow. Pure SA = fast but inaccurate. CMA-ES covariance adaptation is more sample-efficient."
    },
    {
      "id": "EXP_SURROGATE_CMAES_001",
      "priority": 99,
      "family": "surrogate_assisted",
      "status": "completed",
      "name": "surrogate_assisted_cmaes",
      "description": "Use kriging/GP surrogate to pre-filter CMA-ES candidates, reducing simulation count",
      "hypothesis": "SCR algorithm (2025) shows surrogate-assisted CMA-ES works well for expensive black-box functions. Build kriging surrogate from initial samples, use it to filter unpromising candidates before full simulation.",
      "research_source": "SCR: Surrogate-CMAES-RQLIF for expensive black-box optimization (Optimization and Engineering, March 2025). Automotive crashworthiness optimization with surrogate-based hyperparameter tuning.",
      "why_different": "Previous surrogate (NN online) failed due to parallel processing. This approach: (1) builds surrogate from existing samples, (2) uses kriging not NN, (3) only filters obviously bad candidates. More conservative than full surrogate replacement.",
      "implementation": [
        "1. Run baseline CMA-ES on first 10 samples, collect (params, RMSE) pairs",
        "2. Fit kriging/GP model to collected data",
        "3. For remaining samples: use surrogate to filter CMA-ES population",
        "4. Only simulate candidates with low predicted RMSE (top 50%)",
        "5. Update surrogate periodically with new data"
      ],
      "success_criteria": "Reduce simulations by 30%+ while maintaining score >= 1.15",
      "abort_criteria": "Surrogate filtering removes good candidates OR overhead exceeds savings",
      "result": "ABORTED - Prior finding: RMSE landscapes are sample-specific (avg correlation -0.167). Kriging surrogate from samples 1-10 will NOT generalize to samples 11-80.",
      "worker": "W2",
      "finding": "Based on EXP_PRETRAINED_SURROGATE_001 results: RMSE = f(x, y, Y_obs, kappa, bc) is sample-specific. Surrogate needs sample info to work, defeating the purpose."
    },
    {
      "id": "EXP_DIFFERENTIAL_EVOLUTION_001",
      "priority": 99,
      "family": "differential_evolution",
      "status": "completed",
      "result": "FAILED - DE cannot match CMA-ES accuracy. Best: 1.1325 @ 35.2 min (-0.0037 score, -3.8 min vs baseline). CMA-ES covariance adaptation is essential.",
      "score": 1.1325,
      "time_min": 35.2,
      "worker": "W1",
      "name": "differential_evolution_inverse",
      "description": "Use scipy.optimize.differential_evolution for inverse heat problem",
      "finding": "DE's mutation/crossover is less efficient than CMA-ES's covariance adaptation. DE is 10% faster but sacrifices accuracy. differential_evolution family FAILED.",
      "success_criteria": "Score >= 1.16 AND time <= 55 min",
      "abort_criteria": "DE converges slower than CMA-ES OR same local minima issue"
    },
    {
      "id": "EXP_WEIGHTED_LOSS_001",
      "priority": 99,
      "family": "loss_function",
      "status": "completed",
      "result": "FAILED - Optimizing weighted RMSE finds different optimum than unweighted (scored). Score 1.0131 (-0.1557). Diversity destroyed.",
      "score": 1.0131,
      "time_min": 90.7,
      "worker": "W1",
      "name": "weighted_sensor_loss",
      "description": "Weight sensor contributions by their informativeness or reliability",
      "finding": "CRITICAL: Do NOT change loss function - optimize exactly what you are scored on. loss_function family FAILED."
    },
    {
      "id": "EXP_POLAR_PARAM_001",
      "priority": 99,
      "family": "problem_reformulation",
      "status": "completed",
      "result": "FAILED - Rectangular domain incompatible with polar coords. Score 1.1472 @ 72.7 min vs baseline.",
      "score": 1.1472,
      "time_min": 72.7,
      "worker": "W2",
      "name": "polar_parameterization",
      "description": "Reparameterize source positions using polar coordinates centered on domain centroid",
      "finding": "Cartesian is optimal for rectangular domains. problem_reformulation family FAILED."
    },
    {
      "id": "EXP_EARLY_REJECTION_001",
      "priority": 99,
      "family": "efficiency",
      "status": "completed",
      "result": "FAILED - Two-stage evaluation ADDS overhead (191 sims vs 100 baseline). Only 8.6% rejection rate - need >25% to break even. Efficiency family EXHAUSTED.",
      "score": 1.1598,
      "time_min": 146.7,
      "worker": "W1",
      "name": "early_rejection_partial_sim",
      "description": "Reject clearly bad candidates early using partial simulation (first 10% timesteps)",
      "finding": "Early rejection fundamentally flawed: filter cost not offset by savings. CMA-ES candidates cluster near optima, most pass filter. Baseline 40% temporal is optimal."
    },
    {
      "id": "EXP_ADAPTIVE_NM_POLISH_001",
      "priority": 4,
      "family": "refinement",
      "status": "completed",
      "name": "adaptive_nm_iterations",
      "description": "Dynamically adjust NM polish iterations based on convergence rate",
      "hypothesis": "Some samples converge quickly (4-5 NM iters), others need more (10+). Fixed 8 iters is wasteful for easy samples, insufficient for hard ones.",
      "research_source": "Adaptive algorithms adjust parameters based on progress. Early stopping when converged saves compute.",
      "why_different": "Current baseline uses fixed 8 NM iterations. Adaptive could be more efficient.",
      "implementation": [
        "1. Start NM polish with initial iterations (e.g., 4)",
        "2. Check convergence: delta_rmse < threshold?",
        "3. If not converged and iterations < max (e.g., 12), continue",
        "4. If converged early, stop and save time",
        "5. Track time savings and accuracy"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (save time on easy samples)",
      "abort_criteria": "Adaptive logic overhead exceeds savings OR accuracy drops",
      "worker": "W2",
      "result": "FAILED - Adaptive NM iterations does NOT improve. Best: 1.1607 @ 78.3 min (-0.0081 score, +19.9 min vs baseline).",
      "score": 1.1607,
      "time_min": 78.3,
      "finding": "Fixed 8 NM iterations is already optimal. Adaptive batching adds overhead. Source-count based (6/10) is worse. refinement family EXHAUSTED."
    },
    {
      "id": "EXP_CMAES_RESTART_BEST_001",
      "priority": 99,
      "family": "cmaes_improvement",
      "status": "completed",
      "result": "FAILED - Two-phase restart doubles sims (175.7 min), causes diversity loss (1.7 vs 3 candidates), no accuracy improvement. Score 1.0986 vs baseline 1.1688.",
      "score": 1.0986,
      "time_min": 175.7,
      "worker": "W1",
      "name": "cmaes_restart_from_best",
      "description": "Run short CMA-ES, restart from best solution found with tighter sigma",
      "finding": "CMA-ES converges well in single phase. Restart wastes budget re-converging to same solutions. Small sigma in phase 2 destroys diversity."
    },
    {
      "id": "EXP_GRADIENT_INIT_001",
      "priority": 99,
      "family": "initialization",
      "status": "invalid",
      "name": "gradient_informed_init",
      "description": "Use temperature gradient direction to inform initial source position guess",
      "hypothesis": "Heat sources create temperature gradients pointing away from them. Tracing gradient backward could give better init than just hottest sensor.",
      "research_source": "Physics-based initialization. Heat equation has predictable gradient structure.",
      "why_different": "Current smart init uses hottest sensor position. Gradient-informed could be more accurate.",
      "implementation": [
        "1. Compute temperature gradient at each sensor location",
        "2. Trace gradient backward (uphill in temp) from hot sensors",
        "3. Estimate source position as intersection point or max temp region",
        "4. Use as initial guess for CMA-ES",
        "5. Compare with hottest-sensor init"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Gradient-based init is not consistently better than hottest-sensor",
      "invalidation_reason": "DUPLICATE of EXP_PHYSICS_INIT_001 which FAILED. That experiment tested 'sensor temperature gradients to initialize source location estimates' and found gradient init WORSE than hottest-sensor (-0.0046 score, +2.3 min). This experiment proposes essentially the same thing. initialization family already marked EXHAUSTED."
    },
    {
      "id": "EXP_MULTISTART_ELITE_001",
      "priority": 99,
      "family": "multi_start",
      "status": "completed",
      "name": "multistart_elite_selection",
      "description": "Run multiple short CMA-ES from different inits, continue only the best 1-2",
      "hypothesis": "Instead of running N full optimizations, run N short ones and continue only the most promising. More efficient budget allocation.",
      "research_source": "Elite selection in multi-start optimization. Racing algorithms.",
      "why_different": "Current approach runs all inits to completion. Elite selection may be more efficient.",
      "implementation": [
        "1. Start 4-5 CMA-ES runs from different smart inits",
        "2. After 5 generations, evaluate all",
        "3. Continue only top 1-2 performers",
        "4. Run to full convergence",
        "5. Compare with baseline multi-start"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (better budget allocation)",
      "abort_criteria": "Elite selection kills good runs OR overhead exceeds savings",
      "worker": "W2",
      "result": "FAILED - Multi-start overhead dominates. Early fitness not predictive. Score 1.1529 @ 67 min vs baseline 1.1688 @ 58.4 min.",
      "score": 1.1529,
      "time_min": 67.0,
      "finding": "Single-start with good initialization is already optimal. multi_start family FAILED."
    },
    {
      "id": "EXP_LOG_RMSE_LOSS_001",
      "priority": 99,
      "family": "loss_reformulation",
      "status": "completed",
      "result": "FAILED - Monotonic transformation adds overhead without benefit. Score 1.1677 @ 70.5 min vs baseline.",
      "score": 1.1677,
      "time_min": 70.5,
      "worker": "W2",
      "name": "log_rmse_loss",
      "description": "Use log(1+RMSE) instead of RMSE as optimization objective",
      "finding": "loss_reformulation family FAILED. RMSE is optimal objective."
    },
    {
      "id": "EXP_SOLUTION_INJECTION_001",
      "priority": 99,
      "family": "population_enhancement",
      "status": "completed",
      "result": "FAILED - Sequential execution kills parallelism. Score 0.9929 @ 122.5 min vs baseline 1.1688 @ 58.4 min.",
      "score": 0.9929,
      "time_min": 122.5,
      "worker": "W1",
      "name": "solution_injection_cmaes",
      "description": "Inject best solutions from early inits into later CMA-ES populations (GloMPO-inspired)",
      "finding": "Information sharing between inits not worth losing parallelism. population_enhancement family FAILED."
    },
    {
      "id": "EXP_TIKHONOV_REG_001",
      "priority": 99,
      "family": "regularization",
      "status": "completed",
      "worker": "W2",
      "name": "tikhonov_regularized_loss",
      "description": "Add Tikhonov regularization penalty to RMSE for smoother solutions",
      "result": "ABORTED - Prior evidence: EXP_WEIGHTED_LOSS_001 showed changing loss function is catastrophic (score 1.0131 vs 1.1688). Any modification to objective finds DIFFERENT optimum than what we're scored on. Tikhonov would have same issue.",
      "finding": "regularization family EXHAUSTED. Optimizing modified objective finds different optimum."
    },
    {
      "id": "EXP_ADAPTIVE_POPSIZE_001",
      "priority": 99,
      "family": "cmaes_enhancement",
      "status": "completed",
      "worker": "W1",
      "name": "adaptive_population_size",
      "description": "Start with larger population for exploration, reduce for exploitation",
      "result": "FAILED - Two-phase CMA-ES massively over budget (124.6 min, 2.1x). Score 1.0026 vs baseline 1.1688. Phases break covariance adaptation.",
      "score": 1.0026,
      "time_min": 124.6,
      "finding": "Two-phase CMA-ES creates 8 CMA-ES runs per 2-src sample (4 inits x 2 phases). CMA-ES covariance adaptation requires continuous updates - phases discard learning. cmaes_enhancement family EXHAUSTED."
    },
    {
      "id": "EXP_COORD_SIGMA_001",
      "priority": 99,
      "family": "cmaes_tuning",
      "status": "completed",
      "worker": "W1",
      "name": "coordinate_wise_sigma",
      "result": "ABORTED - Flawed premise. CMA-ES only optimizes positions (not intensity). dd-CMA-ES already tested coordinate scaling and failed (2:1 domain ratio too mild).",
      "finding": "Intensity is computed analytically, not by CMA-ES. Prior EXP_DD_CMAES_001 showed coordinate-wise scaling doesn't help."
    },
    {
      "id": "EXP_EARLY_STOP_CMA_001",
      "priority": 99,
      "family": "efficiency",
      "status": "completed",
      "result": "FAILED - Early stopping NEVER triggered (0 saved fevals). CMA-ES continues improving >1% throughout its budget. 5.4x over budget.",
      "score": 1.1378,
      "time_min": 313.3,
      "worker": "W2",
      "name": "cmaes_early_stopping",
      "description": "Stop CMA-ES early if best solution hasn't improved for K generations",
      "finding": "The hypothesis 'many samples converge quickly' is NOT validated. CMA-ES makes >1% improvements throughout. efficiency family EXHAUSTED."
    },
    {
      "id": "EXP_NOISE_INJECTION_001",
      "priority": 99,
      "family": "exploration",
      "status": "completed",
      "worker": "W2",
      "name": "noise_injection_escape",
      "description": "Inject small noise into CMA-ES solutions when convergence stagnates",
      "result": "ABORTED - Prior evidence: EXP_EARLY_STOP_CMA_001 showed CMA-ES continues improving >1% throughout - it does NOT stagnate. If sigma doesn't drop (no stagnation), noise injection would never trigger. No benefit possible.",
      "finding": "exploration/escape family EXHAUSTED. CMA-ES doesn't get stuck in local optima for this problem."
    },
    {
      "id": "EXP_POWELL_POLISH_001",
      "priority": 99,
      "family": "local_search",
      "status": "completed",
      "worker": "W1",
      "name": "powell_polish_instead_nm",
      "description": "Use Powell's method (coordinate-wise search) instead of Nelder-Mead for final polish",
      "result": "FAILED - Powell uses 5-17x more function evaluations (1000-3500 sims for 2-src). 4.2x over budget (244.9 min), score WORSE (-0.0275).",
      "score": 1.1413,
      "time_min": 244.9,
      "finding": "Powell's coordinate-wise line searches require O(n) 1D optimizations per iteration. For 4D: 40-120 evals/iter vs NM's ~5. Nelder-Mead is optimal for 2-4D polish. local_search family should use NM."
    },
    {
      "id": "EXP_RANDOM_TIMESTEPS_001",
      "priority": 4,
      "family": "temporal_sampling",
      "status": "completed",
      "worker": "W2",
      "name": "random_timestep_selection",
      "description": "Use random 40% of timesteps instead of first 40% for CMA-ES evaluation",
      "hypothesis": "First 40% timesteps may miss important late-time dynamics. Random sampling captures different parts of temporal evolution, potentially providing more informative RMSE signal.",
      "research_source": "Stochastic sampling in reduced-order models. Random subsets can capture global behavior better than contiguous subsets.",
      "why_different": "Baseline uses first 40% timesteps (early dynamics only). Random samples entire time window.",
      "implementation": [
        "1. Pre-select random 40% of timestep indices (once per sample)",
        "2. Use same indices for all CMA-ES candidates in that sample",
        "3. Compare RMSE correlation with full-timestep RMSE",
        "4. Compare final optimization quality"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Random timesteps give worse RMSE correlation than contiguous (current method)",
      "result": "FAILED - Random timesteps requires full simulation (3-4x slower). Projected 350 min (5.8x over budget). No accuracy benefit.",
      "score": null,
      "time_min": 350,
      "finding": "Efficiency comes from running FEWER timesteps, not DIFFERENT timesteps. Early timesteps are most informative. temporal_sampling family FAILED."
    },
    {
      "id": "EXP_SEP_CMAES_001",
      "priority": 2,
      "family": "cmaes_variants",
      "status": "completed",
      "name": "separable_cmaes_diagonal",
      "description": "Use Separable CMA-ES (diagonal-only covariance) for faster convergence",
      "hypothesis": "Separable CMA-ES uses diagonal covariance matrix only, enabling higher learning rates. For low-dim problems (2-4D), the correlation information loss may be acceptable while gaining speed. Research shows sep-CMA-ES converges faster on separable/nearly-separable functions.",
      "research_source": "dd-CMA paper (Akimoto & Hansen, Evolutionary Computation 2020). pycma CMA_diagonal option.",
      "why_different": "Baseline uses full covariance CMA-ES. Sep-CMA-ES trades correlation modeling for faster adaptation. Our problem may be sufficiently separable (x, y positions somewhat independent).",
      "implementation": [
        "1. Set CMA option: CMA_diagonal=True (always diagonal)",
        "2. This enables higher learning rate for covariance update",
        "3. Compare convergence speed and final accuracy",
        "4. Check if position correlations matter for this problem"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (faster with comparable accuracy)",
      "abort_criteria": "Diagonal covariance loses critical correlations, accuracy drops significantly",
      "worker": "W2",
      "result": "FAILED - Diagonal covariance hurts both accuracy (-0.0187) and time (5.3x over budget). Full covariance is essential.",
      "score": 1.1501,
      "time_min": 312.0,
      "finding": "Position correlations along heat gradient are essential. Diagonal covariance causes worse convergence AND more iterations."
    },
    {
      "id": "EXP_DD_CMAES_001",
      "priority": 99,
      "family": "cmaes_variants",
      "status": "completed",
      "worker": "W1",
      "name": "diagonal_decoding_cmaes",
      "description": "Use dd-CMA-ES (Diagonal Acceleration) for coordinate-wise scaling",
      "result": "FAILED - dd-CMA-ES is neutral (dd=1.0 vs dd=0 gives same score). 2:1 domain ratio too mild for diagonal decoding.",
      "score": 1.1466,
      "time_min": 50.3,
      "finding": "dd-CMA-ES provides no improvement. Best: 1.1466 @ 50.3 min vs baseline 1.1688 @ 58.4 min. Low-dim problems (2D/4D) don't benefit."
    },
    {
      "id": "EXP_BFGS_POLISH_001",
      "priority": 99,
      "family": "local_search",
      "status": "completed",
      "worker": "W1",
      "name": "bfgs_polish_after_cmaes",
      "result": "ABORTED - Prior evidence. L-BFGS-B polish (EXP_HYBRID_CMAES_LBFGSB_001) and Powell polish (EXP_POWELL_POLISH_001) both FAILED. All alternative polish methods worse than NM.",
      "finding": "BFGS requires finite diff gradient (2n+1 extra evals/iter). L-BFGS-B already tested this approach and failed. NM is optimal for 2-4D polish. local_search family EXHAUSTED."
    },
    {
      "id": "EXP_LRA_CMAES_001",
      "priority": 99,
      "family": "cmaes_advanced",
      "status": "completed",
      "worker": "W1",
      "name": "learning_rate_adapted_cmaes",
      "result": "FAILED - cmaes library underperforms pycma. lr_adapt=True: 1.1440 @ 53 min, lr_adapt=False: 1.1457 @ 52 min. Both worse than baseline 1.1688 @ 58.4 min.",
      "score": 1.1457,
      "time_min": 52.0,
      "finding": "LRA hurts slightly (-0.0017). Main issue is cmaes library vs pycma, not LRA. Stay with pycma.",
      "description": "Use CMA-ES with Learning Rate Adaptation (LRA-CMA-ES) for better handling of multimodal/noisy landscapes",
      "hypothesis": "LRA-CMA-ES maintains constant signal-to-noise ratio in learning. May improve convergence on difficult samples where standard CMA-ES struggles. Based on 2024 paper on adaptive learning rates.",
      "research_source": "CMA-ES with Learning Rate Adaptation (arXiv 2024). cmaes Python library supports LRA.",
      "why_different": "Standard CMA-ES has fixed learning rates. LRA adapts them based on fitness landscape characteristics. May help with multimodal samples.",
      "implementation": [
        "1. Use cmaes library with lr_adapt=True option",
        "2. Compare convergence on difficult 2-source samples",
        "3. Check if improved handling of noisy RMSE landscape",
        "4. Compare total time and final accuracy"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 58 min",
      "abort_criteria": "LRA overhead exceeds any accuracy benefit"
    },
    {
      "id": "EXP_LOWER_SIGMA_001",
      "priority": 99,
      "family": "sigma_tuning",
      "status": "completed",
      "worker": "W1",
      "name": "lower_sigma_baseline",
      "description": "Test lower initial sigma (0.15/0.18) vs baseline (0.15/0.20)",
      "result": "FAILED - Both lower (0.15/0.18) and higher (0.18/0.22) sigma are WORSE than baseline (0.15/0.20). Baseline sigma is locally optimal.",
      "score": 1.1335,
      "time_min": 39.6,
      "finding": "Lower sigma: 1.1335 @ 39.6 min (-0.0353). Higher sigma: 1.1379 @ 38.5 min (-0.0309). Baseline sigma 0.15/0.20 is optimal. sigma_tuning family EXHAUSTED."
    },
    {
      "id": "EXP_FEVAL_POLISH_TRADE_001",
      "priority": 99,
      "family": "budget_reallocation",
      "status": "completed",
      "worker": "W2",
      "result": "ABORTED - Prior evidence: (1) EXP_EARLY_STOP_CMA_001 showed CMA-ES improves >1% throughout - reducing fevals hurts accuracy. (2) EXP_EXTENDED_POLISH_001 showed 12 polish = +24 min for only +0.0015 score. Trading fevals for polish is not viable.",
      "finding": "budget_reallocation family EXHAUSTED. CMA-ES fevals are not tradeable - they're essential for covariance learning.",
      "name": "reduced_fevals_more_polish",
      "description": "Trade CMA-ES fevals for extended NM polish (fevals 15/28, polish 12)",
      "hypothesis": "CMA-ES may be converging before using all fevals. Reallocating budget to NM polish (12 iters instead of 8) may improve final accuracy. Extended polish failed when adding budget; this keeps total budget constant.",
      "research_source": "Extended polish (12 iters) improved accuracy but went over budget. Reducing CMA-ES fevals may recover time.",
      "why_different": "Previous extended polish added time. This experiment trades CMA-ES budget for polish budget, keeping total constant.",
      "implementation": [
        "1. Reduce fevals: 1-src from 20 to 15, 2-src from 36 to 28",
        "2. Increase polish from 8 to 12 iterations",
        "3. Track CMA-ES convergence (sigma at termination)",
        "4. Compare final accuracy and total time"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 58 min",
      "abort_criteria": "Reduced CMA-ES fevals hurt more than extended polish helps"
    },
    {
      "id": "EXP_SIGMA_CONVERGENCE_001",
      "priority": 99,
      "family": "termination_criteria",
      "status": "completed",
      "worker": "W2",
      "name": "sigma_based_termination",
      "description": "Stop CMA-ES when sigma drops below threshold instead of fixed fevals",
      "result": "ABORTED - Prior evidence: EXP_EARLY_STOP_CMA_001 showed CMA-ES doesn't converge early (0 saved fevals). EXP_ADAPTIVE_BUDGET_001 showed early termination hurts accuracy.",
      "finding": "termination_criteria family EXHAUSTED. CMA-ES needs its full budget - any early termination approach will fail."
    },
    {
      "id": "EXP_SEED_ENSEMBLE_001",
      "priority": 99,
      "family": "robustness",
      "status": "completed",
      "worker": "W2",
      "name": "random_seed_ensemble",
      "description": "Run CMA-ES with 2-3 different random seeds, pick best result per sample",
      "result": "ABORTED - Prior evidence: EXP_MULTISTART_ELITE_001 tested multi-start (score 1.1529 @ 67 min, over budget). Seed ensemble is equivalent to multi-start - running 2-3x CMA-ES runs will exceed budget.",
      "finding": "robustness/ensemble approaches multiply simulation count. Not viable within 60 min budget."
    },
    {
      "id": "EXP_SAMPLE_TIMEOUT_001",
      "priority": 99,
      "family": "adaptive_timeout",
      "status": "completed",
      "worker": "W2",
      "name": "per_sample_timeout",
      "description": "Set per-sample timeout, reallocate budget from fast samples to hard ones",
      "result": "ABORTED - Prior evidence: EXP_EARLY_STOP_CMA_001 showed CMA-ES improves >1% throughout. Samples take their full budget for good reason. Reallocating time won't help. EXP_ADAPTIVE_BUDGET_001 also showed early termination hurts.",
      "finding": "adaptive_timeout family EXHAUSTED. CMA-ES needs its full budget per sample."
    },
    {
      "id": "EXP_INTENSITY_BOUNDS_001",
      "priority": 99,
      "family": "problem_bounds",
      "status": "completed",
      "worker": "W2",
      "name": "tighter_intensity_bounds",
      "description": "Use analysis of solved samples to tighten intensity bounds for remaining samples",
      "result": "ABORTED - Flawed premise: Intensity q is computed ANALYTICALLY via least squares, not optimized by CMA-ES. The bounds [0.5, 2.0] are just clipping constraints on the analytical solution. Tighter bounds cannot improve convergence - they would only risk clipping valid solutions.",
      "finding": "problem_bounds family EXHAUSTED. Intensity is not a CMA-ES search dimension."
    },
    {
      "id": "EXP_CHECKPOINTED_ADJOINT_001",
      "priority": 3,
      "family": "gradient_revisited",
      "status": "available",
      "name": "checkpointed_adjoint_method",
      "description": "Memory-efficient adjoint via checkpointing for implicit ADI time-stepping",
      "hypothesis": "Previous adjoint failed due to manual derivation errors. 2025 research shows checkpointing-based adjoint can work with implicit schemes by computing VJPs step-by-step with minimal memory.",
      "research_source": "Fast automated adjoints for spectral PDE solvers (arXiv 2025), Discretize-then-optimize adjoint (HESS 2024)",
      "why_different": "Previous adjoint: manual derivation (error-prone). This: automatic via checkpointing + implicit differentiation. Avoids JAX stability issues by keeping ADI solver.",
      "implementation": [
        "1. Keep existing ADI solver (no stability issues)",
        "2. Implement checkpointing: store state every N timesteps",
        "3. Compute adjoint backward using AD between checkpoints",
        "4. Use scipy.optimize.minimize with gradient",
        "5. Compare gradient accuracy vs finite differences"
      ],
      "success_criteria": "Gradients within 1% of finite diff AND runtime < 60 min",
      "abort_criteria": "Implementation complexity exceeds 2 days OR gradient errors > 10%"
    },
    {
      "id": "EXP_BIPOP_CMAES_001",
      "priority": 3,
      "family": "cmaes_restart_v2",
      "status": "available",
      "name": "bipop_cmaes_restart",
      "description": "BIPOP-CMA-ES alternates between large and small population restarts",
      "hypothesis": "IPOP failed because it only increases population (wasting budget). BIPOP alternates: large pop for global search, small pop for local refinement. May find better balance.",
      "research_source": "BBOB benchmarking IPOP vs BIPOP (ResearchGate), Alternative restart strategies for CMA-ES (arXiv 2012)",
      "why_different": "IPOP (tested): always increases population. BIPOP: alternates large/small. Small population can solve problems IPOP misses (Gallagher, Katsuuras functions).",
      "implementation": [
        "1. Use pycma with bipop=True option",
        "2. Set total budget same as baseline",
        "3. Let BIPOP choose population sizes automatically",
        "4. Track which regime finds best solutions",
        "5. Compare final score and diversity"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 60 min",
      "abort_criteria": "BIPOP wastes budget on small pop restarts OR same issues as IPOP"
    },
    {
      "id": "EXP_GAPPY_CPOD_001",
      "priority": 4,
      "family": "surrogate_v2",
      "status": "available",
      "name": "gappy_clustering_pod",
      "description": "Gappy Clustering-POD for thermal field reconstruction with heterogeneous samples",
      "hypothesis": "Standard POD failed due to sample-specific physics (varying kappa). Gappy C-POD clusters similar samples first, then builds separate POD bases per cluster. May handle heterogeneity.",
      "research_source": "Optimization of Sparse Sensor Layouts with Gappy C-POD (Sensors 2025)",
      "why_different": "Standard POD: one basis for all samples (failed). Gappy C-POD: cluster by physics params (kappa, bc), build cluster-specific bases. Online: identify cluster, use its basis.",
      "implementation": [
        "1. Analyze samples: cluster by (kappa, bc_type, T0_stats)",
        "2. For each cluster: generate training snapshots",
        "3. Build POD basis per cluster",
        "4. Online: identify sample cluster, use appropriate basis",
        "5. Compare speedup vs accuracy tradeoff"
      ],
      "success_criteria": "Speedup >= 2x AND score loss <= 0.01",
      "abort_criteria": "Clusters too small for reliable POD OR kappa variation too continuous"
    },
    {
      "id": "EXP_PHYSICS_CS_001",
      "priority": 4,
      "family": "inverse_method",
      "status": "available",
      "name": "physics_compressive_sensing",
      "description": "Decomposed Physics-Based Compressive Sensing (D-PBCS) for heat source detection",
      "hypothesis": "D-PBCS combines compressed sensing sparsity constraints with heat equation physics. Can reconstruct heat sources from sparse measurements without iterative optimization.",
      "research_source": "D-PBCS for inverse heat source detection (ScienceDirect July 2025)",
      "why_different": "CMA-ES: iterative black-box optimization (many simulations). D-PBCS: single linear solve with sparsity constraints. Physics-informed, not data-driven like failed surrogates.",
      "implementation": [
        "1. Formulate heat source as sparse vector on grid",
        "2. Build observation matrix (sensor locations)",
        "3. Add physics constraints (heat equation residual)",
        "4. Solve: min ||source||_1 s.t. ||Y_obs - H*source||_2 < epsilon",
        "5. Compare accuracy and speed vs CMA-ES"
      ],
      "success_criteria": "Score >= 1.16 AND time <= 30 min (direct solve should be fast)",
      "abort_criteria": "Heat source not actually sparse (continuous distribution) OR physics constraints don't match problem"
    }
  ],
  "algorithm_families": {
    "evolutionary_cmaes": "EXHAUSTED - 34 experiments, no more tuning",
    "evolutionary_other": "PSO failed, may try genetic algorithms",
    "gradient_based": "EXHAUSTED - Adjoint wrong (1e-6 error), JAX blocked (ADI stability), finite diff too slow (99 min). NM is optimal.",
    "gradient_free_local": "Nelder-Mead OK for refinement, COBYLA too slow",
    "surrogate": "Custom NN failed. lq-CMA-ES FAILED (API mismatch). POD re-enabled.",
    "surrogate_lq": "FAILED - fmin_lq_surr returns ONE solution, we need MULTIPLE candidates",
    "surrogate_pod": "FAILED - Sample-specific physics (kappa varies) prevents universal POD basis. Online POD defeats purpose.",
    "decomposition": "ABANDONED - Fast ICA failed due to refinement overhead",
    "ensemble": "ABANDONED - 5.8x over budget, worse score. Fundamentally unworkable.",
    "hybrid": "FAILED - Sequential handoff doesn't fit in budget.",
    "bayesian_opt": "FAILED - GP surrogate doesn't model thermal RMSE landscape.",
    "multi_fidelity_spatial": "FAILED - Coarse grid (SPATIAL) RMSE landscape differs from fine grid.",
    "temporal_fidelity": "SUCCESS! 40% timesteps achieves 1.1362 @ 39 min. NEW BASELINE.",
    "temporal_fidelity_extended": "EXHAUSTED - sigma tuning, IPOP, adaptive timesteps all failed",
    "budget_allocation": "FAILED - Early termination hurts CMA-ES accuracy.",
    "meta_learning": "EXHAUSTED - Both cluster_transfer and WS-CMA-ES FAILED.",
    "preprocessing": "ABORTED - n_sources already in sample data.",
    "diversity": "EXHAUSTED - Niching hurts accuracy more than it helps diversity",
    "initialization": "EXHAUSTED - Gradient-based init WORSE than simple hottest-sensor. Smart init is optimal.",
    "initialization_v2": "EXHAUSTED - 24% samples have boundary sources. Biasing away hurts. Smart init is optimal.",
    "---NEW FAMILIES TO RESEARCH---": "========================================",
    "adjoint_gradient": "FAILED - Manual adjoint derivation is error-prone for ADI time-stepping. Gradient 1e-6 of correct value. Do NOT retry manual adjoint.",
    "differentiable_simulation": "FAILED - Explicit Euler stability requires 4x more timesteps than implicit ADI. JAX autodiff incompatible.",
    "nonlinear_least_squares": "FAILED - LM is local optimizer that gets stuck in local minima. Finite diff Jacobian too expensive. TRF likely same issue.",
    "hybrid_gradient": "FAILED - L-BFGS-B polish NOT better than NM. Finite diff overhead outweighs gradient advantage. NM x8 is optimal.",
    "variable_projection": "FAILED - Baseline already uses VP (analytical q). Gauss-Newton on positions gets stuck in local minima. CMA-ES + analytical q is optimal.",
    "frequency_domain": "NOT TRIED - Heat equation simplifies in Fourier space. May be faster/more accurate.",
    "neural_operator": "EXHAUSTED - Surrogates fail (sample-specific RMSE). PINN fails (needs gradients). No NN approach viable.",
    "alternative_es": "FAILED - OpenAI ES's diagonal covariance loses correlations. CMA-ES optimal for low-dim expensive problems. Do not pursue Natural ES/PEPG.",
    "differential_evolution": "FAILED - DE's mutation/crossover less efficient than CMA-ES covariance adaptation. Best 1.1325 @ 35.2 min (-0.0037 score). CMA-ES is optimal.",
    "multi_objective": "NOT TRIED - Pareto optimization of accuracy/speed jointly.",
    "problem_reformulation": "FAILED - Polar parameterization is WORSE than Cartesian (-0.0216 score, +14.3 min). Rectangular domain incompatible with polar coords. Don't change coordinate systems.",
    "simulated_annealing": "EXHAUSTED - SA is 2-5x slower than CMA-ES with local search, 23% worse without. Do NOT retry.",
    "surrogate_assisted": "ABORTED - RMSE landscapes are sample-specific. Surrogates from some samples don't generalize to others.",
    "cmaes_improvement": "FAILED - Two-phase restart (large sigma then small sigma) doesn't improve. Phase 2 adds overhead without accuracy gain, reduces diversity.",
    "loss_function": "FAILED - Optimizing weighted RMSE finds different optimum than unweighted (scored). Score 1.0131 vs 1.1688. Don't change the loss function.",
    "multi_start": "FAILED - Multi-start elite selection adds overhead without improving accuracy. Early generation fitness NOT predictive of final quality. Single-start with smart init is optimal.",
    "refinement": "EXHAUSTED - Adaptive NM iterations does NOT improve. Fixed 8 NM is optimal. Adaptive batching adds overhead. Source-count based (6/10) worse.",
    "efficiency": "EXHAUSTED - Early rejection adds overhead (191 sims vs 100 baseline). 8.6% rejection rate not enough to break even. 40% temporal is optimal efficiency.",
    "loss_reformulation": "FAILED - Both weighted RMSE and log-RMSE failed. RMSE is optimal objective.",
    "cmaes_enhancement": "EXHAUSTED - Adaptive popsize (2.1x over budget). Combined with IPOP and larger popsize failures: any popsize manipulation hurts. Default CMA-ES popsize is optimal.",
    "temporal_sampling": "FAILED - Random timesteps requires full simulation (3-4x slower). Early timesteps are most informative.",
    "cmaes_variants": "EXHAUSTED - sep-CMA-ES (5.3x over budget, -0.02 score) and dd-CMA-ES (no benefit) both FAILED. Position correlations require full covariance. Standard CMA-ES is optimal.",
    "sigma_tuning": "EXHAUSTED - Lower sigma (0.15/0.18) and higher sigma (0.18/0.22) both WORSE than baseline (0.15/0.20). Baseline sigma is locally optimal."
  },
  "completed_experiments": [
    "EXP_WEIGHTED_LOSS_001",
    "EXP_CMAES_RESTART_BEST_001",
    "CMAES_TUNING_BATCH",
    "EXP_PSO_001",
    "EXP_BASINHOPPING_001",
    "EXP_DIFFEVO_001",
    "EXP_NM_MULTISTART_001",
    "EXP_HYBRID_TWOSTAGE_001",
    "EXP_ULTRACOARSE_001",
    "EXP_ADAPTIVE_SIGMA_001",
    "EXP_OUTLIER_FOCUS_001",
    "EXP_TARGETED_2SRC_001",
    "EXP_SURROGATE_NN_001",
    "EXP_COBYLA_REFINE_001",
    "EXP_FAST_ICA_001",
    "EXP_ENSEMBLE_001",
    "EXP_TRANSFER_LEARN_001",
    "EXP_LQ_CMAES_001",
    "EXP_BAYESIAN_OPT_001",
    "EXP_MULTIFID_OPT_001",
    "EXP_SEQUENTIAL_HANDOFF_001",
    "EXP_FAST_SOURCE_DETECT_001",
    "EXP_WS_CMAES_001",
    "EXP_ADAPTIVE_BUDGET_001",
    "EXP_TEMPORAL_FIDELITY_001",
    "EXP_TEMPORAL_HIGHER_SIGMA_001",
    "EXP_NICHING_CMAES_001",
    "EXP_EXTENDED_POLISH_001",
    "EXP_IPOP_TEMPORAL_001",
    "EXP_ADAPTIVE_TIMESTEP_001",
    "EXP_PHYSICS_INIT_001",
    "EXP_POD_SURROGATE_001",
    "EXP_2SOURCE_FOCUS_001",
    "EXP_ACTIVE_CMAES_001",
    "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
    "EXP_LARGER_POPSIZE_001",
    "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
    "EXP_PARAMETER_SCALING_001",
    "EXP_BOUNDARY_AWARE_INIT_001",
    "EXP_JAX_AUTODIFF_001",
    "EXP_LEVENBERG_MARQUARDT_001",
    "EXP_HYBRID_CMAES_LBFGSB_001",
    "EXP_CONJUGATE_GRADIENT_001",
    "EXP_OPENAI_ES_001",
    "EXP_DIFFERENTIAL_EVOLUTION_001",
    "EXP_SEPARABLE_VP_001",
    "EXP_ADAPTIVE_SA_001",
    "EXP_PRETRAINED_SURROGATE_001",
    "EXP_SURROGATE_CMAES_001",
    "EXP_PINN_DIRECT_001",
    "EXP_MULTISTART_ELITE_001",
    "EXP_ADAPTIVE_NM_POLISH_001",
    "EXP_EARLY_REJECTION_001",
    "EXP_POLAR_PARAM_001",
    "EXP_SOLUTION_INJECTION_001",
    "EXP_LOG_RMSE_LOSS_001",
    "EXP_ADAPTIVE_POPSIZE_001",
    "EXP_RANDOM_TIMESTEPS_001",
    "EXP_DD_CMAES_001",
    "EXP_POWELL_POLISH_001",
    "EXP_LRA_CMAES_001",
    "EXP_COORD_SIGMA_001",
    "EXP_BFGS_POLISH_001",
    "EXP_LOWER_SIGMA_001"
  ]
}