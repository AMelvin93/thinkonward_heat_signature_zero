{
  "version": "5.4",
  "last_updated": "2026-01-20T02:10:00Z",
  "notes": "QUEUE REFILLED after deep research. Added 6 NEW experiments based on: adjoint methods, JAX autodiff, hybrid CMA-ES+gradient, OpenAI-ES, pre-trained surrogate, PINN. Key insight: L-BFGS-B accuracy (1.1627) exists but needs O(1) gradients instead of O(2n) finite differences.",
  "queue_status": "HEALTHY",

  "experiment_queue": [
    {
      "id": "EXP_LQ_CMAES_001",
      "priority": 99,
      "family": "surrogate_lq",
      "status": "completed",
      "result": "FAILED - API mismatch. fmin_lq_surr returns ONE solution, we need MULTIPLE candidates. 29-71% worse RMSE, 7-17% slower.",
      "score": 0.253,
      "time_min": 64.1,
      "worker": "W1",
      "name": "lq_cma_es_builtin",
      "description": "Use pycma's built-in lq-CMA-ES (linear-quadratic surrogate model)",
      "finding": "fmin_lq_surr is designed for single-objective returning ONE best solution. Our scoring needs MULTIPLE diverse candidates. Fundamental mismatch."
    },
    {
      "id": "EXP_SEQUENTIAL_HANDOFF_001",
      "priority": 99,
      "family": "hybrid",
      "status": "completed",
      "result": "FAILED - Best in-budget 1.1132 is WORSE than baseline 1.1247. Best overall 1.1439 @ 126.5 min (2x over budget).",
      "score": 1.1132,
      "time_min": 56.6,
      "worker": "W2",
      "name": "cmaes_to_nm_sequential",
      "description": "CMA-ES exploration then hand off best to Nelder-Mead run",
      "finding": "Sequential handoff fundamentally doesn't work. NM improves score (+0.0192) when given time, but time overhead is prohibitive."
    },
    {
      "id": "EXP_BAYESIAN_OPT_001",
      "priority": 99,
      "family": "bayesian_opt",
      "status": "completed",
      "result": "FAILED - GP surrogate doesn't model thermal RMSE landscape. 31-91% worse accuracy, or 45% over budget if tuned.",
      "score": 0.31,
      "time_min": 57.6,
      "worker": "W1",
      "name": "bayesian_optimization_gp",
      "description": "Use Bayesian Optimization with Gaussian Process surrogate as alternative to CMA-ES",
      "finding": "Bayesian Optimization is NOT suitable for thermal inverse problem. GP surrogate fails to capture the complex RMSE landscape."
    },
    {
      "id": "EXP_MULTIFID_OPT_001",
      "priority": 99,
      "family": "multi_fidelity",
      "status": "completed",
      "result": "FAILED - Coarse grid RMSE landscape differs from fine grid. Best in-budget: RMSE 0.259 @ 45 min (45% worse than baseline).",
      "score": 0.259,
      "time_min": 44.9,
      "worker": "W1",
      "name": "multi_fidelity_proper_ratio",
      "actual_folder": "multi_fidelity_pyramid",
      "description": "Multi-fidelity optimization with literature-recommended grid ratios",
      "finding": "Multi-fidelity via grid coarsening doesn't work for inverse problems. RMSE landscape is fundamentally different at different resolutions."
    },
    {
      "id": "EXP_FAST_SOURCE_DETECT_001",
      "priority": 99,
      "family": "preprocessing",
      "status": "completed",
      "result": "ABORTED - Invalid premise. n_sources already in sample data.",
      "worker": "W2",
      "name": "fast_source_count_detection",
      "description": "Fast detection of 1-source vs 2-source BEFORE optimization",
      "finding": "Baseline already uses sample['n_sources']. Detection not needed."
    },
    {
      "id": "EXP_WS_CMAES_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - WS-CMA-ES causes divergence. Best: RMSE 0.276 @ 66 min (62% worse than baseline).",
      "score": 0.276,
      "time_min": 65.8,
      "worker": "W1",
      "name": "warm_start_cmaes",
      "description": "Use CyberAgentAILab's WS-CMA-ES to transfer solutions between similar samples",
      "finding": "WS-CMA-ES doesn't work for thermal inverse problems. Each sample is unique with no shared optimization landscape structure."
    },
    {
      "id": "EXP_TRANSFER_LEARN_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - Sensor feature clustering doesn't predict solution similarity",
      "score": 1.0804,
      "time_min": 59.7,
      "worker": "W2",
      "name": "sample_clustering_transfer",
      "actual_folder": "cluster_transfer",
      "description": "Cluster similar samples, transfer solutions between them",
      "finding": "Clustering by sensor features doesn't predict solution similarity. Transfer learning doesn't work for this problem."
    },
    {
      "id": "EXP_ADAPTIVE_BUDGET_001",
      "priority": 99,
      "family": "budget_allocation",
      "status": "completed",
      "result": "FAILED - Early termination hurts accuracy. Best in-budget: 1.1143 @ 56 min (1% worse than baseline).",
      "score": 1.1143,
      "time_min": 56.2,
      "worker": "W1",
      "name": "adaptive_sample_budget",
      "description": "Dynamically allocate fevals per sample based on convergence speed",
      "finding": "Early termination based on sigma/stagnation hurts accuracy. CMA-ES needs full budget."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity",
      "status": "completed",
      "result": "SUCCESS - NEW BEST! Score 1.1362 @ 39 min (+0.0115 vs baseline, 32% faster).",
      "score": 1.1362,
      "time_min": 39.0,
      "worker": "W2",
      "name": "early_timestep_filtering",
      "description": "Use reduced timesteps (40%) for candidate filtering, same spatial grid",
      "finding": "40% timesteps is optimal. Maintains 100x50 spatial grid, RMSE correlation 0.95+. Counterintuitive: more fevals HURT."
    },
    {
      "id": "EXP_TEMPORAL_HIGHER_SIGMA_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Higher sigma increases time disproportionately. Cannot beat W2 baseline within budget.",
      "score": 1.1745,
      "time_min": 63.0,
      "best_in_budget_score": 1.1584,
      "best_in_budget_time": 52.1,
      "worker": "W1",
      "name": "temporal_40pct_higher_sigma",
      "description": "Combine 40% temporal fidelity with higher sigma (0.25/0.30) for better accuracy",
      "finding": "Higher sigma (0.25/0.30) improves score by +0.0057 but adds 5+ min. Reducing polish to stay in budget loses the accuracy gain. W2's sigma 0.18/0.22 is optimal."
    },
    {
      "id": "EXP_NICHING_CMAES_001",
      "priority": 99,
      "family": "diversity",
      "status": "completed",
      "result": "FAILED - Scoring formula AVERAGES accuracy over candidates. Adding worse diverse candidates hurts score. Baseline already at 2.75/3 N_valid.",
      "score": 1.0622,
      "time_min": 46.9,
      "worker": "W2",
      "name": "niching_cmaes_diversity",
      "description": "Use Niching CMA-ES to maintain population diversity for multiple candidate solutions",
      "finding": "Diversity is NOT the bottleneck. Baseline already achieves 80% 3-candidate samples. Taboo-based niching hurts accuracy more than diversity helps."
    },
    {
      "id": "EXP_IPOP_TEMPORAL_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - IPOP adds time overhead (75.7 min) without improving accuracy. Splitting fevals across restarts reduces convergence.",
      "score": 1.1687,
      "time_min": 75.7,
      "worker": "W2",
      "name": "ipop_cmaes_temporal",
      "description": "IPOP-CMA-ES (increasing population) with 40% temporal fidelity",
      "finding": "IPOP divides feval budget across restarts, each restart gets insufficient fevals. Problem doesn't have local optima - restarts don't help."
    },
    {
      "id": "EXP_PHYSICS_INIT_001",
      "priority": 99,
      "family": "initialization",
      "status": "completed",
      "result": "FAILED - Gradient init is WORSE than smart init: -0.0046 score, +2.3 min",
      "score": 1.1639,
      "time_min": 69.6,
      "worker": "W1",
      "name": "physics_informed_init",
      "description": "Use sensor temperature gradients to initialize source location estimates",
      "finding": "Temperature gradients at sensors don't accurately point to sources. Heat diffusion corrupts gradient signal. Simple hottest-sensor init is already optimal."
    },
    {
      "id": "EXP_ADAPTIVE_TIMESTEP_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Adaptive timestep switching during CMA-ES is counterproductive. Best: 1.1635 @ 69.9 min (both worse than baseline).",
      "score": 1.1635,
      "time_min": 69.9,
      "worker": "W2",
      "name": "adaptive_timestep_fraction",
      "description": "Start with 25% timesteps, increase to 40% as CMA-ES converges",
      "finding": "CMA-ES covariance adaptation requires consistent fitness landscape. Switching fidelity mid-run disrupts learning and leads to worse performance."
    },
    {
      "id": "EXP_POD_SURROGATE_001",
      "priority": 99,
      "family": "surrogate_pod",
      "status": "completed",
      "result": "ABORTED - POD not viable for this problem due to sample-specific physics (kappa varies). Can't pre-build universal basis.",
      "worker": "W2",
      "name": "pod_reduced_order_model",
      "description": "Use POD (Proper Orthogonal Decomposition) as fast surrogate for simulation",
      "finding": "Each sample has unique physics (kappa, bc, T0). POD requires pre-computed snapshots from SAME system. Online POD would need simulations, defeating purpose. Temporal fidelity already provides similar speedup."
    },
    {
      "id": "EXP_EXTENDED_POLISH_001",
      "priority": 99,
      "family": "refinement",
      "status": "completed",
      "result": "FAILED - 12 iterations takes 82.3 min (37% over budget) for only +0.0015 score. 8 iterations is optimal.",
      "score": 1.1703,
      "time_min": 82.3,
      "worker": "W2",
      "name": "extended_nm_polish",
      "description": "Increase NM polish iterations from 8 to 12-16 for better accuracy",
      "finding": "More polish iterations exceeds time budget. Each additional iteration adds ~6 min on 80 samples. 8 is already optimal."
    },
    {
      "id": "EXP_LARGER_POPSIZE_001",
      "priority": 99,
      "family": "cmaes_accuracy",
      "status": "completed",
      "result": "FAILED - Popsize=12 adds 14 min without improving accuracy. Default popsize is optimal.",
      "score": 1.1666,
      "time_min": 73.0,
      "worker": "W1",
      "name": "larger_cmaes_population",
      "description": "Use larger CMA-ES population size for better accuracy in inverse problem",
      "finding": "Larger popsize reduces generations with fixed feval budget. CMA-ES needs multiple generations to adapt covariance. Default popsize (4+3*ln(n)) is already optimal."
    },
    {
      "id": "EXP_2SOURCE_FOCUS_001",
      "priority": 99,
      "family": "source_specific",
      "status": "completed",
      "result": "FAILED - Specialized feval allocation hurts performance. Reducing 1-src hurts accuracy, increasing 2-src doesn't help.",
      "score": 1.1620,
      "time_min": 69.3,
      "worker": "W2",
      "name": "two_source_specialized",
      "description": "Specialized optimization for 2-source samples (60% of data, higher RMSE)",
      "finding": "Baseline 20/36 feval split is already optimal. 2-source is harder due to 4D search space, not under-optimization. More fevals don't help."
    },
    {
      "id": "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
      "priority": 99,
      "family": "sigma_scheduling",
      "status": "completed",
      "result": "ABORTED - Prior evidence shows this approach will fail. CMA-ES already adapts sigma naturally.",
      "worker": "W1",
      "name": "adaptive_sigma_schedule",
      "description": "Start with higher sigma (0.25/0.30), decay to lower sigma (0.18/0.22) during optimization",
      "finding": "EXP_TEMPORAL_HIGHER_SIGMA_001 showed high sigma adds time without in-budget accuracy gain. EXP_ADAPTIVE_TIMESTEP_001 showed changing conditions mid-run disrupts CMA-ES. No point testing a known-to-fail approach."
    },
    {
      "id": "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "ABORTED - Prior experiment showed truncated polish HURTS accuracy. 40% polish gave -0.0346 score. 60% would similarly hurt.",
      "worker": "W2",
      "name": "progressive_polish_fidelity",
      "description": "Use progressive timestep fidelity during NM polish: 60% early, 100% final",
      "finding": "NM polish requires full timesteps for accurate refinement. Truncated polish 'overfits to proxy noise'. Full timestep polish is optimal."
    },
    {
      "id": "EXP_ACTIVE_CMAES_001",
      "priority": 99,
      "family": "cmaes_variants",
      "status": "completed",
      "result": "ABORTED - Wrong premise. CMA_active already defaults to True in pycma. Baseline already uses active CMA-ES.",
      "worker": "W2",
      "name": "active_cmaes_covariance",
      "description": "Use Active CMA-ES variant with enhanced negative covariance update",
      "finding": "pycma's CMA_active option defaults to True. The baseline already uses active covariance update. No experiment needed."
    },
    {
      "id": "EXP_PARAMETER_SCALING_001",
      "priority": 99,
      "family": "problem_specific",
      "status": "completed",
      "result": "ABORTED - Wrong premise. CMA-ES only optimizes positions (x,y). Intensity (q) is computed analytically via least squares. No scaling needed.",
      "worker": "W1",
      "name": "weighted_parameter_scaling",
      "description": "Different scaling/weights for position (x,y) vs intensity (q) parameters",
      "finding": "Optimizer already separates position (CMA-ES) from intensity (analytical). CMA-ES only sees (x,y) or (x1,y1,x2,y2), not q."
    },
    {
      "id": "EXP_BOUNDARY_AWARE_INIT_001",
      "priority": 99,
      "family": "initialization_v2",
      "status": "completed",
      "result": "ABORTED - Data analysis shows 24% of samples have boundary hotspots. Biasing away would hurt these cases.",
      "worker": "W1",
      "name": "boundary_aware_initialization",
      "description": "Initialize heat sources away from domain boundaries where physics differs",
      "finding": "24% of samples have hottest sensor near boundary (<10% margin). Abort criteria met: biasing away hurts boundary source cases. initialization_v2 family should be marked EXHAUSTED."
    },
    {
      "id": "EXP_COARSE_SURROGATE_001",
      "priority": 99,
      "family": "surrogate",
      "status": "deprioritized",
      "name": "coarse_grid_as_surrogate",
      "description": "Use 40x20 coarse grid as cheap surrogate for candidate filtering",
      "note": "DEPRIORITIZED - Multi-fidelity via spatial coarsening PROVEN to fail (EXP_MULTIFID_OPT_001). Different resolution = different landscape."
    },
    {
      "id": "EXP_ENSEMBLE_001",
      "priority": 99,
      "family": "ensemble",
      "status": "completed",
      "result": "FAILED - 347.9 min (5.8x over budget), score 1.0972 (WORSE than baseline 1.1247)",
      "score": 1.0972,
      "time_min": 347.9,
      "worker": "W1",
      "name": "optimizer_ensemble_voting",
      "description": "Run multiple optimizers in parallel, take best result per sample",
      "finding": "Nelder-Mead won 74% but running both optimizers doubles simulation count. Key: REDUCE simulations, not add optimizers."
    },
    {
      "id": "EXP_SURROGATE_NN_001",
      "priority": 99,
      "family": "surrogate",
      "status": "completed",
      "result": "FAILED - Online learning doesn't work with parallel processing",
      "score": 1.1203,
      "time_min": 75.6,
      "worker": "W1",
      "name": "neural_network_surrogate_prefilter"
    },
    {
      "id": "EXP_COBYLA_REFINE_001",
      "priority": 99,
      "family": "gradient_free_local",
      "status": "completed",
      "result": "FAILED - 88.8 min over budget, COBYLA uses 20+ extra evals vs NM's 3-6",
      "score": 1.1235,
      "time_min": 88.8,
      "worker": "W2",
      "name": "cobyla_trust_region_refinement"
    },
    {
      "id": "EXP_FAST_ICA_001",
      "priority": 99,
      "family": "decomposition",
      "status": "completed",
      "result": "FAILED - 151.1 min projected (91 min over budget), coarse-to-fine adds overhead",
      "score": 1.1046,
      "time_min": 151.1,
      "worker": "W2",
      "name": "accelerated_ica_decomposition",
      "finding": "Coarse-to-fine adds massive overhead. ICA itself is fast but extra refinement stage doubles simulation count."
    },
    {
      "id": "EXP_CONJUGATE_GRADIENT_001",
      "priority": 1,
      "family": "adjoint_gradient",
      "status": "claimed_by_W2",
      "name": "conjugate_gradient_adjoint",
      "description": "Conjugate Gradient Method with adjoint equation for O(1) gradient computation",
      "hypothesis": "L-BFGS-B achieved 1.1627 accuracy but took 202 min due to finite differences. Adjoint method computes gradients in O(1) forward + O(1) adjoint solve, enabling gradient-based optimization within budget.",
      "research_source": "Tutorial on adjoint method for inverse problems (ScienceDirect). Heat source estimation with conjugate gradient (ResearchGate).",
      "why_different": "Previous gradient methods used O(2n) finite differences. Adjoint gives EXACT gradients in O(1). This is a fundamental efficiency improvement.",
      "implementation": [
        "1. Implement adjoint PDE solver for heat equation",
        "2. Compute gradient: dL/d(params) via adjoint state",
        "3. Use scipy.optimize.minimize with 'CG' or 'L-BFGS-B' method",
        "4. Compare accuracy and speed vs CMA-ES baseline",
        "5. Can combine: CMA-ES for init, then gradient refinement"
      ],
      "success_criteria": "Score >= 1.18 AND time <= 55 min (gradient accuracy + speed)",
      "abort_criteria": "Adjoint implementation too complex or gradients not accurate enough"
    },
    {
      "id": "EXP_JAX_AUTODIFF_001",
      "priority": 2,
      "family": "differentiable_simulation",
      "status": "claimed_by_W1",
      "name": "jax_differentiable_solver",
      "description": "Rewrite thermal simulator in JAX for automatic differentiation",
      "hypothesis": "JAX-FEM and similar tools show differentiable PDE solvers work well. Rewriting our heat solver in JAX gives exact gradients automatically via autodiff.",
      "research_source": "JAX-FEM: Differentiable GPU-accelerated 3D FEM (ScienceDirect). Supports heat equation with automatic inverse problem solving.",
      "why_different": "No manual adjoint derivation needed. JAX computes gradients automatically. GPU acceleration bonus.",
      "implementation": [
        "1. Rewrite heat equation solver using JAX primitives",
        "2. Use jax.grad() to get gradient of RMSE w.r.t. source parameters",
        "3. Apply L-BFGS-B or Adam optimizer with exact gradients",
        "4. Compare speed: JAX GPU vs numpy CPU simulation",
        "5. Profile total time including compilation overhead"
      ],
      "success_criteria": "Score >= 1.18 AND time <= 50 min (faster with GPU)",
      "abort_criteria": "JAX rewrite too time-consuming or GPU overhead doesn't pay off for 80 samples"
    },
    {
      "id": "EXP_HYBRID_CMAES_LBFGSB_001",
      "priority": 3,
      "family": "hybrid_gradient",
      "status": "available",
      "name": "cmaes_then_gradient_refinement",
      "description": "Use CMA-ES for global search, then L-BFGS-B with finite differences for final polish",
      "hypothesis": "CMA-ES finds good basin quickly. L-BFGS-B with few iterations can refine within the basin faster than NM. Even with finite differences, 2-3 L-BFGS-B iterations may beat 8 NM iterations.",
      "research_source": "Hybrid GA-CMA-ES paper (MDPI 2024) shows hybrid approaches can leverage global+local strengths.",
      "why_different": "Previous handoff (CMA-ES to NM) failed due to overhead. L-BFGS-B may converge faster in final basin.",
      "implementation": [
        "1. Run CMA-ES with 40% timesteps (unchanged from baseline)",
        "2. Take top candidates after CMA-ES converges",
        "3. Run L-BFGS-B with maxiter=3-5 on full timesteps",
        "4. Use finite differences with small epsilon",
        "5. Compare total time and accuracy vs 8 NM iterations"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 58 min",
      "abort_criteria": "L-BFGS-B polish slower than NM polish"
    },
    {
      "id": "EXP_OPENAI_ES_001",
      "priority": 4,
      "family": "alternative_es",
      "status": "available",
      "name": "openai_evolution_strategy",
      "description": "Try OpenAI's Evolution Strategy as alternative to CMA-ES",
      "hypothesis": "OpenAI-ES is 2-3x faster than CMA-ES in practice and scales better with parallel workers. Our problem has 2-4 parameters (low dim), but scalable ES may still help.",
      "research_source": "OpenAI: Evolution Strategies as a Scalable Alternative to RL (2017). Recent hyperscale ES work (2024).",
      "why_different": "CMA-ES computes full covariance matrix. OpenAI-ES uses diagonal approximation, reducing overhead.",
      "implementation": [
        "1. Install evotorch or nevergrad library for OpenAI-ES",
        "2. Configure population size and learning rate",
        "3. Use same 40% temporal fidelity setup",
        "4. Compare convergence speed and final accuracy",
        "5. Test with different noise levels (antithetic sampling)"
      ],
      "success_criteria": "Score >= 1.16 AND time <= 50 min (faster convergence)",
      "abort_criteria": "OpenAI-ES sample efficiency much worse than CMA-ES for low-dim problem"
    },
    {
      "id": "EXP_PRETRAINED_SURROGATE_001",
      "priority": 5,
      "family": "neural_operator",
      "status": "available",
      "name": "pretrained_nn_surrogate",
      "description": "Train neural network on many simulations OFFLINE, then use as fast surrogate",
      "hypothesis": "Previous NN surrogate failed due to online learning + parallel issues. Pre-training OFFLINE on 10K+ simulations creates a fixed surrogate that works with parallel processing.",
      "research_source": "FNO inverse problems paper (AISTATS 2025). Neural operators can learn PDE solution operators with high accuracy.",
      "why_different": "Previous approach: online learning during optimization (failed). New approach: offline pre-training, fixed surrogate during optimization.",
      "implementation": [
        "1. Generate training data: 10K random source configs -> RMSE values",
        "2. Train small MLP: (x, y, [x2, y2]) -> predicted_RMSE",
        "3. Use trained surrogate for candidate filtering in CMA-ES",
        "4. Only full-simulate candidates with low surrogate RMSE",
        "5. Compare total sims needed vs baseline"
      ],
      "success_criteria": "Reduce simulations by 50%+ while maintaining score >= 1.16",
      "abort_criteria": "Surrogate accuracy too poor or training data generation too expensive"
    },
    {
      "id": "EXP_PINN_DIRECT_001",
      "priority": 6,
      "family": "neural_operator",
      "status": "available",
      "name": "pinn_inverse_heat_source",
      "description": "Use Physics-Informed Neural Network to directly solve the inverse heat source problem",
      "hypothesis": "PINNs can directly optimize for heat source parameters by encoding physics constraints in the loss. Recent 2025 paper specifically addresses heat source field inversion.",
      "research_source": "Heat source field inversion with PINN (ScienceDirect 2025). ASME Journal review on PINNs for heat transfer.",
      "why_different": "Direct approach: NN takes sensor readings, outputs source parameters. No iterative optimization loop.",
      "implementation": [
        "1. Define PINN architecture: input=sensor_temps, output=source_params",
        "2. Loss = data_mismatch + physics_residual (heat equation)",
        "3. Train on sensor observations to find source parameters",
        "4. Use PyTorch or TensorFlow with autodiff",
        "5. Compare accuracy and inference speed"
      ],
      "success_criteria": "Score >= 1.15 AND inference time < 1 sec per sample",
      "abort_criteria": "PINN requires too much training time per sample"
    }
  ],

  "algorithm_families": {
    "evolutionary_cmaes": "EXHAUSTED - 34 experiments, no more tuning",
    "evolutionary_other": "PSO failed, may try genetic algorithms",
    "gradient_based": "REVISIT WITH ADJOINT - finite differences too slow but adjoint gives O(1) gradients",
    "gradient_free_local": "Nelder-Mead OK for refinement, COBYLA too slow",
    "surrogate": "Custom NN failed. lq-CMA-ES FAILED (API mismatch). POD re-enabled.",
    "surrogate_lq": "FAILED - fmin_lq_surr returns ONE solution, we need MULTIPLE candidates",
    "surrogate_pod": "FAILED - Sample-specific physics (kappa varies) prevents universal POD basis. Online POD defeats purpose.",
    "decomposition": "ABANDONED - Fast ICA failed due to refinement overhead",
    "ensemble": "ABANDONED - 5.8x over budget, worse score. Fundamentally unworkable.",
    "hybrid": "FAILED - Sequential handoff doesn't fit in budget.",
    "bayesian_opt": "FAILED - GP surrogate doesn't model thermal RMSE landscape.",
    "multi_fidelity_spatial": "FAILED - Coarse grid (SPATIAL) RMSE landscape differs from fine grid.",
    "temporal_fidelity": "SUCCESS! 40% timesteps achieves 1.1362 @ 39 min. NEW BASELINE.",
    "temporal_fidelity_extended": "EXHAUSTED - sigma tuning, IPOP, adaptive timesteps all failed",
    "budget_allocation": "FAILED - Early termination hurts CMA-ES accuracy.",
    "meta_learning": "EXHAUSTED - Both cluster_transfer and WS-CMA-ES FAILED.",
    "preprocessing": "ABORTED - n_sources already in sample data.",
    "diversity": "EXHAUSTED - Niching hurts accuracy more than it helps diversity",
    "initialization": "EXHAUSTED - Gradient-based init WORSE than simple hottest-sensor. Smart init is optimal.",
    "initialization_v2": "EXHAUSTED - 24% samples have boundary sources. Biasing away hurts. Smart init is optimal.",
    "---NEW FAMILIES TO RESEARCH---": "========================================",
    "adjoint_gradient": "NOT TRIED - O(1) gradient computation via adjoint method. Could enable fast L-BFGS-B.",
    "differentiable_simulation": "NOT TRIED - JAX/PyTorch autodiff through simulator. Exact gradients for free.",
    "frequency_domain": "NOT TRIED - Heat equation simplifies in Fourier space. May be faster/more accurate.",
    "neural_operator": "NOT TRIED - FNO/DeepONet as fast surrogate. Can learn PDE solution operator.",
    "alternative_es": "NOT TRIED - OpenAI-ES, Natural ES, PEPG. Different tradeoffs than CMA-ES.",
    "multi_objective": "NOT TRIED - Pareto optimization of accuracy/speed jointly.",
    "problem_reformulation": "NOT TRIED - Different loss functions, regularization, parameterizations."
  },

  "completed_experiments": [
    "CMAES_TUNING_BATCH",
    "EXP_PSO_001",
    "EXP_BASINHOPPING_001",
    "EXP_DIFFEVO_001",
    "EXP_NM_MULTISTART_001",
    "EXP_HYBRID_TWOSTAGE_001",
    "EXP_ULTRACOARSE_001",
    "EXP_ADAPTIVE_SIGMA_001",
    "EXP_OUTLIER_FOCUS_001",
    "EXP_TARGETED_2SRC_001",
    "EXP_SURROGATE_NN_001",
    "EXP_COBYLA_REFINE_001",
    "EXP_FAST_ICA_001",
    "EXP_ENSEMBLE_001",
    "EXP_TRANSFER_LEARN_001",
    "EXP_LQ_CMAES_001",
    "EXP_BAYESIAN_OPT_001",
    "EXP_MULTIFID_OPT_001",
    "EXP_SEQUENTIAL_HANDOFF_001",
    "EXP_FAST_SOURCE_DETECT_001",
    "EXP_WS_CMAES_001",
    "EXP_ADAPTIVE_BUDGET_001",
    "EXP_TEMPORAL_FIDELITY_001",
    "EXP_TEMPORAL_HIGHER_SIGMA_001",
    "EXP_NICHING_CMAES_001",
    "EXP_EXTENDED_POLISH_001",
    "EXP_IPOP_TEMPORAL_001",
    "EXP_ADAPTIVE_TIMESTEP_001",
    "EXP_PHYSICS_INIT_001",
    "EXP_POD_SURROGATE_001",
    "EXP_2SOURCE_FOCUS_001",
    "EXP_ACTIVE_CMAES_001",
    "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
    "EXP_LARGER_POPSIZE_001",
    "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
    "EXP_PARAMETER_SCALING_001",
    "EXP_BOUNDARY_AWARE_INIT_001"
  ]
}
