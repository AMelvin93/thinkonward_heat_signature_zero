{
  "version": "5.5",
  "last_updated": "2026-01-22T10:00:00Z",
  "notes": "PROACTIVE RESEARCH: Added 3 new experiments (Levenberg-Marquardt, Trust-Region Reflective, Variable Projection) based on nonlinear least squares literature. LM is the classic method for inverse heat problems with fast convergence. Total available: 7 experiments.",
  "queue_status": "HEALTHY",
  "experiment_queue": [
    {
      "id": "EXP_LQ_CMAES_001",
      "priority": 99,
      "family": "surrogate_lq",
      "status": "completed",
      "result": "FAILED - API mismatch. fmin_lq_surr returns ONE solution, we need MULTIPLE candidates. 29-71% worse RMSE, 7-17% slower.",
      "score": 0.253,
      "time_min": 64.1,
      "worker": "W1",
      "name": "lq_cma_es_builtin",
      "description": "Use pycma's built-in lq-CMA-ES (linear-quadratic surrogate model)",
      "finding": "fmin_lq_surr is designed for single-objective returning ONE best solution. Our scoring needs MULTIPLE diverse candidates. Fundamental mismatch."
    },
    {
      "id": "EXP_SEQUENTIAL_HANDOFF_001",
      "priority": 99,
      "family": "hybrid",
      "status": "completed",
      "result": "FAILED - Best in-budget 1.1132 is WORSE than baseline 1.1247. Best overall 1.1439 @ 126.5 min (2x over budget).",
      "score": 1.1132,
      "time_min": 56.6,
      "worker": "W2",
      "name": "cmaes_to_nm_sequential",
      "description": "CMA-ES exploration then hand off best to Nelder-Mead run",
      "finding": "Sequential handoff fundamentally doesn't work. NM improves score (+0.0192) when given time, but time overhead is prohibitive."
    },
    {
      "id": "EXP_BAYESIAN_OPT_001",
      "priority": 99,
      "family": "bayesian_opt",
      "status": "completed",
      "result": "FAILED - GP surrogate doesn't model thermal RMSE landscape. 31-91% worse accuracy, or 45% over budget if tuned.",
      "score": 0.31,
      "time_min": 57.6,
      "worker": "W1",
      "name": "bayesian_optimization_gp",
      "description": "Use Bayesian Optimization with Gaussian Process surrogate as alternative to CMA-ES",
      "finding": "Bayesian Optimization is NOT suitable for thermal inverse problem. GP surrogate fails to capture the complex RMSE landscape."
    },
    {
      "id": "EXP_MULTIFID_OPT_001",
      "priority": 99,
      "family": "multi_fidelity",
      "status": "completed",
      "result": "FAILED - Coarse grid RMSE landscape differs from fine grid. Best in-budget: RMSE 0.259 @ 45 min (45% worse than baseline).",
      "score": 0.259,
      "time_min": 44.9,
      "worker": "W1",
      "name": "multi_fidelity_proper_ratio",
      "actual_folder": "multi_fidelity_pyramid",
      "description": "Multi-fidelity optimization with literature-recommended grid ratios",
      "finding": "Multi-fidelity via grid coarsening doesn't work for inverse problems. RMSE landscape is fundamentally different at different resolutions."
    },
    {
      "id": "EXP_FAST_SOURCE_DETECT_001",
      "priority": 99,
      "family": "preprocessing",
      "status": "completed",
      "result": "ABORTED - Invalid premise. n_sources already in sample data.",
      "worker": "W2",
      "name": "fast_source_count_detection",
      "description": "Fast detection of 1-source vs 2-source BEFORE optimization",
      "finding": "Baseline already uses sample['n_sources']. Detection not needed."
    },
    {
      "id": "EXP_WS_CMAES_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - WS-CMA-ES causes divergence. Best: RMSE 0.276 @ 66 min (62% worse than baseline).",
      "score": 0.276,
      "time_min": 65.8,
      "worker": "W1",
      "name": "warm_start_cmaes",
      "description": "Use CyberAgentAILab's WS-CMA-ES to transfer solutions between similar samples",
      "finding": "WS-CMA-ES doesn't work for thermal inverse problems. Each sample is unique with no shared optimization landscape structure."
    },
    {
      "id": "EXP_TRANSFER_LEARN_001",
      "priority": 99,
      "family": "meta_learning",
      "status": "completed",
      "result": "FAILED - Sensor feature clustering doesn't predict solution similarity",
      "score": 1.0804,
      "time_min": 59.7,
      "worker": "W2",
      "name": "sample_clustering_transfer",
      "actual_folder": "cluster_transfer",
      "description": "Cluster similar samples, transfer solutions between them",
      "finding": "Clustering by sensor features doesn't predict solution similarity. Transfer learning doesn't work for this problem."
    },
    {
      "id": "EXP_ADAPTIVE_BUDGET_001",
      "priority": 99,
      "family": "budget_allocation",
      "status": "completed",
      "result": "FAILED - Early termination hurts accuracy. Best in-budget: 1.1143 @ 56 min (1% worse than baseline).",
      "score": 1.1143,
      "time_min": 56.2,
      "worker": "W1",
      "name": "adaptive_sample_budget",
      "description": "Dynamically allocate fevals per sample based on convergence speed",
      "finding": "Early termination based on sigma/stagnation hurts accuracy. CMA-ES needs full budget."
    },
    {
      "id": "EXP_TEMPORAL_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity",
      "status": "completed",
      "result": "SUCCESS - NEW BEST! Score 1.1362 @ 39 min (+0.0115 vs baseline, 32% faster).",
      "score": 1.1362,
      "time_min": 39.0,
      "worker": "W2",
      "name": "early_timestep_filtering",
      "description": "Use reduced timesteps (40%) for candidate filtering, same spatial grid",
      "finding": "40% timesteps is optimal. Maintains 100x50 spatial grid, RMSE correlation 0.95+. Counterintuitive: more fevals HURT."
    },
    {
      "id": "EXP_TEMPORAL_HIGHER_SIGMA_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Higher sigma increases time disproportionately. Cannot beat W2 baseline within budget.",
      "score": 1.1745,
      "time_min": 63.0,
      "best_in_budget_score": 1.1584,
      "best_in_budget_time": 52.1,
      "worker": "W1",
      "name": "temporal_40pct_higher_sigma",
      "description": "Combine 40% temporal fidelity with higher sigma (0.25/0.30) for better accuracy",
      "finding": "Higher sigma (0.25/0.30) improves score by +0.0057 but adds 5+ min. Reducing polish to stay in budget loses the accuracy gain. W2's sigma 0.18/0.22 is optimal."
    },
    {
      "id": "EXP_NICHING_CMAES_001",
      "priority": 99,
      "family": "diversity",
      "status": "completed",
      "result": "FAILED - Scoring formula AVERAGES accuracy over candidates. Adding worse diverse candidates hurts score. Baseline already at 2.75/3 N_valid.",
      "score": 1.0622,
      "time_min": 46.9,
      "worker": "W2",
      "name": "niching_cmaes_diversity",
      "description": "Use Niching CMA-ES to maintain population diversity for multiple candidate solutions",
      "finding": "Diversity is NOT the bottleneck. Baseline already achieves 80% 3-candidate samples. Taboo-based niching hurts accuracy more than diversity helps."
    },
    {
      "id": "EXP_IPOP_TEMPORAL_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - IPOP adds time overhead (75.7 min) without improving accuracy. Splitting fevals across restarts reduces convergence.",
      "score": 1.1687,
      "time_min": 75.7,
      "worker": "W2",
      "name": "ipop_cmaes_temporal",
      "description": "IPOP-CMA-ES (increasing population) with 40% temporal fidelity",
      "finding": "IPOP divides feval budget across restarts, each restart gets insufficient fevals. Problem doesn't have local optima - restarts don't help."
    },
    {
      "id": "EXP_PHYSICS_INIT_001",
      "priority": 99,
      "family": "initialization",
      "status": "completed",
      "result": "FAILED - Gradient init is WORSE than smart init: -0.0046 score, +2.3 min",
      "score": 1.1639,
      "time_min": 69.6,
      "worker": "W1",
      "name": "physics_informed_init",
      "description": "Use sensor temperature gradients to initialize source location estimates",
      "finding": "Temperature gradients at sensors don't accurately point to sources. Heat diffusion corrupts gradient signal. Simple hottest-sensor init is already optimal."
    },
    {
      "id": "EXP_ADAPTIVE_TIMESTEP_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "FAILED - Adaptive timestep switching during CMA-ES is counterproductive. Best: 1.1635 @ 69.9 min (both worse than baseline).",
      "score": 1.1635,
      "time_min": 69.9,
      "worker": "W2",
      "name": "adaptive_timestep_fraction",
      "description": "Start with 25% timesteps, increase to 40% as CMA-ES converges",
      "finding": "CMA-ES covariance adaptation requires consistent fitness landscape. Switching fidelity mid-run disrupts learning and leads to worse performance."
    },
    {
      "id": "EXP_POD_SURROGATE_001",
      "priority": 99,
      "family": "surrogate_pod",
      "status": "completed",
      "result": "ABORTED - POD not viable for this problem due to sample-specific physics (kappa varies). Can't pre-build universal basis.",
      "worker": "W2",
      "name": "pod_reduced_order_model",
      "description": "Use POD (Proper Orthogonal Decomposition) as fast surrogate for simulation",
      "finding": "Each sample has unique physics (kappa, bc, T0). POD requires pre-computed snapshots from SAME system. Online POD would need simulations, defeating purpose. Temporal fidelity already provides similar speedup."
    },
    {
      "id": "EXP_EXTENDED_POLISH_001",
      "priority": 99,
      "family": "refinement",
      "status": "completed",
      "result": "FAILED - 12 iterations takes 82.3 min (37% over budget) for only +0.0015 score. 8 iterations is optimal.",
      "score": 1.1703,
      "time_min": 82.3,
      "worker": "W2",
      "name": "extended_nm_polish",
      "description": "Increase NM polish iterations from 8 to 12-16 for better accuracy",
      "finding": "More polish iterations exceeds time budget. Each additional iteration adds ~6 min on 80 samples. 8 is already optimal."
    },
    {
      "id": "EXP_LARGER_POPSIZE_001",
      "priority": 99,
      "family": "cmaes_accuracy",
      "status": "completed",
      "result": "FAILED - Popsize=12 adds 14 min without improving accuracy. Default popsize is optimal.",
      "score": 1.1666,
      "time_min": 73.0,
      "worker": "W1",
      "name": "larger_cmaes_population",
      "description": "Use larger CMA-ES population size for better accuracy in inverse problem",
      "finding": "Larger popsize reduces generations with fixed feval budget. CMA-ES needs multiple generations to adapt covariance. Default popsize (4+3*ln(n)) is already optimal."
    },
    {
      "id": "EXP_2SOURCE_FOCUS_001",
      "priority": 99,
      "family": "source_specific",
      "status": "completed",
      "result": "FAILED - Specialized feval allocation hurts performance. Reducing 1-src hurts accuracy, increasing 2-src doesn't help.",
      "score": 1.162,
      "time_min": 69.3,
      "worker": "W2",
      "name": "two_source_specialized",
      "description": "Specialized optimization for 2-source samples (60% of data, higher RMSE)",
      "finding": "Baseline 20/36 feval split is already optimal. 2-source is harder due to 4D search space, not under-optimization. More fevals don't help."
    },
    {
      "id": "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
      "priority": 99,
      "family": "sigma_scheduling",
      "status": "completed",
      "result": "ABORTED - Prior evidence shows this approach will fail. CMA-ES already adapts sigma naturally.",
      "worker": "W1",
      "name": "adaptive_sigma_schedule",
      "description": "Start with higher sigma (0.25/0.30), decay to lower sigma (0.18/0.22) during optimization",
      "finding": "EXP_TEMPORAL_HIGHER_SIGMA_001 showed high sigma adds time without in-budget accuracy gain. EXP_ADAPTIVE_TIMESTEP_001 showed changing conditions mid-run disrupts CMA-ES. No point testing a known-to-fail approach."
    },
    {
      "id": "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
      "priority": 99,
      "family": "temporal_fidelity_extended",
      "status": "completed",
      "result": "ABORTED - Prior experiment showed truncated polish HURTS accuracy. 40% polish gave -0.0346 score. 60% would similarly hurt.",
      "worker": "W2",
      "name": "progressive_polish_fidelity",
      "description": "Use progressive timestep fidelity during NM polish: 60% early, 100% final",
      "finding": "NM polish requires full timesteps for accurate refinement. Truncated polish 'overfits to proxy noise'. Full timestep polish is optimal."
    },
    {
      "id": "EXP_ACTIVE_CMAES_001",
      "priority": 99,
      "family": "cmaes_variants",
      "status": "completed",
      "result": "ABORTED - Wrong premise. CMA_active already defaults to True in pycma. Baseline already uses active CMA-ES.",
      "worker": "W2",
      "name": "active_cmaes_covariance",
      "description": "Use Active CMA-ES variant with enhanced negative covariance update",
      "finding": "pycma's CMA_active option defaults to True. The baseline already uses active covariance update. No experiment needed."
    },
    {
      "id": "EXP_PARAMETER_SCALING_001",
      "priority": 99,
      "family": "problem_specific",
      "status": "completed",
      "result": "ABORTED - Wrong premise. CMA-ES only optimizes positions (x,y). Intensity (q) is computed analytically via least squares. No scaling needed.",
      "worker": "W1",
      "name": "weighted_parameter_scaling",
      "description": "Different scaling/weights for position (x,y) vs intensity (q) parameters",
      "finding": "Optimizer already separates position (CMA-ES) from intensity (analytical). CMA-ES only sees (x,y) or (x1,y1,x2,y2), not q."
    },
    {
      "id": "EXP_BOUNDARY_AWARE_INIT_001",
      "priority": 99,
      "family": "initialization_v2",
      "status": "completed",
      "result": "ABORTED - Data analysis shows 24% of samples have boundary hotspots. Biasing away would hurt these cases.",
      "worker": "W1",
      "name": "boundary_aware_initialization",
      "description": "Initialize heat sources away from domain boundaries where physics differs",
      "finding": "24% of samples have hottest sensor near boundary (<10% margin). Abort criteria met: biasing away hurts boundary source cases. initialization_v2 family should be marked EXHAUSTED."
    },
    {
      "id": "EXP_COARSE_SURROGATE_001",
      "priority": 99,
      "family": "surrogate",
      "status": "deprioritized",
      "name": "coarse_grid_as_surrogate",
      "description": "Use 40x20 coarse grid as cheap surrogate for candidate filtering",
      "note": "DEPRIORITIZED - Multi-fidelity via spatial coarsening PROVEN to fail (EXP_MULTIFID_OPT_001). Different resolution = different landscape."
    },
    {
      "id": "EXP_ENSEMBLE_001",
      "priority": 99,
      "family": "ensemble",
      "status": "completed",
      "result": "FAILED - 347.9 min (5.8x over budget), score 1.0972 (WORSE than baseline 1.1247)",
      "score": 1.0972,
      "time_min": 347.9,
      "worker": "W1",
      "name": "optimizer_ensemble_voting",
      "description": "Run multiple optimizers in parallel, take best result per sample",
      "finding": "Nelder-Mead won 74% but running both optimizers doubles simulation count. Key: REDUCE simulations, not add optimizers."
    },
    {
      "id": "EXP_SURROGATE_NN_001",
      "priority": 99,
      "family": "surrogate",
      "status": "completed",
      "result": "FAILED - Online learning doesn't work with parallel processing",
      "score": 1.1203,
      "time_min": 75.6,
      "worker": "W1",
      "name": "neural_network_surrogate_prefilter"
    },
    {
      "id": "EXP_COBYLA_REFINE_001",
      "priority": 99,
      "family": "gradient_free_local",
      "status": "completed",
      "result": "FAILED - 88.8 min over budget, COBYLA uses 20+ extra evals vs NM's 3-6",
      "score": 1.1235,
      "time_min": 88.8,
      "worker": "W2",
      "name": "cobyla_trust_region_refinement"
    },
    {
      "id": "EXP_FAST_ICA_001",
      "priority": 99,
      "family": "decomposition",
      "status": "completed",
      "result": "FAILED - 151.1 min projected (91 min over budget), coarse-to-fine adds overhead",
      "score": 1.1046,
      "time_min": 151.1,
      "worker": "W2",
      "name": "accelerated_ica_decomposition",
      "finding": "Coarse-to-fine adds massive overhead. ICA itself is fast but extra refinement stage doubles simulation count."
    },
    {
      "id": "EXP_CONJUGATE_GRADIENT_001",
      "priority": 99,
      "family": "adjoint_gradient",
      "status": "completed",
      "result": "FAILED - Adjoint gradient is 5-6 orders of magnitude too small. Manual derivation errors cause L-BFGS-B to see near-zero gradient and not iterate.",
      "score": 0.9006,
      "time_min": 56.5,
      "worker": "W2",
      "name": "conjugate_gradient_adjoint",
      "description": "Conjugate Gradient Method with adjoint equation for O(1) gradient computation",
      "finding": "Gradient verification: adjoint=[-0.000017, 0.000004], finite_diff=[-9.079, 1.980]. Ratio ~0.000002. Manual adjoint derivation for ADI time-stepping is error-prone. Use JAX autodiff instead."
    },
    {
      "id": "EXP_JAX_AUTODIFF_001",
      "priority": 99,
      "family": "differentiable_simulation",
      "status": "completed",
      "result": "ABORTED - Fundamental incompatibility. Explicit Euler stability constraint requires 4x more timesteps than implicit ADI.",
      "worker": "W1",
      "name": "jax_differentiable_solver",
      "description": "Rewrite thermal simulator in JAX for automatic differentiation",
      "finding": "JAX autodiff requires explicit time-stepping. Stability constraint dt < 0.002061 vs original dt=0.004 means 4x more timesteps needed. Running fewer timesteps gives wrong physics (5.7x temperature error). Implicit ADI is essential for efficiency."
    },
    {
      "id": "EXP_HYBRID_CMAES_LBFGSB_001",
      "priority": 99,
      "family": "hybrid_gradient",
      "status": "completed",
      "result": "FAILED - L-BFGS-B polish NOT better than NM polish. Finite diff overhead (O(n) extra sims/iter) outweighs gradient advantage. Best in-budget L-BFGS-B: 1.1174 @ 42 min (WORSE than NM x8: 1.1415 @ 38 min).",
      "score": 1.1174,
      "time_min": 42.4,
      "worker": "W1",
      "name": "cmaes_then_gradient_refinement",
      "description": "Use CMA-ES for global search, then L-BFGS-B with finite differences for final polish",
      "finding": "L-BFGS-B finite diff gradient requires O(n) extra sims per iteration. For 4D (2-src): ~250-350 sims vs NM's ~180. NM x8 is optimal polish method. hybrid_gradient family FAILED."
    },
    {
      "id": "EXP_OPENAI_ES_001",
      "priority": 99,
      "family": "alternative_es",
      "status": "completed",
      "result": "FAILED - OpenAI ES cannot beat CMA-ES. Diagonal covariance loses correlation information. Best: 1.1204 @ 43.3 min (-0.0158 score, +4.3 min vs baseline).",
      "score": 1.1204,
      "time_min": 43.3,
      "worker": "W1",
      "name": "openai_evolution_strategy",
      "description": "Try OpenAI's Evolution Strategy as alternative to CMA-ES",
      "finding": "OpenAI ES's diagonal covariance approximation loses critical parameter correlations that CMA-ES captures. For low-dim expensive black-box optimization, CMA-ES is optimal. alternative_es family FAILED."
    },
    {
      "id": "EXP_PRETRAINED_SURROGATE_001",
      "priority": 99,
      "family": "neural_operator",
      "status": "completed",
      "name": "pretrained_nn_surrogate",
      "description": "Train neural network on many simulations OFFLINE, then use as fast surrogate",
      "hypothesis": "Previous NN surrogate failed due to online learning + parallel issues. Pre-training OFFLINE on 10K+ simulations creates a fixed surrogate that works with parallel processing.",
      "research_source": "FNO inverse problems paper (AISTATS 2025). Neural operators can learn PDE solution operators with high accuracy.",
      "why_different": "Previous approach: online learning during optimization (failed). New approach: offline pre-training, fixed surrogate during optimization.",
      "implementation": [
        "1. Generate training data: 10K random source configs -> RMSE values",
        "2. Train small MLP: (x, y, [x2, y2]) -> predicted_RMSE",
        "3. Use trained surrogate for candidate filtering in CMA-ES",
        "4. Only full-simulate candidates with low surrogate RMSE",
        "5. Compare total sims needed vs baseline"
      ],
      "success_criteria": "Reduce simulations by 50%+ while maintaining score >= 1.16",
      "abort_criteria": "Surrogate accuracy too poor or training data generation too expensive",
      "result": "ABORTED - RMSE landscape is sample-specific (avg correlation -0.167). Universal surrogate cannot predict RMSE without sample information.",
      "worker": "W2",
      "finding": "Tested RMSE landscape correlation across 5 samples. Average Spearman r = -0.167 (anti-correlated). Each sample has unique physics (kappa, bc, Y_obs). Surrogate needs sample info, defeating purpose."
    },
    {
      "id": "EXP_PINN_DIRECT_001",
      "priority": 99,
      "family": "neural_operator",
      "status": "completed",
      "name": "pinn_inverse_heat_source",
      "description": "Use Physics-Informed Neural Network to directly solve the inverse heat source problem",
      "hypothesis": "PINNs can directly optimize for heat source parameters by encoding physics constraints in the loss. Recent 2025 paper specifically addresses heat source field inversion.",
      "research_source": "Heat source field inversion with PINN (ScienceDirect 2025). ASME Journal review on PINNs for heat transfer.",
      "why_different": "Direct approach: NN takes sensor readings, outputs source parameters. No iterative optimization loop.",
      "implementation": [
        "1. Define PINN architecture: input=sensor_temps, output=source_params",
        "2. Loss = data_mismatch + physics_residual (heat equation)",
        "3. Train on sensor observations to find source parameters",
        "4. Use PyTorch or TensorFlow with autodiff",
        "5. Compare accuracy and inference speed"
      ],
      "success_criteria": "Score >= 1.15 AND inference time < 1 sec per sample",
      "abort_criteria": "PINN requires too much training time per sample",
      "result": "ABORTED - PINN requires efficient gradients. Adjoint failed (1e-6 error), JAX failed (stability), finite diff too slow (99 min vs 60 budget).",
      "worker": "W2",
      "finding": "L-BFGS-B achieves 15% better RMSE than NM but takes 2.3x longer due to finite diff. ADI solver blocks autodiff. Gradient-free methods (CMA-ES + NM) remain optimal."
    },
    {
      "id": "EXP_LEVENBERG_MARQUARDT_001",
      "priority": 99,
      "family": "nonlinear_least_squares",
      "status": "completed",
      "result": "FAILED - LM is local optimizer that gets stuck in local minima. Finite diff Jacobian needs 3-5 sims/iter. Best in-budget: 1.0350 @ 56 min (-9% vs baseline).",
      "score": 1.035,
      "time_min": 56.0,
      "worker": "W1",
      "name": "levenberg_marquardt_inverse",
      "description": "Use Levenberg-Marquardt algorithm via scipy.optimize.least_squares for inverse heat problem",
      "finding": "LM fundamentally unsuitable: (1) local optimizer vs multi-modal landscape, (2) expensive finite diff Jacobian (3-5 sims/iter), (3) CMA-ES is 3x more sample-efficient. nonlinear_least_squares family should be marked FAILED."
    },
    {
      "id": "EXP_TRUST_REGION_REFLECTIVE_001",
      "priority": 99,
      "family": "nonlinear_least_squares",
      "status": "deprioritized",
      "name": "trust_region_reflective_bounded",
      "description": "Use Trust-Region Reflective method for bounded inverse heat problem",
      "deprioritize_reason": "DEPRIORITIZED based on EXP_LEVENBERG_MARQUARDT_001 results. TRF is also a local optimizer like LM - will have same local minima issues on multi-modal RMSE landscape. Same expensive finite-diff Jacobian problem. Not worth testing.",
      "original_hypothesis": "TRF handles bounded constraints naturally (source positions have box constraints). Our x,y are bounded to domain [0,Lx]x[0,Ly]. TRF respects bounds without penalty methods, potentially more accurate."
    },
    {
      "id": "EXP_SEPARABLE_VP_001",
      "priority": 99,
      "family": "variable_projection",
      "status": "completed",
      "result": "FAILED - VP's Gauss-Newton is local optimizer that gets stuck. Baseline already uses VP implicitly (analytical q). Best: 1.0025 @ 54.3 min (-0.1337 score).",
      "score": 1.0025,
      "time_min": 54.3,
      "worker": "W1",
      "name": "variable_projection_separable",
      "description": "Exploit separable structure via Variable Projection: optimize positions, solve intensity analytically",
      "finding": "Baseline already uses VP (analytical q for each CMA-ES candidate). Gauss-Newton on positions gets stuck in local minima. CMA-ES + analytical q is optimal for exploiting separability."
    },
    {
      "id": "EXP_ADAPTIVE_SA_001",
      "priority": 99,
      "family": "simulated_annealing",
      "status": "completed",
      "name": "adaptive_simulated_annealing",
      "description": "Use Adaptive Simulated Annealing (ASA) for heat source inverse problem",
      "hypothesis": "2024 Nature paper shows ASA successfully reconstructs internal heat sources in biological tissue. ASA adaptively adjusts temperature schedule based on search state, improving global optimization for complex inverse problems.",
      "research_source": "Noninvasive reconstruction of internal heat source in biological tissue using ASA (Nature Scientific Reports, July 2024). ASA avoids complex regularization by treating inverse problem as optimization.",
      "why_different": "SA is a global optimizer like CMA-ES but with different exploration mechanism (probabilistic uphill moves). ASA adapts cooling schedule dynamically. May find different optima than CMA-ES. Very different from local methods (LM) that failed.",
      "implementation": [
        "1. Use scipy.optimize.dual_annealing (generalized SA with local search)",
        "2. Set bounds for source positions",
        "3. Use same objective function as baseline (RMSE after q optimization)",
        "4. Compare global search efficiency vs CMA-ES",
        "5. If promising, implement full ASA with adaptive cooling"
      ],
      "success_criteria": "Score >= 1.16 AND time <= 55 min",
      "abort_criteria": "SA converges slower than CMA-ES OR gets stuck in same local minima",
      "result": "FAILED - dual_annealing is fundamentally unsuitable. With local search: 2-5x over budget. Without: 23% worse accuracy.",
      "score": 0.8666,
      "time_min": 28.2,
      "worker": "W2",
      "finding": "SA local search uses L-BFGS-B finite differences = slow. Pure SA = fast but inaccurate. CMA-ES covariance adaptation is more sample-efficient."
    },
    {
      "id": "EXP_SURROGATE_CMAES_001",
      "priority": 99,
      "family": "surrogate_assisted",
      "status": "completed",
      "name": "surrogate_assisted_cmaes",
      "description": "Use kriging/GP surrogate to pre-filter CMA-ES candidates, reducing simulation count",
      "hypothesis": "SCR algorithm (2025) shows surrogate-assisted CMA-ES works well for expensive black-box functions. Build kriging surrogate from initial samples, use it to filter unpromising candidates before full simulation.",
      "research_source": "SCR: Surrogate-CMAES-RQLIF for expensive black-box optimization (Optimization and Engineering, March 2025). Automotive crashworthiness optimization with surrogate-based hyperparameter tuning.",
      "why_different": "Previous surrogate (NN online) failed due to parallel processing. This approach: (1) builds surrogate from existing samples, (2) uses kriging not NN, (3) only filters obviously bad candidates. More conservative than full surrogate replacement.",
      "implementation": [
        "1. Run baseline CMA-ES on first 10 samples, collect (params, RMSE) pairs",
        "2. Fit kriging/GP model to collected data",
        "3. For remaining samples: use surrogate to filter CMA-ES population",
        "4. Only simulate candidates with low predicted RMSE (top 50%)",
        "5. Update surrogate periodically with new data"
      ],
      "success_criteria": "Reduce simulations by 30%+ while maintaining score >= 1.15",
      "abort_criteria": "Surrogate filtering removes good candidates OR overhead exceeds savings",
      "result": "ABORTED - Prior finding: RMSE landscapes are sample-specific (avg correlation -0.167). Kriging surrogate from samples 1-10 will NOT generalize to samples 11-80.",
      "worker": "W2",
      "finding": "Based on EXP_PRETRAINED_SURROGATE_001 results: RMSE = f(x, y, Y_obs, kappa, bc) is sample-specific. Surrogate needs sample info to work, defeating the purpose."
    },
    {
      "id": "EXP_DIFFERENTIAL_EVOLUTION_001",
      "priority": 99,
      "family": "differential_evolution",
      "status": "completed",
      "result": "FAILED - DE cannot match CMA-ES accuracy. Best: 1.1325 @ 35.2 min (-0.0037 score, -3.8 min vs baseline). CMA-ES covariance adaptation is essential.",
      "score": 1.1325,
      "time_min": 35.2,
      "worker": "W1",
      "name": "differential_evolution_inverse",
      "description": "Use scipy.optimize.differential_evolution for inverse heat problem",
      "finding": "DE's mutation/crossover is less efficient than CMA-ES's covariance adaptation. DE is 10% faster but sacrifices accuracy. differential_evolution family FAILED.",
      "success_criteria": "Score >= 1.16 AND time <= 55 min",
      "abort_criteria": "DE converges slower than CMA-ES OR same local minima issue"
    },
    {
      "id": "EXP_WEIGHTED_LOSS_001",
      "priority": 4,
      "family": "loss_function",
      "status": "available",
      "name": "weighted_sensor_loss",
      "description": "Weight sensor contributions by their informativeness or reliability",
      "hypothesis": "Not all sensors equally informative. Sensors closer to sources have stronger signal. Weighting by signal-to-noise or gradient magnitude may improve optimization.",
      "research_source": "Optimal sensor weighting for inverse problems (general principle). FEM-based optimal sensor placement (arXiv 2509.19679).",
      "why_different": "Current baseline weights all sensors equally. Adaptive weighting may help optimizer focus on informative measurements.",
      "implementation": [
        "1. Compute sensor weights based on temperature variance across solutions",
        "2. Or use gradient magnitude at sensor locations",
        "3. Modify RMSE computation: weighted_rmse = sqrt(sum(w_i * (y_i - pred_i)^2))",
        "4. Test if weighted loss improves convergence or accuracy",
        "5. Compare with uniform weighting baseline"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Weighting doesn't improve or makes optimization harder"
    },
    {
      "id": "EXP_POLAR_PARAM_001",
      "priority": 5,
      "family": "problem_reformulation",
      "status": "available",
      "name": "polar_parameterization",
      "description": "Reparameterize source positions using polar coordinates centered on domain centroid",
      "hypothesis": "Cartesian (x,y) may have different landscape characteristics than polar (r,theta). CMA-ES may converge faster in one parameterization.",
      "research_source": "Reparameterization can change optimization landscape characteristics. Used successfully in robotics/control.",
      "why_different": "All experiments used Cartesian coordinates. Polar may have different curvature structure.",
      "implementation": [
        "1. Convert source positions to polar: r = sqrt((x-cx)^2 + (y-cy)^2), theta = atan2(y-cy, x-cx)",
        "2. CMA-ES optimizes (r, theta) instead of (x, y)",
        "3. Transform back to Cartesian for simulation",
        "4. Handle bounds in polar coordinates",
        "5. Compare convergence speed and accuracy"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Polar coordinates don't improve convergence or complicate boundary handling"
    },
    {
      "id": "EXP_EARLY_REJECTION_001",
      "priority": 3,
      "family": "efficiency",
      "status": "available",
      "name": "early_rejection_partial_sim",
      "description": "Reject clearly bad candidates early using partial simulation (first 10% timesteps)",
      "hypothesis": "If a candidate is very poor, this will be evident even with partial simulation. Rejecting early saves compute.",
      "research_source": "Early rejection strategies common in optimization. Multi-fidelity approaches use coarse evaluations for filtering.",
      "why_different": "Previous multi-fidelity used coarse GRID. This uses coarse TIME - same grid, fewer timesteps.",
      "implementation": [
        "1. Run first 10% timesteps (~100 steps) for each CMA-ES candidate",
        "2. Compute partial RMSE from early temperature profile",
        "3. Reject candidates with partial RMSE > 2x current best",
        "4. Run full simulation only for promising candidates",
        "5. Track time savings vs accuracy trade-off"
      ],
      "success_criteria": "Reduce simulation time by 20%+ while maintaining score >= 1.16",
      "abort_criteria": "Early rejection eliminates good candidates OR overhead exceeds savings"
    },
    {
      "id": "EXP_ADAPTIVE_NM_POLISH_001",
      "priority": 4,
      "family": "refinement",
      "status": "available",
      "name": "adaptive_nm_iterations",
      "description": "Dynamically adjust NM polish iterations based on convergence rate",
      "hypothesis": "Some samples converge quickly (4-5 NM iters), others need more (10+). Fixed 8 iters is wasteful for easy samples, insufficient for hard ones.",
      "research_source": "Adaptive algorithms adjust parameters based on progress. Early stopping when converged saves compute.",
      "why_different": "Current baseline uses fixed 8 NM iterations. Adaptive could be more efficient.",
      "implementation": [
        "1. Start NM polish with initial iterations (e.g., 4)",
        "2. Check convergence: delta_rmse < threshold?",
        "3. If not converged and iterations < max (e.g., 12), continue",
        "4. If converged early, stop and save time",
        "5. Track time savings and accuracy"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (save time on easy samples)",
      "abort_criteria": "Adaptive logic overhead exceeds savings OR accuracy drops"
    },
    {
      "id": "EXP_CMAES_RESTART_BEST_001",
      "priority": 4,
      "family": "cmaes_improvement",
      "status": "available",
      "name": "cmaes_restart_from_best",
      "description": "Run short CMA-ES, restart from best solution found with tighter sigma",
      "hypothesis": "Two-phase approach: broad search with large sigma, then focused search with small sigma around best. May improve final accuracy.",
      "research_source": "Restart strategies in CMA-ES (IPOP, BIPOP). Hansen et al.",
      "why_different": "Previous IPOP/BIPOP tested but increased population. This restarts with SAME population but tighter sigma around best.",
      "implementation": [
        "1. Run CMA-ES phase 1: sigma=0.25, 15-20 generations",
        "2. Extract best solution found",
        "3. Restart CMA-ES from best: sigma=0.05, 10-15 generations",
        "4. Compare with single-phase baseline",
        "5. Track time vs accuracy trade-off"
      ],
      "success_criteria": "Score >= 1.18 AND time <= 55 min",
      "abort_criteria": "Two-phase doesn't improve over single-phase"
    },
    {
      "id": "EXP_GRADIENT_INIT_001",
      "priority": 5,
      "family": "initialization",
      "status": "available",
      "name": "gradient_informed_init",
      "description": "Use temperature gradient direction to inform initial source position guess",
      "hypothesis": "Heat sources create temperature gradients pointing away from them. Tracing gradient backward could give better init than just hottest sensor.",
      "research_source": "Physics-based initialization. Heat equation has predictable gradient structure.",
      "why_different": "Current smart init uses hottest sensor position. Gradient-informed could be more accurate.",
      "implementation": [
        "1. Compute temperature gradient at each sensor location",
        "2. Trace gradient backward (uphill in temp) from hot sensors",
        "3. Estimate source position as intersection point or max temp region",
        "4. Use as initial guess for CMA-ES",
        "5. Compare with hottest-sensor init"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 55 min",
      "abort_criteria": "Gradient-based init is not consistently better than hottest-sensor"
    },
    {
      "id": "EXP_MULTISTART_ELITE_001",
      "priority": 4,
      "family": "multi_start",
      "status": "available",
      "name": "multistart_elite_selection",
      "description": "Run multiple short CMA-ES from different inits, continue only the best 1-2",
      "hypothesis": "Instead of running N full optimizations, run N short ones and continue only the most promising. More efficient budget allocation.",
      "research_source": "Elite selection in multi-start optimization. Racing algorithms.",
      "why_different": "Current approach runs all inits to completion. Elite selection may be more efficient.",
      "implementation": [
        "1. Start 4-5 CMA-ES runs from different smart inits",
        "2. After 5 generations, evaluate all",
        "3. Continue only top 1-2 performers",
        "4. Run to full convergence",
        "5. Compare with baseline multi-start"
      ],
      "success_criteria": "Score >= 1.17 AND time <= 50 min (better budget allocation)",
      "abort_criteria": "Elite selection kills good runs OR overhead exceeds savings"
    }
  ],
  "algorithm_families": {
    "evolutionary_cmaes": "EXHAUSTED - 34 experiments, no more tuning",
    "evolutionary_other": "PSO failed, may try genetic algorithms",
    "gradient_based": "EXHAUSTED - Adjoint wrong (1e-6 error), JAX blocked (ADI stability), finite diff too slow (99 min). NM is optimal.",
    "gradient_free_local": "Nelder-Mead OK for refinement, COBYLA too slow",
    "surrogate": "Custom NN failed. lq-CMA-ES FAILED (API mismatch). POD re-enabled.",
    "surrogate_lq": "FAILED - fmin_lq_surr returns ONE solution, we need MULTIPLE candidates",
    "surrogate_pod": "FAILED - Sample-specific physics (kappa varies) prevents universal POD basis. Online POD defeats purpose.",
    "decomposition": "ABANDONED - Fast ICA failed due to refinement overhead",
    "ensemble": "ABANDONED - 5.8x over budget, worse score. Fundamentally unworkable.",
    "hybrid": "FAILED - Sequential handoff doesn't fit in budget.",
    "bayesian_opt": "FAILED - GP surrogate doesn't model thermal RMSE landscape.",
    "multi_fidelity_spatial": "FAILED - Coarse grid (SPATIAL) RMSE landscape differs from fine grid.",
    "temporal_fidelity": "SUCCESS! 40% timesteps achieves 1.1362 @ 39 min. NEW BASELINE.",
    "temporal_fidelity_extended": "EXHAUSTED - sigma tuning, IPOP, adaptive timesteps all failed",
    "budget_allocation": "FAILED - Early termination hurts CMA-ES accuracy.",
    "meta_learning": "EXHAUSTED - Both cluster_transfer and WS-CMA-ES FAILED.",
    "preprocessing": "ABORTED - n_sources already in sample data.",
    "diversity": "EXHAUSTED - Niching hurts accuracy more than it helps diversity",
    "initialization": "EXHAUSTED - Gradient-based init WORSE than simple hottest-sensor. Smart init is optimal.",
    "initialization_v2": "EXHAUSTED - 24% samples have boundary sources. Biasing away hurts. Smart init is optimal.",
    "---NEW FAMILIES TO RESEARCH---": "========================================",
    "adjoint_gradient": "FAILED - Manual adjoint derivation is error-prone for ADI time-stepping. Gradient 1e-6 of correct value. Do NOT retry manual adjoint.",
    "differentiable_simulation": "FAILED - Explicit Euler stability requires 4x more timesteps than implicit ADI. JAX autodiff incompatible.",
    "nonlinear_least_squares": "FAILED - LM is local optimizer that gets stuck in local minima. Finite diff Jacobian too expensive. TRF likely same issue.",
    "hybrid_gradient": "FAILED - L-BFGS-B polish NOT better than NM. Finite diff overhead outweighs gradient advantage. NM x8 is optimal.",
    "variable_projection": "FAILED - Baseline already uses VP (analytical q). Gauss-Newton on positions gets stuck in local minima. CMA-ES + analytical q is optimal.",
    "frequency_domain": "NOT TRIED - Heat equation simplifies in Fourier space. May be faster/more accurate.",
    "neural_operator": "EXHAUSTED - Surrogates fail (sample-specific RMSE). PINN fails (needs gradients). No NN approach viable.",
    "alternative_es": "FAILED - OpenAI ES's diagonal covariance loses correlations. CMA-ES optimal for low-dim expensive problems. Do not pursue Natural ES/PEPG.",
    "differential_evolution": "FAILED - DE's mutation/crossover less efficient than CMA-ES covariance adaptation. Best 1.1325 @ 35.2 min (-0.0037 score). CMA-ES is optimal.",
    "multi_objective": "NOT TRIED - Pareto optimization of accuracy/speed jointly.",
    "problem_reformulation": "NOT TRIED - Different loss functions, regularization, parameterizations.",
    "simulated_annealing": "EXHAUSTED - SA is 2-5x slower than CMA-ES with local search, 23% worse without. Do NOT retry.",
    "surrogate_assisted": "ABORTED - RMSE landscapes are sample-specific. Surrogates from some samples don't generalize to others."
  },
  "completed_experiments": [
    "CMAES_TUNING_BATCH",
    "EXP_PSO_001",
    "EXP_BASINHOPPING_001",
    "EXP_DIFFEVO_001",
    "EXP_NM_MULTISTART_001",
    "EXP_HYBRID_TWOSTAGE_001",
    "EXP_ULTRACOARSE_001",
    "EXP_ADAPTIVE_SIGMA_001",
    "EXP_OUTLIER_FOCUS_001",
    "EXP_TARGETED_2SRC_001",
    "EXP_SURROGATE_NN_001",
    "EXP_COBYLA_REFINE_001",
    "EXP_FAST_ICA_001",
    "EXP_ENSEMBLE_001",
    "EXP_TRANSFER_LEARN_001",
    "EXP_LQ_CMAES_001",
    "EXP_BAYESIAN_OPT_001",
    "EXP_MULTIFID_OPT_001",
    "EXP_SEQUENTIAL_HANDOFF_001",
    "EXP_FAST_SOURCE_DETECT_001",
    "EXP_WS_CMAES_001",
    "EXP_ADAPTIVE_BUDGET_001",
    "EXP_TEMPORAL_FIDELITY_001",
    "EXP_TEMPORAL_HIGHER_SIGMA_001",
    "EXP_NICHING_CMAES_001",
    "EXP_EXTENDED_POLISH_001",
    "EXP_IPOP_TEMPORAL_001",
    "EXP_ADAPTIVE_TIMESTEP_001",
    "EXP_PHYSICS_INIT_001",
    "EXP_POD_SURROGATE_001",
    "EXP_2SOURCE_FOCUS_001",
    "EXP_ACTIVE_CMAES_001",
    "EXP_PROGRESSIVE_POLISH_FIDELITY_001",
    "EXP_LARGER_POPSIZE_001",
    "EXP_ADAPTIVE_SIGMA_SCHEDULE_001",
    "EXP_PARAMETER_SCALING_001",
    "EXP_BOUNDARY_AWARE_INIT_001",
    "EXP_JAX_AUTODIFF_001",
    "EXP_LEVENBERG_MARQUARDT_001",
    "EXP_HYBRID_CMAES_LBFGSB_001",
    "EXP_CONJUGATE_GRADIENT_001",
    "EXP_OPENAI_ES_001",
    "EXP_DIFFERENTIAL_EVOLUTION_001",
    "EXP_SEPARABLE_VP_001",
    "EXP_ADAPTIVE_SA_001",
    "EXP_PRETRAINED_SURROGATE_001",
    "EXP_SURROGATE_CMAES_001",
    "EXP_PINN_DIRECT_001"
  ]
}