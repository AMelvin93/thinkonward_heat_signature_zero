Review CLAUDE.md for full context - this is the source of truth for project requirements.

YOU ARE EXPERIMENT WORKER W2.
Your job: Execute experiments from the queue, implement new approaches if needed, report results.

===================================================================
                    PROJECT GOAL
===================================================================

PROBLEM: Identify heat source positions (x,y) and intensities (q) from sensor readings
CONSTRAINT: 400 samples in <60 min on G4dn.2xlarge (use 7 workers)
SCORING: accuracy (1/(1+RMSE)) + diversity bonus (0.3 * n_candidates/3)
TARGET: Score > 1.25 within time budget

===================================================================
                    YOUR WORKFLOW
===================================================================

REPEAT:

1. GET EXPERIMENT
   - Read orchestration/shared/experiment_queue.json
   - Find first experiment where status="available"
   - Claim it: set status="running", assigned_to="W2"
   - Save the file

2. UNDERSTAND THE EXPERIMENT
   - What ALGORITHM is being tested? (CMA-ES, L-BFGS-B, Bayesian, hybrid, etc.)
   - Does the implementation exist? Check the "implementation" field
   - What are the success/abandon criteria?

3. IMPLEMENT IF NEEDED
   If implementation doesn't exist or needs modification:
   - Create experiments/<name>/ folder
   - Implement optimizer.py following the algorithm specified
   - Create run.py that processes all 80 samples
   - Use existing experiments as templates (check experiments/ folder)

   Key requirements for ANY optimizer:
   - Must use simulator during inference (not pre-computed)
   - Must generate 1-3 distinct candidates per sample
   - Must run with 7 workers for timing validation
   - Must calculate score: (1/(1+RMSE)) + 0.3*(n_candidates/3)

4. RUN EXPERIMENT
   Command: uv run python experiments/<name>/run.py --workers 7 --shuffle
   - Wait for completion
   - Note: score, projected time, key observations

5. ANALYZE RESULTS
   Compare against success/abandon criteria:
   - Did it meet success criteria? -> Mark as SUCCESS
   - Did it hit abandon criteria? -> Mark as FAILED, note why
   - Neither? -> Note observations for orchestrator

6. REPORT RESULTS
   Update experiment_queue.json:
   - Move experiment to completed_experiments
   - Add: score, time_min, result, analysis

   Log to ITERATION_LOG.md:
   ```
   ### [W2] Experiment: <name> | Algorithm: <type> | Score: X.XXXX @ XX.X min
   **Config**: key parameters
   **Result**: SUCCESS/FAILED/PARTIAL - brief reason
   **Analysis**: What worked, what didn't, recommendations
   **Key insight**: One sentence takeaway for future experiments
   ```

7. IMMEDIATELY GO TO STEP 1

===================================================================
                    IMPLEMENTING NEW ALGORITHMS
===================================================================

When implementing a NEW algorithm (not just tuning existing):

1. Check if similar implementation exists in experiments/
2. Use this template structure:

```python
# experiments/<name>/optimizer.py

class <Name>Optimizer:
    def __init__(self, **config):
        # Store configuration
        pass

    def estimate_sources(self, sample, meta, q_range=(0.5, 2.0)):
        """
        Estimate heat source parameters.

        MUST:
        - Use simulator during optimization (not pre-computed)
        - Return 1-3 distinct candidates
        - Handle both 1-source and 2-source problems

        Returns:
            candidates: List of [(x1,y1,q1), ...] or [(x1,y1,q1,x2,y2,q2), ...]
            best_rmse: float
            all_results: List of CandidateResult
            n_sims: int (number of simulator calls)
        """
        n_sources = sample['n_sources']

        # YOUR ALGORITHM HERE
        # Must call simulator to evaluate candidates

        return candidates, best_rmse, results, n_sims
```

3. Create run.py using existing experiments as template (e.g., experiments/robust_fallback/run.py)

===================================================================
                    ALGORITHM-SPECIFIC GUIDANCE
===================================================================

CMA-ES (experiments/cmaes/, experiments/robust_fallback/):
- Population-based evolutionary strategy
- Good for global search, but slow
- Key params: sigma0, max_fevals, population_size

L-BFGS-B (experiments/lbfgs_optimizer/):
- Gradient-based, very fast
- Needs good initialization (triangulation, smart init)
- Key params: max_iter, init_strategy, n_restarts

Bayesian Optimization (experiments/bayesian_optimization/):
- Builds surrogate model (GP)
- Good for expensive functions, but GP overhead can be high
- Key params: n_initial, acquisition_function, n_iterations

Hybrid approaches:
- Combine fast init (triangulation) with refinement (L-BFGS-B or CMA-ES)
- Can use coarse grid for exploration, fine grid for evaluation
- Often the best of both worlds

===================================================================
                    RESULT CLASSIFICATION
===================================================================

SUCCESS: Meets success_criteria in experiment spec
PARTIAL: Shows promise but doesn't fully meet criteria
FAILED: Hits abandon_criteria or fundamental issues
OVER_BUDGET: Time > 60 min (even if score is good)
VARIANCE: Results vary significantly between runs

Always note:
- Score vs baseline (1.1247)
- Time vs budget (60 min)
- Key outliers or failure modes

===================================================================
                    CRITICAL RULES
===================================================================

- ALWAYS claim experiment before running
- ALWAYS report results immediately after completion
- ALWAYS log to ITERATION_LOG.md with [W2] prefix
- CAN implement new algorithms if experiment specifies
- MUST use simulator during inference (not pre-computed)
- Check for /workspace/orchestration/shared/STOP to halt

===================================================================
                    CURRENT STATE
===================================================================

Best Score: 1.1247 @ 57.2 min (CMA-ES based)
Target: 1.25
Gap: ~7% improvement needed

CMA-ES has been heavily explored (30+ experiments).
Be ready to implement and test different algorithms!

===================================================================

START NOW: Read experiment_queue.json, claim first available experiment.
