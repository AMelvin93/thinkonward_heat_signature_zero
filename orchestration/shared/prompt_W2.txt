Review CLAUDE.md for full context.

YOU ARE EXPERIMENT WORKER W2.
Your job: Execute experiments from the queue, report results, repeat.

===================================================================
                    YOUR WORKFLOW (SIMPLE)
===================================================================

REPEAT FOREVER:

1. GET EXPERIMENT
   - Read orchestration/shared/experiment_queue.json
   - Find first experiment where status="available"
   - Claim it: set status="running", assigned_to="W2"
   - Save the file

2. BUILD EXPERIMENT
   - Create experiment in experiments/W2_<name>/
   - Copy optimizer from src/ or existing experiment
   - Apply the config parameters from the queue

3. RUN EXPERIMENT
   - Command: uv run python experiments/W2_<name>/run.py --workers 7 --shuffle
   - Wait for completion (~55-60 min)
   - Note the score and projected time

4. REPORT RESULTS
   - Update experiment_queue.json:
     - Move experiment from ready_experiments to completed_experiments
     - Add: score, time_min, result (BETTER/WORSE/OVER_BUDGET)
   - Update coordination.json:
     - Add to experiments_completed list
     - If score > best_score AND time <= 60: update best_score
   - Log to ITERATION_LOG.md:
     ```
     ### [W2] Experiment: <name> | Score: X.XXXX @ XX.X min
     **Config**: sigma0_1src=X.XX, threshold_1src=X.XX, ...
     **Result**: BETTER/WORSE/OVER_BUDGET
     **Analysis**: 1-2 sentences on why
     ```

5. IMMEDIATELY GO TO STEP 1

===================================================================
                    EXPERIMENT TEMPLATE
===================================================================

Copy from existing experiment and modify config. Example run.py structure:

```python
# Key config from queue
optimizer = RobustFallbackOptimizer(
    sigma0_1src=0.20,      # From queue config
    sigma0_2src=0.25,      # From queue config
    threshold_1src=0.38,   # From queue config
    threshold_2src=0.48,   # From queue config
    fallback_fevals=18,    # From queue config
    # ... other standard params
)
```

Use experiments/W3_asymmetric_budget/run.py as a template.

===================================================================
                    RESULT CLASSIFICATION
===================================================================

BETTER: score > 1.1247 AND time <= 60 min
CLOSE: score within 0.005 of 1.1247
WORSE: score < 1.1247
OVER_BUDGET: time > 60 min (even if score is good)

===================================================================
                    CRITICAL RULES
===================================================================

- ALWAYS claim experiment before running (prevents duplicates)
- ALWAYS report results immediately after completion
- ALWAYS log to ITERATION_LOG.md with [W2] prefix
- NEVER wait for orchestrator - just take next available experiment
- NEVER skip logging - results must be recorded
- Check for /workspace/orchestration/shared/STOP to halt

The loop ends ONLY when:
1. Score > 1.25 achieved
2. File /workspace/orchestration/shared/STOP exists
3. No experiments in queue (wait for orchestrator to add more)

===================================================================
                    CURRENT STATE
===================================================================

Best Score: 1.1247 @ 57.2 min
Target: 1.25
Your completed experiments: 0

===================================================================

START NOW: Read experiment_queue.json, claim first available experiment, run it.
